{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466d394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.dsl_research_assistant.captions.image_captioning import ImageCaptioning\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c4aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_tasks = [ (\"xtuner/llava-llama-3-8b-v1_1-transformers\", \"image-text-to-text\")]  #\"mPLUG/DocOwl2\", \"ds4sd/SmolDocling-256M-preview\", (\"Salesforce/blip-image-captioning-base\",\"image-to-text\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227ab1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating caption for the image...\n",
      "Generated caption: a group of women standing in different poses\n"
     ]
    }
   ],
   "source": [
    "image_captioning = ImageCaptioning(model_name=\"Salesforce/blip-image-captioning-base\", task=\"image-to-text\")\n",
    "image_path = \"C:\\\\Users\\\\grado\\\\Desktop\\\\dsl-research-assistant\\\\backend\\\\image_captions\\\\Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper\\\\Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-2.png\"\n",
    "prompt = \"Describe this image in a few sentences.\"\n",
    "context = \"This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. Gaussian Shell Maps for Efficient 3D Human Generation Rameen Abdal *1 Wang Yifan *1 Zifan Shi *â€ 1,2 Yinghao Xu 1 Ryan Po 1 Zhengfei Kuang 1 Qifeng Chen 2 Dit-Yan Yeung 2 Gordon Wetzstein 1 1 Stanford University, USA HKUST, Hong Kong 2\"\n",
    "print(\"Generating caption for the image...\")\n",
    "caption = image_captioning.generate_caption(image_path, prompt, context)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423eaf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation in directory: C:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\image_captions\n",
      "Processing directory: C:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\image_captions\n",
      "Processing directory: C:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\image_captions\\Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-1.png. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-10.png\n",
      "GT Caption: Gaussian Shell Maps. Gaussian Shell Maps is an efficient framework for 3D human generation connecting 3D Gaussians with CNN-based generators. 3D Gaussians are anchored to 'shells' derived from the SMPL template [37] (only two shells are visualized for clarity), and the appearance is modeled in texture space. Trained only on 2D images, we show that our method can generate diverse articulable humans in real-time with state-of-the-art quality directly in high resolution without the need for upsampling and hence avoiding aliasing artifacts.\n",
      "Generated: a diagram showing the different types of the body\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.015384615384615385, 'p': 0.125, 'f': 0.02739725832238708}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.015384615384615385, 'p': 0.125, 'f': 0.02739725832238708}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-11.png\n",
      "GT Caption: Method Overview. We propose an expressive yet highly efficient representation, Gaussian Shell Map (GSM), for 3D human generation. Combining the idea of 3D Gaussians and Shell Maps (Sec. 3), we sample 3D Gaussians on 'shells', which are mesh layers offsetted from the SMPL template, forming a shell volume to model complex and diverse geometry and appearance; the Gaussian parameters are learned in the texture space, allowing us to leverage existing CNN-based generative architecture (Sec. 4.1). Articulation is straightforward by interpolating the deformation of the shell (Sec. 4.2). The generation is supervised by single-view 2D images using several discriminator critics, including part-specific face, hands, and feet discriminators (Sec. 4.3).\n",
      "Generated: a woman ' s body is shown in the middle of the image\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.16666666666666666, 'f': 0.0336134435647201}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-12.png\n",
      "GT Caption: Novel Views and Animation. We can render a generated identity in novel views and articulate it for animations.\n",
      "Generated: a group of four different pictures of a woman\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.058823529411764705, 'p': 0.14285714285714285, 'f': 0.08333332920138908}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.14285714285714285, 'f': 0.08333332920138908}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-2.png. Skipping...\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-3.png. Skipping...\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-4.png\n",
      "GT Caption: Latent Code Interpolation. Latent code interpolation of our model trained on DeepFashion Dataset.\n",
      "Generated: the ultimate guide to wearing jeans\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-5.png. Skipping...\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-6.png\n",
      "GT Caption: Qualitative Comparison. We compare our results with GNARF and EVA3D baselines on DeepFashion and SHHQ datasets. In each case, we show the deformed body poses of the identities generated by the methods. The competing methods exhibit artifacts marked in red. Notice that our approach generates high-quality textures, like facial details and more realistic deformations.\n",
      "Generated: a woman in a black shirt and grey pants\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.041666666666666664, 'p': 0.25, 'f': 0.07142856897959192}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.25, 'f': 0.07142856897959192}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-7.png\n",
      "GT Caption: Multi-view Consistency. The figure shows a closeup of the garment in two different views. The green dashed line follows the body shape and indicates the same 3D position. In AG3D [15], the pattern significantly changes, while ours, utilizing texture maps, has built-in view consistency.\n",
      "Generated: a line of women wearing jeans and tank tops\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.1, 'p': 0.4444444444444444, 'f': 0.16326530312369852}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1, 'p': 0.4444444444444444, 'f': 0.16326530312369852}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-8.png\n",
      "GT Caption: Per-shell Visualization . We visualize the per-shell contributions. The shells decompose the human body into different parts, where the inner shells capture the torso, and the outer shells capture the clothing and hair.\n",
      "Generated: a woman in a suit and jacket is dancing\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.043478260869565216, 'p': 0.125, 'f': 0.06451612520291386}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.125, 'f': 0.06451612520291386}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "Image: Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-picture-9.png\n",
      "GT Caption: w/o Mask and Part Discriminators\n",
      "Generated: a group of people in different poses\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-table-1.png. Skipping...\n",
      "No ground truth caption found for Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper-table-2.png. Skipping...\n",
      "Processing directory: C:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\image_captions\\Abdelfattah_MaskCLR_Attention-Guided_Contrastive_Learning_for_Robust_Action_Representation_Learning_CVPR_2024_paper\n",
      "No ground truth caption found for Abdelfattah_MaskCLR_Attention-Guided_Contrastive_Learning_for_Robust_Action_Representation_Learning_CVPR_2024_paper-picture-1.png. Skipping...\n",
      "Image: Abdelfattah_MaskCLR_Attention-Guided_Contrastive_Learning_for_Robust_Action_Representation_Learning_CVPR_2024_paper-picture-10.png\n",
      "GT Caption: (a) Activated joints by MotionBERT [51] and our MaskCLR. Actions are from NTU60-XSub [33] dataset. Labels from left to right: 'throw', 'wear a shoe', 'brush hair', 'drink water', and 'pickup.'\n",
      "Generated: the graph of the regression plot\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Abdelfattah_MaskCLR_Attention-Guided_Contrastive_Learning_for_Robust_Action_Representation_Learning_CVPR_2024_paper-picture-11.png\n",
      "GT Caption: (b) Accuracy under Gaussian noise N (0 , 0 002 . 2 ) .\n",
      "Generated: the graph of the line of the curve is shown in the diagram\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
      "\n",
      "CIDEr Score: 0.0000\n",
      "\n",
      "No ground truth caption found for Abdelfattah_MaskCLR_Attention-Guided_Contrastive_Learning_for_Robust_Action_Representation_Learning_CVPR_2024_paper-picture-12.png. Skipping...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe this image in a few sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m generated_caption \u001b[38;5;241m=\u001b[39m \u001b[43mimage_captioning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m bleu \u001b[38;5;241m=\u001b[39m sentence_bleu([gt_caption\u001b[38;5;241m.\u001b[39msplit()], generated_caption\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     31\u001b[0m rouge \u001b[38;5;241m=\u001b[39m Rouge()\n",
      "File \u001b[1;32m~\\Desktop\\dsl-research-assistant\\backend\\src\\dsl_research_assistant\\captions\\image_captioning.py:126\u001b[0m, in \u001b[0;36mImageCaptioning.generate_caption\u001b[1;34m(self, image_path, prompt, context)\u001b[0m\n\u001b[0;32m    124\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     caption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-text-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    130\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>user<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreate caption for this scientific figure?<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:148\u001b[0m, in \u001b[0;36mImageToTextPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call the image-to-text pipeline without an inputs argument!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         )\n\u001b[0;32m   1429\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:220\u001b[0m, in \u001b[0;36mImageToTextPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# FIXME: We need to pop here due to a difference in how `generation.py` and `generation.tf_utils.py`\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m#  parse inputs. In the Tensorflow version, `generate` raises an error if we don't use `input_ids` whereas\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m#  the PyTorch version matches it with `self.model.main_input_name` or `self.model.encoder.main_input_name`\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m#  in the `_prepare_model_inputs` method.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m inputs \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmain_input_name)\n\u001b[1;32m--> 220\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grado\\Desktop\\dsl-research-assistant\\backend\\.venv\\lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py:1084\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.generate\u001b[1;34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(input_ids)\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1081\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1082\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m-> 1084\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m input_ids[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mbos_token_id\n\u001b[0;32m   1088\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import evaluate\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "cider_scorer = Cider()\n",
    "base_dir = \"C:\\\\Users\\\\grado\\\\Desktop\\\\dsl-research-assistant\\\\backend\\\\image_captions\"\n",
    "#image_captioning = ImageCaptioning(model_name=\"ds4sd/SmolDocling-256M-preview\", task=\"image-text-to-text\")\n",
    "image_captioning = ImageCaptioning(model_name=\"Salesforce/blip-image-captioning-base\", task=\"image-to-text\")\n",
    "blue_scores = []\n",
    "print(f\"Starting evaluation in directory: {base_dir}\")\n",
    "for subdir, dirs, files in os.walk(base_dir):\n",
    "    print(f\"Processing directory: {subdir}\")\n",
    "    if \"caption.txt\" in files:\n",
    "        caption_file = os.path.join(subdir, \"caption.txt\")\n",
    "        with open(caption_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt_captions = [line.strip() for line in f.readlines() if not line.startswith(\"__\")]\n",
    "        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        for idx, image_file in enumerate(image_files):\n",
    "            image_path = os.path.join(subdir, image_file)\n",
    "            gt_caption = gt_captions[idx] if idx < len(gt_captions) else \"\"\n",
    "            if not gt_caption:\n",
    "                print(f\"No ground truth caption found for {image_file}. Skipping...\")\n",
    "                continue\n",
    "            prompt = \"Describe this image in a few sentences.\"\n",
    "            context = \"\"\n",
    "            generated_caption = image_captioning.generate_caption(image_path, prompt, context)\n",
    "            bleu = sentence_bleu([gt_caption.split()], generated_caption.split())\n",
    "            rouge = Rouge()\n",
    "            rouge_scores = rouge.get_scores(generated_caption, gt_caption)[0]\n",
    "            gts = {0: [gt_caption]}\n",
    "            res = {0: [generated_caption]}\n",
    "            cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "            print(f\"Image: {image_file}\")\n",
    "            print(f\"GT Caption: {gt_caption}\")\n",
    "            print(f\"Generated: {generated_caption}\")\n",
    "            print(f\"BLEU Score: {bleu:.4f}\")\n",
    "            print(f\"ROUGE Scores: {rouge_scores}\\n\")\n",
    "            print(f\"CIDEr Score: {cider_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098be49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0\n",
      "Average ROUGE Scores: {'rouge-1': {'r': 0.02127659574468085, 'p': 0.1, 'f': 0.035087716405047945}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.1, 'f': 0.035087716405047945}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Average BLEU Score:\", sum(blue_scores) / len(blue_scores) if blue_scores else 0)\n",
    "print(\"Average ROUGE Scores:\", rouge_scores if 'rouge_scores' in locals() else \"No scores computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c6665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
