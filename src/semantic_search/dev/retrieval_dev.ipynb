{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing HF embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/python_envs/dsl-research-assistant/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scraped abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 12:41:26.180904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-14 12:41:30.523603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-14 12:41:45.307631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index or document store not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking and encoding: 100%|██████████| 14956/14956 [00:45<00:00, 328.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 3 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2685/2685 [00:20<00:00, 133.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LocalEmbeddingModel(chunk_size=256)\n",
    "store = FAISSDocumentStore(model, db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/references-1')\n",
    "\n",
    "if not store.load_index():\n",
    "    docs = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-refs-abstracts.csv')\n",
    "    docs.loc[docs.abstract.isna(), 'abstract'] = ''\n",
    "    docs['has_abstract'] = docs.abstract.apply(len) > 0\n",
    "\n",
    "    docs['ref_work'] = docs['ref_work'].str.split('/').str[-1]\n",
    "    docs.rename(columns={'ref_work': 'id', 'abstract': 'text'}, inplace=True)\n",
    "    docs = docs[docs.has_abstract]\n",
    "\n",
    "    store.create_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking and encoding: 100%|██████████| 1/1 [00:00<00:00, 536.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'score': 0.8255381213288242,\n",
       "  'document_id': 'W4307106676',\n",
       "  'chunk_text': 'faster, and lighter. our code is available in https : / / github. com / rmokady / clip _ prefix _ caption.'},\n",
       " {'rank': 2,\n",
       "  'score': 0.793164941719414,\n",
       "  'document_id': 'W4387323008',\n",
       "  'chunk_text': 'available at https : / / github. com / wusize / clipself.'},\n",
       " {'rank': 3,\n",
       "  'score': 0.7750313673011238,\n",
       "  'document_id': 'W3190434222',\n",
       "  'chunk_text': 'recently, there have been breakthroughs in computer vision ( \" cv \" ) models that are more generalizable with the advent of models such as clip and align. in this paper, we analyze clip and highlight some of the challenges such models pose. clip reduces the need for task specific training data, potentially opening up many niche tasks to automation. clip also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. additionally, through some preliminary probes we find that clip can inherit biases found in prior computer vision systems. given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. these results add evidence to the growing body of work calling for a change in the notion of a \\' better \\' model - - to move beyond simply looking at higher accuracy at task - oriented capability evaluations, and towards a broader \\' better \\' that takes into account deployment - critical features such as different use contexts, and people who interact with the model when thinking about model deployment.'},\n",
       " {'rank': 4,\n",
       "  'score': 0.7655741127301011,\n",
       "  'document_id': 'W4391421051',\n",
       "  'chunk_text': 'potential faulty negative samples in clip - caption contrast by rectifying the alignment target with ot assignment to ensure precise temporal modeling. extensive experiments on video retrieval, videoqa, and action segmentation verify the effectiveness of our method. code is available at https : / / lin - yijie. github. io / projects / norton.'},\n",
       " {'rank': 5,\n",
       "  'score': 0.7602766307286472,\n",
       "  'document_id': 'W4390874338',\n",
       "  'chunk_text': \"audio description ( ad ) is the task of generating descriptions of visual content, at suitable time intervals, for the benefit of visually impaired audiences. for movies, this presents notable challenges – ad must occur only during existing pauses in dialogue, should refer to characters by name, and ought to aid understanding of the storyline as a whole. to this end, we develop a new model for automatically generating movie ad, given clip visual features of the frames, the cast list, and the temporal locations of the speech ; addressing all three of the ' who ', ' when ', and ' what ' questions : ( i ) who – we introduce a character bank consisting of the character ' s name, the actor that played the part, and a clip feature of their face, for the principal cast of each movie, and demonstrate how this can be used to improve naming in the generated ad ; ( ii ) when – we investigate several models for determining whether an ad should be generated for a time interval or not, based on the visual content of the interval and its neighbours ; and ( iii ) what – we implement a new vision - language model for this task, that can ingest the proposals from the character bank, whilst conditioning on the visual features using cross - attention, and demonstrate how\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.search(\"CLIP related works\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Docling papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 73809 vectors\n"
     ]
    }
   ],
   "source": [
    "embedding_model = LocalEmbeddingModel(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='cuda',\n",
    "    batch_size=8\n",
    ")\n",
    "document_store = FAISSDocumentStore(\n",
    "    embedding_model=embedding_model,\n",
    "    db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/chunk1-txt-all',\n",
    ")\n",
    "\n",
    "if not document_store.load_index():\n",
    "    document_store.create_index('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er-efficient visual instruction model. arXiv 2304.15010 , 2023. 1, 3, 4\n",
      "- [9] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role...\n"
     ]
    }
   ],
   "source": [
    "print(document_store.search(\"visual\", top_k=5)[0]['chunk_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata for CVPR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import utils\n",
    "reload(utils)\n",
    "\n",
    "from semantic_search.utils import get_title_from_fpath, get_orig_metadata, multithread_apply, parse_list_string, count_references, get_ref_metadata\n",
    "\n",
    "# Set pandas display options to show wider dataframes\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text in columns\n",
    "pd.set_option('display.width', 1000)         # Set the display width\n",
    "pd.set_option('display.max_columns', 20)     # Show more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06f8ad35e094fadbead424dc90ffb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>fname</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 fpath                                                                                                            fname                                                                                                                      title                                            doi                              oaid\n",
       "0                                                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                             Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                                                               Fixed Point Diffusion Models              https://doi.org/10.1063/1.2121687  https://openalex.org/W2000456051\n",
       "1                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                             Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                                                             BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection   https://doi.org/10.1109/cvpr52733.2024.01901  https://openalex.org/W4402727763\n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild  https://doi.org/10.1109/cvprw63382.2024.00473  https://openalex.org/W4402916217"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "df = pd.DataFrame([(str(fpath), fpath.name) for fpath in Path(raw_dir).glob(\"*.txt\")], columns=['fpath', 'fname'])\n",
    "df['title'] = df['fpath'].apply(get_title_from_fpath)\n",
    "df['doi', 'oaid', 'refs_oaid'] = multithread_apply(df['title'].values, get_orig_metadata, n_workers=5)\n",
    "df['refs_doi'] = ''\n",
    "# df.to_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-ids+refs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving abstracts of cited works\n",
    "(only on subset of papers whose references we found using OpenAlex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:44<00:00,  4.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oaid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>type</th>\n",
       "      <th>topic</th>\n",
       "      <th>domain</th>\n",
       "      <th>field</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2194775991</td>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</td>\n",
       "      <td>article</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W2108598243</td>\n",
       "      <td>ImageNet: A large-scale hierarchical image database</td>\n",
       "      <td>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \"ImageNet\", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</td>\n",
       "      <td>article</td>\n",
       "      <td>Advanced Image and Video Retrieval Techniques</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W1976499671</td>\n",
       "      <td>Comparison of simple potential functions for simulating liquid water</td>\n",
       "      <td>Classical Monte Carlo simulations have been carried out for liquid water in the NPT ensemble at 25 °C and 1 atm using six of the simpler intermolecular potential functions for the water dimer: Bernal–Fowler (BF), SPC, ST2, TIPS2, TIP3P, and TIP4P. Comparisons are made with experimental thermodynamic and structural data including the recent neutron diffraction results of Thiessen and Narten. The computed densities and potential energies are in reasonable accord with experiment except for the original BF model, which yields an 18% overestimate of the density and poor structural results. The TIPS2 and TIP4P potentials yield oxygen–oxygen partial structure functions in good agreement with the neutron diffraction results. The accord with the experimental OH and HH partial structure functions is poorer; however, the computed results for these functions are similar for all the potential functions. Consequently, the discrepancy may be due to the correction terms needed in processing the neutron data or to an effect uniformly neglected in the computations. Comparisons are also made for self-diffusion coefficients obtained from molecular dynamics simulations. Overall, the SPC, ST2, TIPS2, and TIP4P models give reasonable structural and thermodynamic descriptions of liquid water and they should be useful in simulations of aqueous solutions. The simplicity of the SPC, TIPS2, and TIP4P functions is also attractive from a computational standpoint.</td>\n",
       "      <td>article</td>\n",
       "      <td>Chemical and Physical Properties in Aqueous Solutions</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Chemical Engineering</td>\n",
       "      <td>Filtration and Separation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W1861492603</td>\n",
       "      <td>Microsoft COCO: Common Objects in Context</td>\n",
       "      <td></td>\n",
       "      <td>book-chapter</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W2565639579</td>\n",
       "      <td>Feature Pyramid Networks for Object Detection</td>\n",
       "      <td>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               oaid                                                                 title  \\\n",
       "0  https://openalex.org/W2194775991                          Deep Residual Learning for Image Recognition   \n",
       "1  https://openalex.org/W2108598243                   ImageNet: A large-scale hierarchical image database   \n",
       "2  https://openalex.org/W1976499671  Comparison of simple potential functions for simulating liquid water   \n",
       "3  https://openalex.org/W1861492603                             Microsoft COCO: Common Objects in Context   \n",
       "4  https://openalex.org/W2565639579                         Feature Pyramid Networks for Object Detection   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            abstract  \\\n",
       "0                                                                                                                                                                      Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.   \n",
       "1                                                                               The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \"ImageNet\", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.   \n",
       "2  Classical Monte Carlo simulations have been carried out for liquid water in the NPT ensemble at 25 °C and 1 atm using six of the simpler intermolecular potential functions for the water dimer: Bernal–Fowler (BF), SPC, ST2, TIPS2, TIP3P, and TIP4P. Comparisons are made with experimental thermodynamic and structural data including the recent neutron diffraction results of Thiessen and Narten. The computed densities and potential energies are in reasonable accord with experiment except for the original BF model, which yields an 18% overestimate of the density and poor structural results. The TIPS2 and TIP4P potentials yield oxygen–oxygen partial structure functions in good agreement with the neutron diffraction results. The accord with the experimental OH and HH partial structure functions is poorer; however, the computed results for these functions are similar for all the potential functions. Consequently, the discrepancy may be due to the correction terms needed in processing the neutron data or to an effect uniformly neglected in the computations. Comparisons are also made for self-diffusion coefficients obtained from molecular dynamics simulations. Overall, the SPC, ST2, TIPS2, and TIP4P models give reasonable structural and thermodynamic descriptions of liquid water and they should be useful in simulations of aqueous solutions. The simplicity of the SPC, TIPS2, and TIP4P functions is also attractive from a computational standpoint.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "4                                                                                                                                                                                                                                                                                                                                    Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.   \n",
       "\n",
       "           type                                                  topic             domain                 field                                 subfield  \n",
       "0       article                   Advanced Neural Network Applications  Physical Sciences      Computer Science  Computer Vision and Pattern Recognition  \n",
       "1       article          Advanced Image and Video Retrieval Techniques  Physical Sciences      Computer Science  Computer Vision and Pattern Recognition  \n",
       "2       article  Chemical and Physical Properties in Aqueous Solutions  Physical Sciences  Chemical Engineering                Filtration and Separation  \n",
       "3  book-chapter                   Advanced Neural Network Applications  Physical Sciences      Computer Science  Computer Vision and Pattern Recognition  \n",
       "4      preprint                   Advanced Neural Network Applications  Physical Sciences      Computer Science  Computer Vision and Pattern Recognition  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-ids+refs.csv')\n",
    "df['referenced_works'] = df['referenced_works'].apply(parse_list_string)\n",
    "df[['total_references', 'references_in_dataset']] = df.apply(lambda x: count_references(x, df), axis=1, result_type='expand')\n",
    "\n",
    "# Retrieve references via OpenAlex API\n",
    "all_refs = pd.Series(np.concatenate(df.referenced_works.values)).unique()\n",
    "all_refs_batched = [all_refs[i:i+100] for i in range(0, len(all_refs), 100)]\n",
    "results = multithread_apply(all_refs_batched, get_ref_metadata, n_workers=5)\n",
    "\n",
    "ref_df = pd.DataFrame(np.concatenate(results), columns=['oaid', 'title', 'abstract', 'type', 'topic', 'domain', 'field', 'subfield'])\n",
    "ref_df.to_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-refs-abstracts.csv', index=False)\n",
    "ref_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check papers with missing abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Korycki_Class-Incremental_Mixture_of_Gaussians_for_Deep_Continual_Learning_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hoque_IrrNet_Spatio-Temporal_Segmentation_Guided_Classification_for_Irrigation_Mapping_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Zhang_MOHO_Learning_Single-view_Hand-held_Object_Reconstruction_with_Multi-view_Occlusion-Aware_Supervision_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Paissan_Structured_Sparse_Back-propagation_for_Lightweight_On-Device_Continual_Learning_on_Microcontroller_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Halawa_Multi-Task_Multi-Modal_Self-Supervised_Learning_for_Facial_Expression_Recognition_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Loaliyan_Comparative_Analysis_of_Generalization_and_Harmonization_Methods_for_3D_Brain_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hu_Low_Latency_Point_Cloud_Rendering_with_Learned_Splatting_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Ko_Semantic_Line_Combination_Detector_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Duan_Towards_Backward-Compatible_Continual_Learning_of_Image_Compression_CVPR_2024_paper.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "def extract_abstract(fpath: str):\n",
    "    doc_text = fpath.read_text(encoding=\"utf-8\")\n",
    "    abstract_match = re.search(r'## Abstract\\n\\n(.*?)(?=\\n\\n## \\d+\\.)', doc_text, re.DOTALL)\n",
    "    return abstract_match.group(1) if abstract_match else ''\n",
    "\n",
    "for fpath in Path(raw_dir).glob(\"*.txt\"):\n",
    "    if extract_abstract(fpath) == '':\n",
    "        print(fpath)\n",
    "\n",
    "# abstracts =[check_single_doc(fpath) for fpath in Path(raw_dir).glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SemanticScholar API to retrieve references not available via OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "      <th>paperId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "      <td>9c50996ba35eb605cb9bcd5835103b441cf38e07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 fpath                                                                                                                      title                                            doi                              oaid                                   paperId\n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild  https://doi.org/10.1109/cvprw63382.2024.00473  https://openalex.org/W4402916217  9c50996ba35eb605cb9bcd5835103b441cf38e07"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_search.utils import parse_list_string\n",
    "\n",
    "df = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-ids+refs.csv')\n",
    "df['refs_oaid'] = df['refs_oaid'].apply(parse_list_string)\n",
    "oa_df = df[['fpath', 'title', 'doi', 'oaid', 'refs_oaid']]\n",
    "\n",
    "metadata_fpath = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/semschol-ids.csv'\n",
    "df = pd.read_csv(metadata_fpath)\n",
    "semschol_df = df[['fpath', 'paperId']]\n",
    "\n",
    "combined_df = pd.merge(oa_df, semschol_df, on='fpath', how='left')\n",
    "combined_df = combined_df[combined_df.refs_oaid.apply(len) == 0].drop(columns=['refs_oaid'])\n",
    "\n",
    "combined_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  8.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "from semanticscholar.SemanticScholarException import ObjectNotFoundException\n",
    "from functools import partial\n",
    "\n",
    "paper_id = combined_df.iloc[0].paperId\n",
    "print(f'Title: {combined_df.iloc[0].title}')\n",
    "sch = SemanticScholar()\n",
    "\n",
    "def get_referenced_dois(sch: SemanticScholar, paper_id: str):\n",
    "    \n",
    "    try:\n",
    "          # 'paperId', 'title' TODO: Do title check with OA data?\n",
    "        raw = sch.get_paper_references(paper_id=paper_id, fields=['externalIds'], limit=1000)\n",
    "    except ObjectNotFoundException:\n",
    "        return []\n",
    "\n",
    "    dois = []\n",
    "    for item in raw.items:\n",
    "        external_ids = item['citedPaper'].get('externalIds')\n",
    "        if external_ids is None: continue\n",
    "        doi = external_ids.get('DOI', None)\n",
    "        if doi is None: continue\n",
    "        dois.append(doi)\n",
    "    return dois\n",
    "\n",
    "res = multithread_apply(combined_df.paperId.values[:5], partial(get_referenced_dois, sch), n_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10.48550/arXiv.2401.11605',\n",
       "  '10.48550/arXiv.2401.08639',\n",
       "  '10.48550/arXiv.2310.18605',\n",
       "  '10.1109/WACV57701.2024.00532',\n",
       "  '10.48550/arXiv.2304.14108',\n",
       "  '10.1109/ICCV51070.2023.00387',\n",
       "  '10.1109/CVPR52729.2023.00043',\n",
       "  '10.48550/arXiv.2210.12867',\n",
       "  '10.48550/arXiv.2210.08402',\n",
       "  '10.48550/arXiv.2206.00927',\n",
       "  '10.48550/arXiv.2206.00364',\n",
       "  '10.48550/arXiv.2205.15019',\n",
       "  '10.48550/arXiv.2204.13902',\n",
       "  '10.1109/CVPR52688.2022.00070',\n",
       "  '10.1109/CVPR52688.2022.01042',\n",
       "  '10.1609/aaai.v36i6.20619',\n",
       "  '10.1109/tci.2021.3118944',\n",
       "  '10.1142/11590',\n",
       "  '10.18653/v1/2021.findings-emnlp.344',\n",
       "  '10.1007/978-3-319-24574-4_28',\n",
       "  '10.1109/CVPR.2009.5206848',\n",
       "  '10.1007/978-94-009-8177-5',\n",
       "  '10.1145/321296.321305',\n",
       "  '10.1090/S0025-5718-1965-0198670-6',\n",
       "  '10.1007/978-3-642-01492-5_2',\n",
       "  '10.1109/5.726791',\n",
       "  '10.1007/13663.1687-1812',\n",
       "  '10.2307/3608793',\n",
       "  '10.1098/rstl.1685.0053'],\n",
       " ['10.48550/arXiv.2310.15670',\n",
       "  '10.48550/arXiv.2308.09616',\n",
       "  '10.1109/ICCV51070.2023.01703',\n",
       "  '10.1109/ICCV51070.2023.00637',\n",
       "  '10.1109/IROS55552.2023.10341778',\n",
       "  '10.48550/arXiv.2307.01492',\n",
       "  '10.1109/ICCV51070.2023.00772',\n",
       "  '10.1109/CVPR52729.2023.01713',\n",
       "  '10.48550/arXiv.2305.14018',\n",
       "  '10.48550/arXiv.2304.14365',\n",
       "  '10.48550/arXiv.2304.07193',\n",
       "  '10.48550/arXiv.2303.16628',\n",
       "  '10.1109/ICCV51070.2023.00335',\n",
       "  '10.1109/CVPR52729.2023.02076',\n",
       "  '10.1109/TIV.2023.3274536',\n",
       "  '10.1109/LRA.2024.3401172',\n",
       "  '10.1109/CVPR52729.2023.02274',\n",
       "  '10.1109/ICCV51070.2023.01675',\n",
       "  '10.1109/CVPR52729.2023.01712',\n",
       "  '10.48550/arXiv.2211.17111',\n",
       "  '10.48550/arXiv.2211.10581',\n",
       "  '10.1109/CVPR52729.2023.01710',\n",
       "  '10.1109/CVPR52729.2023.01385',\n",
       "  '10.48550/arXiv.2210.07372',\n",
       "  '10.48550/arXiv.2210.02443',\n",
       "  '10.48550/arXiv.2209.10248',\n",
       "  '10.48550/arXiv.2209.05588',\n",
       "  '10.1109/CVPR52729.2023.02073',\n",
       "  '10.48550/arXiv.2208.11112',\n",
       "  '10.48550/arXiv.2208.10976',\n",
       "  '10.3390/rs14153824',\n",
       "  '10.48550/arXiv.2206.10092',\n",
       "  '10.1109/CVPR52729.2023.01296',\n",
       "  '10.1109/ICCV51070.2023.00302',\n",
       "  '10.1109/ICRA48891.2023.10160968',\n",
       "  '10.48550/arXiv.2205.08534',\n",
       "  '10.48550/arXiv.2203.17270',\n",
       "  '10.48550/arXiv.2203.17054',\n",
       "  '10.1109/CVPR52688.2022.00116',\n",
       "  '10.48550/arXiv.2203.05625',\n",
       "  '10.1109/CVPR52688.2022.01167',\n",
       "  '10.1109/ICCV48922.2021.00313',\n",
       "  '10.1109/ICCV48922.2021.00339',\n",
       "  '10.1109/ICCVW54120.2021.00107',\n",
       "  '10.1109/ICCV48922.2021.01499',\n",
       "  '10.1007/978-3-030-58568-6_12',\n",
       "  '10.1109/CVPR46437.2021.01161',\n",
       "  '10.1007/978-3-030-58452-8_13',\n",
       "  '10.1109/ICCV.2019.00852',\n",
       "  '10.1109/CVPRW.2019.00103',\n",
       "  '10.1109/TPAMI.2019.2938758',\n",
       "  '10.1109/cvpr42600.2020.01164',\n",
       "  '10.1109/CVPR.2017.106',\n",
       "  '10.1109/TCSVT.2017.2740321',\n",
       "  '10.1109/cvpr.2016.90',\n",
       "  '10.1109/ICCV.2015.179',\n",
       "  '10.1109/CVPR.2015.7299152',\n",
       "  '10.1109/CVPR.2014.97',\n",
       "  '10.1007/978-3-319-10602-1_48',\n",
       "  '10.1109/CVPR.2009.5206848'],\n",
       " ['10.1109/CVPRW63382.2024.00483',\n",
       "  '10.48550/arXiv.2403.12572',\n",
       "  '10.48550/arXiv.2403.11440',\n",
       "  '10.1109/CVPRW63382.2024.00490',\n",
       "  '10.48550/arXiv.2403.11450',\n",
       "  '10.48550/arXiv.2403.10825',\n",
       "  '10.1109/CVPRW63382.2024.00461',\n",
       "  '10.1109/TAFFC.2024.3453443',\n",
       "  '10.1109/ISMAR59233.2023.00138',\n",
       "  '10.1109/ISMAR-Adjunct60411.2023.00137',\n",
       "  '10.3390/electronics12173595',\n",
       "  '10.1109/CVPRW59228.2023.00606',\n",
       "  '10.1109/CVPR52729.2023.00541',\n",
       "  '10.1109/CVPRW59228.2023.00607',\n",
       "  '10.1109/CVPRW59228.2023.00615',\n",
       "  '10.1109/CVPRW59228.2023.00611',\n",
       "  '10.1109/CVPRW59228.2023.00617',\n",
       "  '10.1109/CVPRW59228.2023.00620',\n",
       "  '10.48550/arXiv.2303.09145',\n",
       "  '10.1109/CVPRW59228.2023.00610',\n",
       "  '10.1109/CVPRW59228.2023.00626',\n",
       "  '10.1109/TAFFC.2022.3188390',\n",
       "  '10.1109/TCSS.2024.3478839',\n",
       "  '10.1109/CVPRW56347.2022.00263',\n",
       "  '10.1109/CVPR52688.2022.00401',\n",
       "  '10.1109/CVPR52688.2022.01204',\n",
       "  '10.1109/CVPRW56347.2022.00259',\n",
       "  '10.1109/ICCVW54120.2021.00408',\n",
       "  '10.2139/ssrn.4395579',\n",
       "  '10.1007/s00521-021-06012-8',\n",
       "  '10.1109/FG47880.2020.00126',\n",
       "  '10.1145/3343031.3351167',\n",
       "  '10.3103/S0735272719050042',\n",
       "  '10.3103/S1060992X18040021',\n",
       "  '10.1109/TSP.2018.8441443',\n",
       "  '10.1007/s11263-019-01158-4',\n",
       "  '10.1109/FG.2018.00020',\n",
       "  '10.1109/TAFFC.2017.2740923',\n",
       "  '10.1109/CVPRW.2017.248',\n",
       "  '10.1145/2993148.2997627',\n",
       "  '10.1016/j.imavis.2014.06.002',\n",
       "  '10.48550/arXiv.2403.11879',\n",
       "  '10.48550/arXiv.2403.10488',\n",
       "  '10.1007/978-3-031-25075-0_4'],\n",
       " ['10.1016/s0262-4079(24)01566-5',\n",
       "  '10.48550/arXiv.2304.07193',\n",
       "  '10.48550/arXiv.2301.08727',\n",
       "  '10.48550/arXiv.2205.12755',\n",
       "  '10.1109/CVPR52688.2022.01060',\n",
       "  '10.1109/CVPR52688.2022.01167',\n",
       "  '10.1109/TPAMI.2021.3054824',\n",
       "  '10.1109/IGARSS.2019.8900532',\n",
       "  '10.1109/CVPR.2019.00293',\n",
       "  '10.1007/s12045-017-0525-7',\n",
       "  '10.1109/CVPR.2018.00907',\n",
       "  '10.1109/CVPR.2017.634',\n",
       "  '10.1109/CVPR.2017.243',\n",
       "  '10.1109/cvpr.2016.90',\n",
       "  '10.1145/3065386',\n",
       "  '10.1109/CVPR.2009.5206848',\n",
       "  '10.1002/9781119991083.ch42',\n",
       "  '10.1109/4235.585893'],\n",
       " ['10.1016/j.jag.2023.103543',\n",
       "  '10.1109/ICCV51070.2023.00873',\n",
       "  '10.1109/ICCV51070.2023.00884',\n",
       "  '10.1109/CVPR52688.2022.00870',\n",
       "  '10.48550/arXiv.2205.14623',\n",
       "  '10.1109/CVPR52688.2022.00872',\n",
       "  '10.48550/arXiv.2203.16194',\n",
       "  '10.1109/CVPR46437.2021.00388',\n",
       "  '10.1109/WACV56688.2023.00512',\n",
       "  '10.1007/978-3-030-58536-5_33',\n",
       "  '10.1109/cvpr42600.2020.00652',\n",
       "  '10.1007/978-3-030-58536-5_24',\n",
       "  '10.1016/J.IMAGE.2018.12.002',\n",
       "  '10.1609/aaai.v32i1.12276',\n",
       "  '10.1109/CVPR.2018.00931',\n",
       "  '10.1109/CVPR.2017.179',\n",
       "  '10.1109/CVPR.2017.291',\n",
       "  '10.1109/CVPR.2016.85',\n",
       "  '10.1109/CVPR.2016.509',\n",
       "  '10.1109/CVPR.2016.438',\n",
       "  '10.1016/j.cviu.2015.02.008',\n",
       "  '10.1109/ICCV.2015.316',\n",
       "  '10.1007/978-3-642-33783-3_44',\n",
       "  '10.1145/1576246.1531330',\n",
       "  '10.1109/CVPR.2009.5206697',\n",
       "  '10.1007/978-3-540-24673-2_3',\n",
       "  '10.1109/TIP.2003.819861',\n",
       "  '10.1117/12.965761']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'https://openalex.org/W2963839617',\n",
       "  'doi': 'https://doi.org/10.1109/fg.2018.00020'},\n",
       " {'id': 'https://openalex.org/W2051297709',\n",
       "  'doi': 'https://doi.org/10.1016/j.imavis.2014.06.002'},\n",
       " {'id': 'https://openalex.org/W2745497104',\n",
       "  'doi': 'https://doi.org/10.1109/taffc.2017.2740923'},\n",
       " {'id': 'https://openalex.org/W2713788831',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw.2017.248'},\n",
       " {'id': 'https://openalex.org/W3155551469',\n",
       "  'doi': 'https://doi.org/10.1007/s00521-021-06012-8'},\n",
       " {'id': 'https://openalex.org/W4285250231',\n",
       "  'doi': 'https://doi.org/10.1109/taffc.2022.3188390'},\n",
       " {'id': 'https://openalex.org/W2798536775',\n",
       "  'doi': 'https://doi.org/10.1007/s11263-019-01158-4'},\n",
       " {'id': 'https://openalex.org/W3126750668',\n",
       "  'doi': 'https://doi.org/10.1109/fg47880.2020.00126'},\n",
       " {'id': 'https://openalex.org/W4292794012',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw56347.2022.00259'},\n",
       " {'id': 'https://openalex.org/W2548529926',\n",
       "  'doi': 'https://doi.org/10.1145/2993148.2997627'},\n",
       " {'id': 'https://openalex.org/W3209397829',\n",
       "  'doi': 'https://doi.org/10.1109/iccvw54120.2021.00408'},\n",
       " {'id': 'https://openalex.org/W4312391126',\n",
       "  'doi': 'https://doi.org/10.1109/cvpr52688.2022.01204'},\n",
       " {'id': 'https://openalex.org/W4312769845',\n",
       "  'doi': 'https://doi.org/10.1109/cvpr52688.2022.00401'},\n",
       " {'id': 'https://openalex.org/W4385815442',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00626'},\n",
       " {'id': 'https://openalex.org/W4386161521',\n",
       "  'doi': 'https://doi.org/10.3390/electronics12173595'},\n",
       " {'id': 'https://openalex.org/W4321353834',\n",
       "  'doi': 'https://doi.org/10.1007/978-3-031-25075-0_4'},\n",
       " {'id': 'https://openalex.org/W4402170672',\n",
       "  'doi': 'https://doi.org/10.1109/taffc.2024.3453443'},\n",
       " {'id': 'https://openalex.org/W4402916681',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw63382.2024.00483'},\n",
       " {'id': 'https://openalex.org/W4292829120',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw56347.2022.00263'},\n",
       " {'id': 'https://openalex.org/W4386066245',\n",
       "  'doi': 'https://doi.org/10.1109/cvpr52729.2023.00541'},\n",
       " {'id': 'https://openalex.org/W4385805174',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00611'},\n",
       " {'id': 'https://openalex.org/W4385805042',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00620'},\n",
       " {'id': 'https://openalex.org/W2914812561',\n",
       "  'doi': 'https://doi.org/10.3103/s1060992x18040021'},\n",
       " {'id': 'https://openalex.org/W4385801060',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00610'},\n",
       " {'id': 'https://openalex.org/W4385815492',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00607'},\n",
       " {'id': 'https://openalex.org/W4385805114',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00615'},\n",
       " {'id': 'https://openalex.org/W4389302500',\n",
       "  'doi': 'https://doi.org/10.1109/ismar-adjunct60411.2023.00137'},\n",
       " {'id': 'https://openalex.org/W4389314056',\n",
       "  'doi': 'https://doi.org/10.1109/ismar59233.2023.00138'},\n",
       " {'id': 'https://openalex.org/W4402916216',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw63382.2024.00461'},\n",
       " {'id': 'https://openalex.org/W4404056942',\n",
       "  'doi': 'https://doi.org/10.1109/tcss.2024.3478839'},\n",
       " {'id': 'https://openalex.org/W2954134384',\n",
       "  'doi': 'https://doi.org/10.3103/s0735272719050042'},\n",
       " {'id': 'https://openalex.org/W2981847730',\n",
       "  'doi': 'https://doi.org/10.1145/3343031.3351167'},\n",
       " {'id': 'https://openalex.org/W4327810694',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2303.09145'},\n",
       " {'id': 'https://openalex.org/W4385805069',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00606'},\n",
       " {'id': 'https://openalex.org/W2888726060',\n",
       "  'doi': 'https://doi.org/10.1109/tsp.2018.8441443'},\n",
       " {'id': 'https://openalex.org/W4392974064',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.11440'},\n",
       " {'id': 'https://openalex.org/W4393023350',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.10825'},\n",
       " {'id': 'https://openalex.org/W4393027396',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.12572'},\n",
       " {'id': 'https://openalex.org/W3195216867',\n",
       "  'doi': 'https://doi.org/10.2139/ssrn.4395579'},\n",
       " {'id': 'https://openalex.org/W4385804798',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw59228.2023.00617'},\n",
       " {'id': 'https://openalex.org/W4392933360',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.10488'},\n",
       " {'id': 'https://openalex.org/W4392970093',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.11450'},\n",
       " {'id': 'https://openalex.org/W4392971634',\n",
       "  'doi': 'https://doi.org/10.48550/arxiv.2403.11879'},\n",
       " {'id': 'https://openalex.org/W4402916570',\n",
       "  'doi': 'https://doi.org/10.1109/cvprw63382.2024.00490'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyalex\n",
    "pyalex.Works().filter_or(doi=dois).select(['id', 'doi']).get(per_page=len(dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "QueryError",
     "evalue": "'doi:10.7717/peerj.4375' is not a valid OpenAlex ID.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mQueryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpyalex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWorks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenalex_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoi:10.7717/peerj.4375\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_page\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyalex/api.py:551\u001b[39m, in \u001b[36mBaseOpenAlex.get\u001b[39m\u001b[34m(self, return_meta, page, per_page, cursor)\u001b[39m\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_params(\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m, page)\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_params(\u001b[33m\"\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m\"\u001b[39m, cursor)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m resp_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_meta:\n\u001b[32m    554\u001b[39m     warnings.warn(\n\u001b[32m    555\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_meta is deprecated, call .meta on the result\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    556\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    557\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyalex/api.py:522\u001b[39m, in \u001b[36mBaseOpenAlex._get_from_url\u001b[39m\u001b[34m(self, url, session)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    519\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(res.json()[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    520\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mquery parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res.json()[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    521\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m QueryError(res.json()[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    524\u001b[39m res.raise_for_status()\n\u001b[32m    525\u001b[39m res_json = res.json()\n",
      "\u001b[31mQueryError\u001b[39m: 'doi:10.7717/peerj.4375' is not a valid OpenAlex ID."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pyalex.Works().filter_or(openalex_id=['doi:10.7717/peerj.4375']).get(per_page=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing SemanticScholar API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to retrieve:\n",
    "Database papers:\n",
    "- paperId, externalIds, abstract, referenceCount\n",
    "\n",
    "Then get references for each of those papers:\n",
    "- paperId, externalIds, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/43 papers have abstract (2%)\n"
     ]
    }
   ],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "from semanticscholar.SemanticScholarException import ObjectNotFoundException\n",
    "\n",
    "def get_referenced_papers(sch: SemanticScholar, paper_id: str):\n",
    "    try:\n",
    "        raw = sch.get_paper_references(paper_id=paper_id, fields=['paperId', 'title', 'externalIds', 'abstract'], limit=1000)\n",
    "    except ObjectNotFoundException:\n",
    "        return [{}]\n",
    "    \n",
    "    res = []\n",
    "    for item in raw.items:\n",
    "        tmp = {\n",
    "            'originalPaperId': paper_id, \n",
    "            'paperId': item['citedPaper']['paperId'], \n",
    "            'title': item['citedPaper']['title'],\n",
    "            'abstract': item['citedPaper']['abstract']\n",
    "        }\n",
    "        if 'externalIds' in item['citedPaper'] and item['citedPaper']['externalIds'] is not None:\n",
    "            tmp.update({f'externalIds.{k}': v for k, v in item['citedPaper']['externalIds'].items()})\n",
    "        res.append(tmp)\n",
    "    return res\n",
    "\n",
    "sch = SemanticScholar()\n",
    "\n",
    "paper_id = df.iloc[7].paperId\n",
    "res = get_referenced_papers(sch, paper_id)\n",
    "df2 = pd.DataFrame(res)\n",
    "print(f'{(~df2.abstract.isna()).sum()}/{len(df2)} papers have abstract ({(1-df2.abstract.isna().sum()/len(df2))*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalPaperId</th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7433da608f60204cf0845fbd26cb83982e891875</td>\n",
       "      <td>f09e3845b9857b0c1a251bdf0f572eaa1519cc2f</td>\n",
       "      <td>Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7433da608f60204cf0845fbd26cb83982e891875</td>\n",
       "      <td>163b4d6a79a5b19af88b8585456363340d9efd04</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7433da608f60204cf0845fbd26cb83982e891875</td>\n",
       "      <td>a1e7b7a560b493c235eed2429cfbb9c12324ff4d</td>\n",
       "      <td>Scaling Adversarial Training to Large Perturbation Bounds</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7433da608f60204cf0845fbd26cb83982e891875</td>\n",
       "      <td>426b0ee8c723aa6086402f743d5cbb447622d9b6</td>\n",
       "      <td>When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7433da608f60204cf0845fbd26cb83982e891875</td>\n",
       "      <td>c570475cab4c8d0662144c4d414c17e776d39409</td>\n",
       "      <td>A Light Recipe to Train Robust Vision Transformers</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            originalPaperId                                   paperId                                                                                                            title abstract\n",
       "0  7433da608f60204cf0845fbd26cb83982e891875  f09e3845b9857b0c1a251bdf0f572eaa1519cc2f  Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks     None\n",
       "1  7433da608f60204cf0845fbd26cb83982e891875  163b4d6a79a5b19af88b8585456363340d9efd04                                                                                           GPT-4 Technical Report     None\n",
       "2  7433da608f60204cf0845fbd26cb83982e891875  a1e7b7a560b493c235eed2429cfbb9c12324ff4d                                                        Scaling Adversarial Training to Large Perturbation Bounds     None\n",
       "3  7433da608f60204cf0845fbd26cb83982e891875  426b0ee8c723aa6086402f743d5cbb447622d9b6                       When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture     None\n",
       "4  7433da608f60204cf0845fbd26cb83982e891875  c570475cab4c8d0662144c4d414c17e776d39409                                                               A Light Recipe to Train Robust Vision Transformers     None"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['originalPaperId', 'paperId', 'title', 'abstract']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Attackers can deceive neural networks by adding human imperceptive perturbations to their input data; this reveals the vulnerability and weak robustness of current deep-learning networks. Many attack techniques have been proposed to evaluate the model's robustness. Gradient-based attacks suffer from severely overestimating the robustness. This paper identifies that the relative error in calculated gradients caused by floating-point errors, including floating-point underflow and rounding errors, is a fundamental reason why gradient-based attacks fail to accurately assess the model's robustness. Although it is hard to eliminate the relative error in the gradients, we can control its effect on the gradient-based attacks. Correspondingly, we propose an efficient loss function by minimizing the detrimental impact of the floating-point errors on the attacks. Experimental results show that it is more efficient and reliable than other loss functions when examined across a wide range of defence mechanisms.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = sch.get_paper(paper_id='f09e3845b9857b0c1a251bdf0f572eaa1519cc2f')\n",
    "res.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': '7433da608f60204cf0845fbd26cb83982e891875', 'externalIds': {'DBLP': 'conf/cvpr/JainD24', 'DOI': '10.1109/CVPR52733.2024.02336', 'CorpusId': 272722903}, 'title': 'Towards Understanding and Improving Adversarial Robustness of Vision Transformers', 'abstract': 'Recent literature has demonstrated that vision transformers (VITs) exhibit superior performance compared to convolutional neural networks (CNNs). The majority of recent research on adversarial robustness, however, has predomi-nantly focused on CNNs. In this work, we bridge this gap by analyzing the effectiveness of existing attacks on VITs. We demonstrate that due to the softmax computations in every attention block in VITs, they are inherently vulnerable to floating point underflow errors. This can lead to a gradient masking effect resulting in suboptimal attack strength of well-known attacks, like PGD, Carlini and Wagner (CW) and GAMA. Motivated by this, we propose Adaptive Attention Scaling (AAS) attack that can automatically find the optimal scaling factors of pre-softmax outputs using gradient-based optimization. We show that the proposed simple strategy can be incorporated with any existing adversarial attacks as well as adversarial training methods and achieved improved performance. On VIT-B16, we demonstrate an improved attack strength of up to 2.2% on CIFAR10 and upto 2.9% on CIFAR100 by incorporating the proposed AAS attack with state-of-the-art single attack methods like GAMA attack. Further, we utilise the proposed AAS attack for every few epochs in existing adversarial training methods, which is termed as Adaptive Attention Scaling Adversarial Training (AAS-AT). On incorporating AAS-AT with existing methods, we outperform them on VITs over 1.3-3.5% on CIFAR10. We observe improved performance on ImageNet-100 as well.', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.02336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.02336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '151444035', 'name': 'Samyak Jain'}, {'authorId': '2283339824', 'name': 'Tanima Dutta'}], 'references': [{'paperId': 'f09e3845b9857b0c1a251bdf0f572eaa1519cc2f', 'title': 'Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks', 'abstract': None, 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52729.2023.00395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52729.2023.00395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2117164874', 'name': 'Yunrui Yu'}, {'authorId': '2153074991', 'name': 'Chengjie Xu'}]}, {'paperId': '163b4d6a79a5b19af88b8585456363340d9efd04', 'title': 'GPT-4 Technical Report', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2303.08774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2275249853', 'name': 'OpenAI Josh Achiam'}, {'authorId': '2275250875', 'name': 'Steven Adler'}, {'authorId': '144517868', 'name': 'Sandhini Agarwal'}, {'authorId': '2274773568', 'name': 'Lama Ahmad'}, {'authorId': '2258629', 'name': 'Ilge Akkaya'}, {'authorId': '2275244794', 'name': 'Florencia Leoni Aleman'}, {'authorId': '2275252021', 'name': 'Diogo Almeida'}, {'authorId': '2275252424', 'name': 'Janko Altenschmidt'}, {'authorId': '2275245579', 'name': 'Sam Altman'}, {'authorId': '2275246437', 'name': 'Shyamal Anadkat'}, {'authorId': '2275139370', 'name': 'Red Avila'}, {'authorId': '2256699302', 'name': 'Igor Babuschkin'}, {'authorId': '2054519183', 'name': 'S. Balaji'}, {'authorId': '2275251659', 'name': 'Valerie Balcom'}, {'authorId': '47626612', 'name': 'Paul Baltescu'}, {'authorId': '2275198557', 'name': 'Haim-ing Bao'}, {'authorId': '2275251620', 'name': 'Mo Bavarian'}, {'authorId': '2275245092', 'name': 'J. Belgum'}, {'authorId': '4689792', 'name': 'Irwan Bello'}, {'authorId': '2275245414', 'name': 'Jake Berdine'}, {'authorId': '2275245581', 'name': 'Gabriel Bernadett-Shapiro'}, {'authorId': '133740015', 'name': 'Christopher Berner'}, {'authorId': '2275251674', 'name': 'Lenny Bogdonoff'}, {'authorId': '2275246071', 'name': 'Oleg Boiko'}, {'authorId': '2275248137', 'name': 'Made-laine Boyd'}, {'authorId': '2275245419', 'name': 'Anna-Luisa Brakman'}, {'authorId': '2065151121', 'name': 'Greg Brockman'}, {'authorId': '2275219628', 'name': 'Tim Brooks'}, {'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '2146257251', 'name': 'Kevin Button'}, {'authorId': '2275157286', 'name': 'Trevor Cai'}, {'authorId': '2274782053', 'name': 'Rosie Campbell'}, {'authorId': '2275245404', 'name': 'Andrew Cann'}, {'authorId': '2275246368', 'name': 'Brittany Carey'}, {'authorId': '2275120298', 'name': 'Chelsea Carlson'}, {'authorId': '144114446', 'name': 'Rory Carmichael'}, {'authorId': '1466431052', 'name': 'Brooke Chan'}, {'authorId': '2275545855', 'name': 'Che Chang'}, {'authorId': '2057091285', 'name': 'Fotis Chantzis'}, {'authorId': '2253841704', 'name': 'Derek Chen'}, {'authorId': '2275188918', 'name': 'Sully Chen'}, {'authorId': '2275179180', 'name': 'Ruby Chen'}, {'authorId': '2275289833', 'name': 'Jason Chen'}, {'authorId': '2108828435', 'name': 'Mark Chen'}, {'authorId': '1490681878', 'name': 'Benjamin Chess'}, {'authorId': '2275251158', 'name': 'Chester Cho'}, {'authorId': '2276186593', 'name': 'Casey Chu'}, {'authorId': '2275839391', 'name': 'Hyung Won Chung'}, {'authorId': '2275231534', 'name': 'Dave Cummings'}, {'authorId': '49645091', 'name': 'Jeremiah Currier'}, {'authorId': '2276187456', 'name': 'Yunxing Dai'}, {'authorId': '2275251205', 'name': 'C. Decareaux'}, {'authorId': '2275244920', 'name': 'Thomas Degry'}, {'authorId': '2275247090', 'name': 'Noah Deutsch'}, {'authorId': '2275251200', 'name': 'Damien Deville'}, {'authorId': '2275244298', 'name': 'Arka Dhar'}, {'authorId': '35363891', 'name': 'David Dohan'}, {'authorId': '2275252295', 'name': 'Steve Dowling'}, {'authorId': '2275245491', 'name': 'Sheila Dunning'}, {'authorId': '66821245', 'name': 'Adrien Ecoffet'}, {'authorId': '2275245457', 'name': 'Atty Eleti'}, {'authorId': '2146257131', 'name': 'Tyna Eloundou'}, {'authorId': '2065430571', 'name': 'David Farhi'}, {'authorId': '2096916416', 'name': 'L. Fedus'}, {'authorId': '2275249996', 'name': 'Niko Felix'}, {'authorId': '2275245820', 'name': \"Sim'on Posada Fishman\"}, {'authorId': '2275244914', 'name': 'Juston Forte'}, {'authorId': '2275251173', 'name': 'Is-abella Fulford'}, {'authorId': '2027599537', 'name': 'Leo Gao'}, {'authorId': '2275200811', 'name': 'Elie Georges'}, {'authorId': '2275254804', 'name': 'C. Gibson'}, {'authorId': '2275144649', 'name': 'Vik Goel'}, {'authorId': '2325028819', 'name': 'Tarun Gogineni'}, {'authorId': '2261041177', 'name': 'Gabriel Goh'}, {'authorId': '2158366935', 'name': 'Raphael Gontijo-Lopes'}, {'authorId': '2265066144', 'name': 'Jonathan Gordon'}, {'authorId': '2275250003', 'name': 'Morgan Grafstein'}, {'authorId': '145565184', 'name': 'Scott Gray'}, {'authorId': '2275247307', 'name': 'Ryan Greene'}, {'authorId': '2275137274', 'name': 'Joshua Gross'}, {'authorId': '2253699903', 'name': 'S. Gu'}, {'authorId': '2276101257', 'name': 'Yufei Guo'}, {'authorId': '2004021329', 'name': 'Chris Hallacy'}, {'authorId': '2275540338', 'name': 'Jesse Han'}, {'authorId': '2275295848', 'name': 'Jeff Harris'}, {'authorId': '2275226809', 'name': 'Yuchen He'}, {'authorId': '2275245527', 'name': 'Mike Heaton'}, {'authorId': '2151087994', 'name': 'Jo-hannes Heidecke'}, {'authorId': '2242286342', 'name': 'Chris Hesse'}, {'authorId': '2226452668', 'name': 'Alan Hickey'}, {'authorId': '2275246148', 'name': 'Wade Hickey'}, {'authorId': '2275245339', 'name': 'Peter Hoeschele'}, {'authorId': '103681415', 'name': 'Brandon Houghton'}, {'authorId': '2275214107', 'name': 'Kenny Hsu'}, {'authorId': '2275210604', 'name': 'Shengli Hu'}, {'authorId': '2275777049', 'name': 'Xin Hu'}, {'authorId': '39378983', 'name': 'Joost Huizinga'}, {'authorId': '2276187117', 'name': 'Shantanu Jain'}, {'authorId': '2171110177', 'name': 'Shawn Jain'}, {'authorId': '2151094350', 'name': 'Joanne Jang'}, {'authorId': '2253471334', 'name': 'Angela Jiang'}, {'authorId': '2275172062', 'name': 'Roger Jiang'}, {'authorId': '2275752035', 'name': 'Haozhun Jin'}, {'authorId': '2275203081', 'name': 'Denny Jin'}, {'authorId': '2275250083', 'name': 'Shino Jomoto'}, {'authorId': '2275247096', 'name': 'B. Jonn'}, {'authorId': '35450887', 'name': 'Heewoo Jun'}, {'authorId': '2403754', 'name': 'Tomer Kaftan'}, {'authorId': '2275230678', 'name': 'Lukasz Kaiser'}, {'authorId': '2275169038', 'name': 'Ali Kamali'}, {'authorId': '3151440', 'name': 'I. Kanitscheider'}, {'authorId': '2844898', 'name': 'N. Keskar'}, {'authorId': '2152264064', 'name': 'Tabarak Khan'}, {'authorId': '2275246102', 'name': 'Logan Kilpatrick'}, {'authorId': '2260346092', 'name': 'Jong Wook Kim'}, {'authorId': '2149054292', 'name': 'Christina Kim'}, {'authorId': '2275296777', 'name': 'Yongjik Kim'}, {'authorId': '2275112980', 'name': 'Hendrik Kirchner'}, {'authorId': '51131802', 'name': 'J. Kiros'}, {'authorId': '2146257375', 'name': 'Matthew Knight'}, {'authorId': '1485556711', 'name': 'Daniel Kokotajlo'}, {'authorId': '2275246094', 'name': 'Lukasz Kondraciuk'}, {'authorId': '1666171360', 'name': 'Andrew Kondrich'}, {'authorId': '2275252322', 'name': 'Aris Konstantinidis'}, {'authorId': '2275245594', 'name': 'Kyle Kosic'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '2275229877', 'name': 'Vishal Kuo'}, {'authorId': '2275247085', 'name': 'Michael Lampe'}, {'authorId': '2275246287', 'name': 'Ikai Lan'}, {'authorId': '2274915115', 'name': 'Teddy Lee'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '52152632', 'name': 'Jade Leung'}, {'authorId': '2275256930', 'name': 'Daniel Levy'}, {'authorId': '2275285124', 'name': 'Chak Li'}, {'authorId': '2275176375', 'name': 'Rachel Lim'}, {'authorId': '2275759230', 'name': 'Molly Lin'}, {'authorId': '2253840098', 'name': 'Stephanie Lin'}, {'authorId': '1380985420', 'name': 'Ma-teusz Litwin'}, {'authorId': '2275248327', 'name': 'Theresa Lopez'}, {'authorId': '2257272397', 'name': 'Ryan Lowe'}, {'authorId': '2275245628', 'name': 'Patricia Lue'}, {'authorId': '119341078', 'name': 'A. Makanju'}, {'authorId': '2275245649', 'name': 'Kim Malfacini'}, {'authorId': '46430291', 'name': 'Sam Manning'}, {'authorId': '14113256', 'name': 'Todor Markov'}, {'authorId': '2275245336', 'name': 'Yaniv Markovski'}, {'authorId': '2114362965', 'name': 'Bianca Martin'}, {'authorId': '2275231822', 'name': 'Katie Mayer'}, {'authorId': '2275247045', 'name': 'Andrew Mayne'}, {'authorId': '39593364', 'name': 'Bob McGrew'}, {'authorId': '2047820455', 'name': 'S. McKinney'}, {'authorId': '3028785', 'name': 'C. McLeavey'}, {'authorId': '2274772421', 'name': 'Paul McMillan'}, {'authorId': '2275234856', 'name': 'Jake McNeil'}, {'authorId': '2275210659', 'name': 'David Medina'}, {'authorId': '2275132306', 'name': 'Aalok Mehta'}, {'authorId': '10698483', 'name': 'Jacob Menick'}, {'authorId': '2275246330', 'name': 'Luke Metz'}, {'authorId': '2275252694', 'name': 'Andrey Mishchenko'}, {'authorId': '2051714782', 'name': 'Pamela Mishkin'}, {'authorId': '2275245453', 'name': 'Vinnie Monaco'}, {'authorId': '1404556973', 'name': 'Evan Morikawa'}, {'authorId': '3407880', 'name': 'Daniel P. Mossing'}, {'authorId': '2275154456', 'name': 'Tong Mu'}, {'authorId': '2117715631', 'name': 'Mira Murati'}, {'authorId': '147746767', 'name': 'O. Murk'}, {'authorId': '2275246116', 'name': \"David M'ely\"}, {'authorId': '3422774', 'name': 'Ashvin Nair'}, {'authorId': '7406311', 'name': 'Reiichiro Nakano'}, {'authorId': '2057426488', 'name': 'Rajeev Nayak'}, {'authorId': '2072676', 'name': 'Arvind Neelakantan'}, {'authorId': '2273886618', 'name': 'Richard Ngo'}, {'authorId': '2275115983', 'name': 'Hyeonwoo Noh'}, {'authorId': '2228518120', 'name': 'Ouyang Long'}, {'authorId': '1435765036', 'name': \"Cullen O'Keefe\"}, {'authorId': '2713380', 'name': 'J. Pachocki'}, {'authorId': '34800652', 'name': 'Alex Paino'}, {'authorId': '2275244652', 'name': 'Joe Palermo'}, {'authorId': '2275246178', 'name': 'Ashley Pantuliano'}, {'authorId': '50213542', 'name': 'Giambattista Parascandolo'}, {'authorId': '2275245818', 'name': 'Joel Parish'}, {'authorId': '2275245435', 'name': 'Emy Parparita'}, {'authorId': '2274774915', 'name': 'Alexandre Passos'}, {'authorId': '2068123790', 'name': 'Mikhail Pavlov'}, {'authorId': '2275125663', 'name': 'Andrew Peng'}, {'authorId': '2275245529', 'name': 'Adam Perelman'}, {'authorId': '2275250075', 'name': 'Filipe de Avila Belbute Peres'}, {'authorId': '2136008481', 'name': 'Michael Petrov'}, {'authorId': '1463773776', 'name': 'Henrique Pondé de Oliveira Pinto'}, {'authorId': '2275246346', 'name': 'Michael Pokorny'}, {'authorId': '2275246814', 'name': 'Michelle Pokrass'}, {'authorId': '144401061', 'name': 'Vitchyr H. Pong'}, {'authorId': '2275150061', 'name': 'Tolly Powell'}, {'authorId': '146162186', 'name': 'Alethea Power'}, {'authorId': '2151088845', 'name': 'Boris Power'}, {'authorId': '2275243930', 'name': 'Elizabeth Proehl'}, {'authorId': '2285654208', 'name': 'Raul Puri'}, {'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '2275178294', 'name': 'Jack W. Rae'}, {'authorId': '2261024614', 'name': 'Aditya Ramesh'}, {'authorId': '2275225165', 'name': 'Cameron Raymond'}, {'authorId': '2275252438', 'name': 'Francis Real'}, {'authorId': '2275252095', 'name': 'Kendra Rimbach'}, {'authorId': '2275207240', 'name': 'Carl Ross'}, {'authorId': '11150265', 'name': 'Bob Rotsted'}, {'authorId': '2275250007', 'name': 'Henri Roussez'}, {'authorId': '2260406867', 'name': 'Nick Ryder'}, {'authorId': '47204843', 'name': 'M. Saltarelli'}, {'authorId': '2275246803', 'name': 'Ted Sanders'}, {'authorId': '2852106', 'name': 'Shibani Santurkar'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2275265666', 'name': 'Heather Schmidt'}, {'authorId': '2252874293', 'name': 'David Schnurr'}, {'authorId': '47971768', 'name': 'John Schulman'}, {'authorId': '2196579', 'name': 'Daniel Selsam'}, {'authorId': '2275244711', 'name': 'Kyla Sheppard'}, {'authorId': '102475503', 'name': 'Toki Sherbakov'}, {'authorId': '2275246834', 'name': 'Jessica Shieh'}, {'authorId': '118335789', 'name': 'S. Shoker'}, {'authorId': '67311962', 'name': 'Pranav Shyam'}, {'authorId': '2700360', 'name': 'Szymon Sidor'}, {'authorId': '2064673055', 'name': 'Eric Sigler'}, {'authorId': '2151735251', 'name': 'Maddie Simens'}, {'authorId': '2275252299', 'name': 'Jordan Sitkin'}, {'authorId': '2117680841', 'name': 'Katarina Slama'}, {'authorId': '103422608', 'name': 'Ian Sohl'}, {'authorId': '2901424', 'name': 'Benjamin Sokolowsky'}, {'authorId': '2307592658', 'name': 'Yang Song'}, {'authorId': '2275245668', 'name': 'Natalie Staudacher'}, {'authorId': '9927844', 'name': 'F. Such'}, {'authorId': '2275252251', 'name': 'Natalie Summers'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '2275750817', 'name': 'Jie Tang'}, {'authorId': '145950540', 'name': 'N. Tezak'}, {'authorId': '2151289331', 'name': 'Madeleine Thompson'}, {'authorId': '2275252092', 'name': 'Phil Tillet'}, {'authorId': '2267339677', 'name': 'Amin Tootoonchian'}, {'authorId': '2275249879', 'name': 'Elizabeth Tseng'}, {'authorId': '2275249709', 'name': 'Preston Tuggle'}, {'authorId': '2275244171', 'name': 'Nick Turley'}, {'authorId': '2065005836', 'name': 'Jerry Tworek'}, {'authorId': '2275203310', 'name': \"Juan Felipe Cer'on Uribe\"}, {'authorId': '2275244586', 'name': 'Andrea Vallone'}, {'authorId': '2275245661', 'name': 'Arun Vijayvergiya'}, {'authorId': '153387869', 'name': 'Chelsea Voss'}, {'authorId': '2275245962', 'name': 'Carroll L. Wainwright'}, {'authorId': '2275528432', 'name': 'Justin Jay Wang'}, {'authorId': '2275540420', 'name': 'Alvin Wang'}, {'authorId': '2275189326', 'name': 'Ben Wang'}, {'authorId': '2170081200', 'name': 'Jonathan Ward'}, {'authorId': '2253952872', 'name': 'Jason Wei'}, {'authorId': '2275244218', 'name': 'CJ Weinmann'}, {'authorId': '2275245663', 'name': 'Akila Welihinda'}, {'authorId': '2930640', 'name': 'P. Welinder'}, {'authorId': '2275139180', 'name': 'Jiayi Weng'}, {'authorId': '2065741038', 'name': 'Lilian Weng'}, {'authorId': '2275252154', 'name': 'Matt Wiethoff'}, {'authorId': '2275249733', 'name': 'Dave Willner'}, {'authorId': '2059411355', 'name': 'Clemens Winter'}, {'authorId': '2275244177', 'name': 'Samuel Wolrich'}, {'authorId': '2275225207', 'name': 'Hannah Wong'}, {'authorId': '2275245771', 'name': 'Lauren Workman'}, {'authorId': '2275299848', 'name': 'Sherwin Wu'}, {'authorId': '2274911253', 'name': 'Jeff Wu'}, {'authorId': '2307456650', 'name': 'Michael Wu'}, {'authorId': '2275190169', 'name': 'Kai Xiao'}, {'authorId': '2275452480', 'name': 'Tao Xu'}, {'authorId': '2275310096', 'name': 'Sarah Yoo'}, {'authorId': '2275593618', 'name': 'Kevin Yu'}, {'authorId': '2275194186', 'name': 'Qim-ing Yuan'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '49629836', 'name': 'Rowan Zellers'}, {'authorId': '2262080679', 'name': 'Chong Zhang'}, {'authorId': '2275288889', 'name': 'Marvin Zhang'}, {'authorId': '2275545682', 'name': 'Shengjia Zhao'}, {'authorId': '2275257857', 'name': 'Tianhao Zheng'}, {'authorId': '2275201537', 'name': 'Juntang Zhuang'}, {'authorId': '2275245715', 'name': 'William Zhuk'}, {'authorId': '2368067', 'name': 'Barret Zoph'}]}, {'paperId': 'a1e7b7a560b493c235eed2429cfbb9c12324ff4d', 'title': 'Scaling Adversarial Training to Large Perturbation Bounds', 'abstract': None, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2210.09852', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2210.09852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '1398341975', 'name': 'Sravanti Addepalli'}, {'authorId': '151444035', 'name': 'Samyak Jain'}, {'authorId': '1602820179', 'name': 'Gaurang Sriramanan'}, {'authorId': '144682140', 'name': 'R. Venkatesh Babu'}]}, {'paperId': '426b0ee8c723aa6086402f743d5cbb447622d9b6', 'title': 'When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture', 'abstract': None, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2210.07540', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2210.07540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '145566197', 'name': 'Yi Mo'}, {'authorId': '1492154834', 'name': 'Dongxian Wu'}, {'authorId': '2115568564', 'name': 'Yifei Wang'}, {'authorId': '2527106', 'name': 'Yiwen Guo'}, {'authorId': '2115869684', 'name': 'Yisen Wang'}]}, {'paperId': 'c570475cab4c8d0662144c4d414c17e776d39409', 'title': 'A Light Recipe to Train Robust Vision Transformers', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2209.07399', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2209.07399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2175939276', 'name': 'Edoardo Debenedetti'}, {'authorId': '3482535', 'name': 'Vikash Sehwag'}, {'authorId': '143615345', 'name': 'Prateek Mittal'}]}, {'paperId': '35c0800e657faa18cf3fc3629bdbeafbb976b006', 'title': 'Are Transformers More Robust Than CNNs?', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2111.05464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '48442730', 'name': 'Yutong Bai'}, {'authorId': '10407760', 'name': 'Jieru Mei'}, {'authorId': '145081362', 'name': 'A. Yuille'}, {'authorId': '3011497', 'name': 'Cihang Xie'}]}, {'paperId': '9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342', 'title': 'Improving Robustness using Generated Data', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2110.09468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2071666', 'name': 'Sven Gowal'}, {'authorId': '8478422', 'name': 'Sylvestre-Alvise Rebuffi'}, {'authorId': '8792285', 'name': 'Olivia Wiles'}, {'authorId': '3205302', 'name': 'Florian Stimberg'}, {'authorId': '2792016', 'name': 'D. A. Calian'}, {'authorId': '144467964', 'name': 'Timothy Mann'}]}, {'paperId': '3c2622daa8a658d5c85ea9869cb460a70b0f878d', 'title': 'Towards Transferable Adversarial Attacks on Vision Transformers', 'abstract': 'Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance.', 'openAccessPdf': {'url': 'https://ojs.aaai.org/index.php/AAAI/article/download/20169/19928', 'status': 'GOLD', 'license': None, 'disclaimer': 'Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2109.04176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': None, 'name': 'Zhipeng Wei'}, {'authorId': '2108536365', 'name': 'Jingjing Chen'}, {'authorId': '2126058635', 'name': 'Micah Goldblum'}, {'authorId': '3099139', 'name': 'Zuxuan Wu'}, {'authorId': '1962083', 'name': 'T. Goldstein'}, {'authorId': '1717861', 'name': 'Yu-Gang Jiang'}]}, {'paperId': '0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9', 'title': 'On Improving Adversarial Transferability of Vision Transformers', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2106.04169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '40894826', 'name': 'Muzammal Naseer'}, {'authorId': '48430646', 'name': 'Kanchana Ranasinghe'}, {'authorId': '2111180748', 'name': 'Salman Siddique Khan'}, {'authorId': '2358803', 'name': 'F. Khan'}, {'authorId': '29905643', 'name': 'F. Porikli'}]}, {'paperId': '43e51c1bfd69df518e2907f7a955e485985ba423', 'title': 'On the Robustness of Vision Transformers to Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2104.02610', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2104.02610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '145460048', 'name': 'Kaleel Mahmood'}, {'authorId': '9461237', 'name': 'Rigel Mahmood'}, {'authorId': '144534186', 'name': 'Marten van Dijk'}]}, {'paperId': 'd2a3bb6356d439146cd8d8e72dc728a1e3d93e7f', 'title': 'Understanding Robustness of Transformers for Image Classification', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2103.14586', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.14586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '1798880', 'name': 'Srinadh Bhojanapalli'}, {'authorId': '38534744', 'name': 'Ayan Chakrabarti'}, {'authorId': '1752652', 'name': 'Daniel Glasner'}, {'authorId': '2108467824', 'name': 'Daliang Li'}, {'authorId': '2465270', 'name': 'Thomas Unterthiner'}, {'authorId': '2799898', 'name': 'Andreas Veit'}]}, {'paperId': '152d454e54112742d5f9e6de64f25387c84bcbbf', 'title': 'Evaluating the Robustness of Geometry-Aware Instance-Reweighted Adversarial Training', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.01914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2702886', 'name': 'Dorjan Hitaj'}, {'authorId': '1403242401', 'name': 'Giulio Pagnotta'}, {'authorId': '11269472', 'name': 'I. Masi'}, {'authorId': '1686516', 'name': 'L. Mancini'}]}, {'paperId': 'ad7ddcc14984caae308c397f1a589aae75d4ab71', 'title': 'Training data-efficient image transformers & distillation through attention', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2012.12877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2113243762', 'name': 'Hugo Touvron'}, {'authorId': '51021910', 'name': 'M. Cord'}, {'authorId': '3271933', 'name': 'Matthijs Douze'}, {'authorId': '1403239967', 'name': 'Francisco Massa'}, {'authorId': '3469062', 'name': 'Alexandre Sablayrolles'}, {'authorId': '2065248680', 'name': \"Herv'e J'egou\"}]}, {'paperId': '5fc631cc1dbb5383a7751c39d3f8f3548e8d6ed2', 'title': 'Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2011.14969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '1602820179', 'name': 'Gaurang Sriramanan'}, {'authorId': '1398341975', 'name': 'Sravanti Addepalli'}, {'authorId': '30553248', 'name': 'Arya Baburaj'}, {'authorId': '144682140', 'name': 'R. Venkatesh Babu'}]}, {'paperId': '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', 'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2010.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2841331', 'name': 'Alexey Dosovitskiy'}, {'authorId': '39611591', 'name': 'Lucas Beyer'}, {'authorId': '144629422', 'name': 'Alexander Kolesnikov'}, {'authorId': '3319373', 'name': 'Dirk Weissenborn'}, {'authorId': '2743563', 'name': 'Xiaohua Zhai'}, {'authorId': '2465270', 'name': 'Thomas Unterthiner'}, {'authorId': '2274215058', 'name': 'Mostafa Dehghani'}, {'authorId': '46352821', 'name': 'Matthias Minderer'}, {'authorId': '2280399', 'name': 'G. Heigold'}, {'authorId': '1802148', 'name': 'S. Gelly'}, {'authorId': '39328010', 'name': 'Jakob Uszkoreit'}, {'authorId': '2815290', 'name': 'N. Houlsby'}]}, {'paperId': '99a599d8fe56529f47e78243ed61250190f96196', 'title': 'Geometry-aware Instance-reweighted Adversarial Training', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2010.01736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '47539929', 'name': 'Jingfeng Zhang'}, {'authorId': '2143475848', 'name': 'Jianing Zhu'}, {'authorId': '47537639', 'name': 'Gang Niu'}, {'authorId': '2087238859', 'name': 'Bo Han'}, {'authorId': '67154907', 'name': 'Masashi Sugiyama'}, {'authorId': '1744045', 'name': 'M. Kankanhalli'}]}, {'paperId': '473a854a939eca4bf39420ff496f8e24e223d460', 'title': 'Perceptual Adversarial Robustness: Defense Against Unseen Threat Models', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2006.12655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '114339785', 'name': 'Cassidy Laidlaw'}, {'authorId': '144190575', 'name': 'Sahil Singla'}, {'authorId': '34389431', 'name': 'S. Feizi'}]}, {'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0', 'title': 'Language Models are Few-Shot Learners', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2005.14165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '39849748', 'name': 'Nick Ryder'}, {'authorId': '2065894334', 'name': 'Melanie Subbiah'}, {'authorId': '152724169', 'name': 'J. Kaplan'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '2072676', 'name': 'Arvind Neelakantan'}, {'authorId': '67311962', 'name': 'Pranav Shyam'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '144517868', 'name': 'Sandhini Agarwal'}, {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '103143311', 'name': 'T. Henighan'}, {'authorId': '48422824', 'name': 'R. Child'}, {'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '2052152920', 'name': 'Daniel M. Ziegler'}, {'authorId': '49387725', 'name': 'Jeff Wu'}, {'authorId': '2059411355', 'name': 'Clemens Winter'}, {'authorId': '144239765', 'name': 'Christopher Hesse'}, {'authorId': '2108828435', 'name': 'Mark Chen'}, {'authorId': '2064673055', 'name': 'Eric Sigler'}, {'authorId': '1380985420', 'name': 'Ma-teusz Litwin'}, {'authorId': '145565184', 'name': 'Scott Gray'}, {'authorId': '1490681878', 'name': 'Benjamin Chess'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '133740015', 'name': 'Christopher Berner'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}, {'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]}, {'paperId': '764eff31d9596033859895d9513b838d2c57a6fb', 'title': 'Improving Adversarial Robustness Requires Revisiting Misclassified Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1919541', 'name': 'Yisen Wang'}, {'authorId': '1838855', 'name': 'Difan Zou'}, {'authorId': '2882166', 'name': 'Jinfeng Yi'}, {'authorId': '145148600', 'name': 'J. Bailey'}, {'authorId': '9576855', 'name': 'Xingjun Ma'}, {'authorId': '9937103', 'name': 'Quanquan Gu'}]}, {'paperId': '9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5', 'title': 'Adversarial Weight Perturbation Helps Robust Generalization', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1492154834', 'name': 'Dongxian Wu'}, {'authorId': '3085483', 'name': 'Shutao Xia'}, {'authorId': '1919541', 'name': 'Yisen Wang'}]}, {'paperId': '18939eadc9c4460c8385e0591cde214a1ead067b', 'title': 'Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2003.01690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '39171784', 'name': 'Francesco Croce'}, {'authorId': '143610806', 'name': 'Matthias Hein'}]}, {'paperId': '58c143069444c7dff4be53531a47efefc40be497', 'title': 'On Adaptive Attacks to Adversarial Example Defenses', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2002.08347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2444919', 'name': 'Florian Tramèr'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '40634590', 'name': 'Wieland Brendel'}, {'authorId': '143826246', 'name': 'A. Madry'}]}, {'paperId': 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', 'title': 'On Evaluating Adversarial Robustness', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1902.06705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '38939786', 'name': 'Anish Athalye'}, {'authorId': '1967156', 'name': 'Nicolas Papernot'}, {'authorId': '40634590', 'name': 'Wieland Brendel'}, {'authorId': '19237612', 'name': 'Jonas Rauber'}, {'authorId': '2754804', 'name': 'Dimitris Tsipras'}, {'authorId': '153440022', 'name': 'I. Goodfellow'}, {'authorId': '143826246', 'name': 'A. Madry'}, {'authorId': '145714153', 'name': 'Alexey Kurakin'}]}, {'paperId': '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'title': 'Theoretically Principled Trade-off between Robustness and Accuracy', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1901.08573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '40975176', 'name': 'Hongyang Zhang'}, {'authorId': '29001000', 'name': 'Yaodong Yu'}, {'authorId': '2784735', 'name': 'Jiantao Jiao'}, {'authorId': '143977260', 'name': 'E. Xing'}, {'authorId': '1701847', 'name': 'L. Ghaoui'}, {'authorId': '1694621', 'name': 'Michael I. Jordan'}]}, {'paperId': '8b9127bee0f7d109da2672ba06d0f39a5a60335a', 'title': 'Thermometer Encoding: One Hot Way To Resist Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '47619311', 'name': 'Jacob Buckman'}, {'authorId': '39788470', 'name': 'Aurko Roy'}, {'authorId': '2402716', 'name': 'Colin Raffel'}, {'authorId': '153440022', 'name': 'I. Goodfellow'}]}, {'paperId': '651adaa058f821a890f2c5d1053d69eb481a8352', 'title': 'Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1802.00420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '38939786', 'name': 'Anish Athalye'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]}, {'paperId': 'c468bbde6a22d961829e1970e6ad5795e05418d1', 'title': 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1801.03924', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1801.03924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2844849', 'name': 'Richard Zhang'}, {'authorId': '2094770', 'name': 'Phillip Isola'}, {'authorId': '1763086', 'name': 'Alexei A. Efros'}, {'authorId': '2177801', 'name': 'Eli Shechtman'}, {'authorId': '39231399', 'name': 'Oliver Wang'}]}, {'paperId': '9a089c56eec68df722b2a5a52727143aacdc2532', 'title': 'Mitigating adversarial effects through randomization', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1711.01991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '3011497', 'name': 'Cihang Xie'}, {'authorId': None, 'name': 'Jianyu Wang'}, {'authorId': '2852303', 'name': 'Zhishuai Zhang'}, {'authorId': '145888238', 'name': 'Zhou Ren'}, {'authorId': '145081362', 'name': 'A. Yuille'}]}, {'paperId': 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', 'title': 'PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1710.10766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '115504645', 'name': 'Yang Song'}, {'authorId': '3307885', 'name': 'Taesup Kim'}, {'authorId': '2388416', 'name': 'Sebastian Nowozin'}, {'authorId': '2490652', 'name': 'Stefano Ermon'}, {'authorId': '1684887', 'name': 'Nate Kushman'}]}, {'paperId': '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'title': 'Towards Deep Learning Models Resistant to Adversarial Attacks', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1706.06083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '143826246', 'name': 'A. Madry'}, {'authorId': '17775913', 'name': 'Aleksandar Makelov'}, {'authorId': '152772922', 'name': 'Ludwig Schmidt'}, {'authorId': '2754804', 'name': 'Dimitris Tsipras'}, {'authorId': '2869958', 'name': 'Adrian Vladu'}]}, {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'title': 'Attention is All you Need', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1706.03762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '40348417', 'name': 'Ashish Vaswani'}, {'authorId': '1846258', 'name': 'Noam M. Shazeer'}, {'authorId': '3877127', 'name': 'Niki Parmar'}, {'authorId': '39328010', 'name': 'Jakob Uszkoreit'}, {'authorId': '145024664', 'name': 'Llion Jones'}, {'authorId': '19177000', 'name': 'Aidan N. Gomez'}, {'authorId': '40527594', 'name': 'Lukasz Kaiser'}, {'authorId': '3443442', 'name': 'Illia Polosukhin'}]}, {'paperId': 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'title': 'Towards Evaluating the Robustness of Neural Networks', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1608.04644', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1608.04644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]}, {'paperId': '975b6ec05f04662a967af8c7504b7f552a0ee0bd', 'title': 'Defensive Distillation is Not Robust to Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1607.04311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]}, {'paperId': 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'title': 'Explaining and Harnessing Adversarial Examples', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1412.6572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '153440022', 'name': 'I. Goodfellow'}, {'authorId': '1789737', 'name': 'Jonathon Shlens'}, {'authorId': '2574060', 'name': 'Christian Szegedy'}]}, {'paperId': 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'title': 'Intriguing properties of neural networks', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1312.6199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2574060', 'name': 'Christian Szegedy'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '1761978', 'name': 'D. Erhan'}, {'authorId': '153440022', 'name': 'I. Goodfellow'}, {'authorId': '2276554', 'name': 'R. Fergus'}]}, {'paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'title': 'ImageNet: A large-scale hierarchical image database', 'abstract': None, 'openAccessPdf': {'url': 'http://www.image-net.org/papers/imagenet_cvpr09.pdf', 'status': 'CLOSED', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2009.5206848?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2009.5206848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '153302678', 'name': 'Jia Deng'}, {'authorId': '144847596', 'name': 'Wei Dong'}, {'authorId': '2166511', 'name': 'R. Socher'}, {'authorId': '2040091191', 'name': 'Li-Jia Li'}, {'authorId': '94451829', 'name': 'K. Li'}, {'authorId': '48004138', 'name': 'Li Fei-Fei'}]}, {'paperId': 'f2e7d8f6bed2bae7311d3e1788d6872387e8bb86', 'title': 'Do Perceptually Aligned Gradients Imply Adversarial Robustness?', 'abstract': None, 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2207.11378', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2207.11378?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2207.11378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2061747201', 'name': 'Roy Ganz'}, {'authorId': '2047309422', 'name': 'Bahjat Kawar'}, {'authorId': '1753908', 'name': 'Michael Elad'}]}, {'paperId': '0def290ae38abb4a04e35e0bcdc86b71d237f494', 'title': 'On the Adversarial Robustness of Vision Transformers', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.15670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '1380245502', 'name': 'Rulin Shao'}, {'authorId': '2987927', 'name': 'Zhouxing Shi'}, {'authorId': '2882166', 'name': 'Jinfeng Yi'}, {'authorId': '153191489', 'name': 'Pin-Yu Chen'}, {'authorId': '1793529', 'name': 'Cho-Jui Hsieh'}]}, {'paperId': 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', 'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows', 'abstract': None, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2103.14030', 'status': 'GREEN', 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.14030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '2109371439', 'name': 'Ze Liu'}, {'authorId': '51091819', 'name': 'Yutong Lin'}, {'authorId': '2112823372', 'name': 'Yue Cao'}, {'authorId': '1823518756', 'name': 'Han Hu'}, {'authorId': '2107995927', 'name': 'Yixuan Wei'}, {'authorId': '2148904543', 'name': 'Zheng Zhang'}, {'authorId': '145676588', 'name': 'Stephen Lin'}, {'authorId': '2261753424', 'name': 'B. Guo'}]}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1810.04805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '39172707', 'name': 'Jacob Devlin'}, {'authorId': '1744179', 'name': 'Ming-Wei Chang'}, {'authorId': '2544107', 'name': 'Kenton Lee'}, {'authorId': '3259253', 'name': 'Kristina Toutanova'}]}, {'paperId': 'e225dd59ef4954db21479cdcbee497624b2d6d0f', 'title': 'Countering Adversarial Images using Input Transformations', 'abstract': None, 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1711.00117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '144993411', 'name': 'Chuan Guo'}, {'authorId': '2139712', 'name': 'Mayank Rana'}, {'authorId': '5723508', 'name': 'Moustapha Cissé'}, {'authorId': '1803520', 'name': 'L. Maaten'}]}, {'paperId': None, 'title': 'Authorized licensed use limited to the terms of the', 'abstract': None, 'openAccessPdf': None, 'authors': []}, {'paperId': None, 'title': 'Restrictions apply', 'abstract': None, 'openAccessPdf': None, 'authors': []}]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = sch.get_paper(paper_id='7433da608f60204cf0845fbd26cb83982e891875', fields=['title', 'paperId', 'externalIds', 'abstract', 'references.title', 'references.paperId', 'references.abstract'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "r = requests.post(\n",
    "    'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "    params={'fields': 'references.abstract'},  # 'title,paperId,externalIds,abstract,references.title,references.paperId,\n",
    "    json={'ids': ['7433da608f60204cf0845fbd26cb83982e891875']}\n",
    ")\n",
    "res = r.json()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': 'f09e3845b9857b0c1a251bdf0f572eaa1519cc2f',\n",
       "  'title': 'Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': 'CLOSED',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52729.2023.00395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52729.2023.00395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2117164874', 'name': 'Yunrui Yu'},\n",
       "   {'authorId': '2153074991', 'name': 'Chengjie Xu'}]},\n",
       " {'paperId': '163b4d6a79a5b19af88b8585456363340d9efd04',\n",
       "  'title': 'GPT-4 Technical Report',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2303.08774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2275249853', 'name': 'OpenAI Josh Achiam'},\n",
       "   {'authorId': '2275250875', 'name': 'Steven Adler'},\n",
       "   {'authorId': '144517868', 'name': 'Sandhini Agarwal'},\n",
       "   {'authorId': '2274773568', 'name': 'Lama Ahmad'},\n",
       "   {'authorId': '2258629', 'name': 'Ilge Akkaya'},\n",
       "   {'authorId': '2275244794', 'name': 'Florencia Leoni Aleman'},\n",
       "   {'authorId': '2275252021', 'name': 'Diogo Almeida'},\n",
       "   {'authorId': '2275252424', 'name': 'Janko Altenschmidt'},\n",
       "   {'authorId': '2275245579', 'name': 'Sam Altman'},\n",
       "   {'authorId': '2275246437', 'name': 'Shyamal Anadkat'},\n",
       "   {'authorId': '2275139370', 'name': 'Red Avila'},\n",
       "   {'authorId': '2256699302', 'name': 'Igor Babuschkin'},\n",
       "   {'authorId': '2054519183', 'name': 'S. Balaji'},\n",
       "   {'authorId': '2275251659', 'name': 'Valerie Balcom'},\n",
       "   {'authorId': '47626612', 'name': 'Paul Baltescu'},\n",
       "   {'authorId': '2275198557', 'name': 'Haim-ing Bao'},\n",
       "   {'authorId': '2275251620', 'name': 'Mo Bavarian'},\n",
       "   {'authorId': '2275245092', 'name': 'J. Belgum'},\n",
       "   {'authorId': '4689792', 'name': 'Irwan Bello'},\n",
       "   {'authorId': '2275245414', 'name': 'Jake Berdine'},\n",
       "   {'authorId': '2275245581', 'name': 'Gabriel Bernadett-Shapiro'},\n",
       "   {'authorId': '133740015', 'name': 'Christopher Berner'},\n",
       "   {'authorId': '2275251674', 'name': 'Lenny Bogdonoff'},\n",
       "   {'authorId': '2275246071', 'name': 'Oleg Boiko'},\n",
       "   {'authorId': '2275248137', 'name': 'Made-laine Boyd'},\n",
       "   {'authorId': '2275245419', 'name': 'Anna-Luisa Brakman'},\n",
       "   {'authorId': '2065151121', 'name': 'Greg Brockman'},\n",
       "   {'authorId': '2275219628', 'name': 'Tim Brooks'},\n",
       "   {'authorId': '35167962', 'name': 'Miles Brundage'},\n",
       "   {'authorId': '2146257251', 'name': 'Kevin Button'},\n",
       "   {'authorId': '2275157286', 'name': 'Trevor Cai'},\n",
       "   {'authorId': '2274782053', 'name': 'Rosie Campbell'},\n",
       "   {'authorId': '2275245404', 'name': 'Andrew Cann'},\n",
       "   {'authorId': '2275246368', 'name': 'Brittany Carey'},\n",
       "   {'authorId': '2275120298', 'name': 'Chelsea Carlson'},\n",
       "   {'authorId': '144114446', 'name': 'Rory Carmichael'},\n",
       "   {'authorId': '1466431052', 'name': 'Brooke Chan'},\n",
       "   {'authorId': '2275545855', 'name': 'Che Chang'},\n",
       "   {'authorId': '2057091285', 'name': 'Fotis Chantzis'},\n",
       "   {'authorId': '2253841704', 'name': 'Derek Chen'},\n",
       "   {'authorId': '2275188918', 'name': 'Sully Chen'},\n",
       "   {'authorId': '2275179180', 'name': 'Ruby Chen'},\n",
       "   {'authorId': '2275289833', 'name': 'Jason Chen'},\n",
       "   {'authorId': '2108828435', 'name': 'Mark Chen'},\n",
       "   {'authorId': '1490681878', 'name': 'Benjamin Chess'},\n",
       "   {'authorId': '2275251158', 'name': 'Chester Cho'},\n",
       "   {'authorId': '2276186593', 'name': 'Casey Chu'},\n",
       "   {'authorId': '2275839391', 'name': 'Hyung Won Chung'},\n",
       "   {'authorId': '2275231534', 'name': 'Dave Cummings'},\n",
       "   {'authorId': '49645091', 'name': 'Jeremiah Currier'},\n",
       "   {'authorId': '2276187456', 'name': 'Yunxing Dai'},\n",
       "   {'authorId': '2275251205', 'name': 'C. Decareaux'},\n",
       "   {'authorId': '2275244920', 'name': 'Thomas Degry'},\n",
       "   {'authorId': '2275247090', 'name': 'Noah Deutsch'},\n",
       "   {'authorId': '2275251200', 'name': 'Damien Deville'},\n",
       "   {'authorId': '2275244298', 'name': 'Arka Dhar'},\n",
       "   {'authorId': '35363891', 'name': 'David Dohan'},\n",
       "   {'authorId': '2275252295', 'name': 'Steve Dowling'},\n",
       "   {'authorId': '2275245491', 'name': 'Sheila Dunning'},\n",
       "   {'authorId': '66821245', 'name': 'Adrien Ecoffet'},\n",
       "   {'authorId': '2275245457', 'name': 'Atty Eleti'},\n",
       "   {'authorId': '2146257131', 'name': 'Tyna Eloundou'},\n",
       "   {'authorId': '2065430571', 'name': 'David Farhi'},\n",
       "   {'authorId': '2096916416', 'name': 'L. Fedus'},\n",
       "   {'authorId': '2275249996', 'name': 'Niko Felix'},\n",
       "   {'authorId': '2275245820', 'name': \"Sim'on Posada Fishman\"},\n",
       "   {'authorId': '2275244914', 'name': 'Juston Forte'},\n",
       "   {'authorId': '2275251173', 'name': 'Is-abella Fulford'},\n",
       "   {'authorId': '2027599537', 'name': 'Leo Gao'},\n",
       "   {'authorId': '2275200811', 'name': 'Elie Georges'},\n",
       "   {'authorId': '2275254804', 'name': 'C. Gibson'},\n",
       "   {'authorId': '2275144649', 'name': 'Vik Goel'},\n",
       "   {'authorId': '2325028819', 'name': 'Tarun Gogineni'},\n",
       "   {'authorId': '2261041177', 'name': 'Gabriel Goh'},\n",
       "   {'authorId': '2158366935', 'name': 'Raphael Gontijo-Lopes'},\n",
       "   {'authorId': '2265066144', 'name': 'Jonathan Gordon'},\n",
       "   {'authorId': '2275250003', 'name': 'Morgan Grafstein'},\n",
       "   {'authorId': '145565184', 'name': 'Scott Gray'},\n",
       "   {'authorId': '2275247307', 'name': 'Ryan Greene'},\n",
       "   {'authorId': '2275137274', 'name': 'Joshua Gross'},\n",
       "   {'authorId': '2253699903', 'name': 'S. Gu'},\n",
       "   {'authorId': '2276101257', 'name': 'Yufei Guo'},\n",
       "   {'authorId': '2004021329', 'name': 'Chris Hallacy'},\n",
       "   {'authorId': '2275540338', 'name': 'Jesse Han'},\n",
       "   {'authorId': '2275295848', 'name': 'Jeff Harris'},\n",
       "   {'authorId': '2275226809', 'name': 'Yuchen He'},\n",
       "   {'authorId': '2275245527', 'name': 'Mike Heaton'},\n",
       "   {'authorId': '2151087994', 'name': 'Jo-hannes Heidecke'},\n",
       "   {'authorId': '2242286342', 'name': 'Chris Hesse'},\n",
       "   {'authorId': '2226452668', 'name': 'Alan Hickey'},\n",
       "   {'authorId': '2275246148', 'name': 'Wade Hickey'},\n",
       "   {'authorId': '2275245339', 'name': 'Peter Hoeschele'},\n",
       "   {'authorId': '103681415', 'name': 'Brandon Houghton'},\n",
       "   {'authorId': '2275214107', 'name': 'Kenny Hsu'},\n",
       "   {'authorId': '2275210604', 'name': 'Shengli Hu'},\n",
       "   {'authorId': '2275777049', 'name': 'Xin Hu'},\n",
       "   {'authorId': '39378983', 'name': 'Joost Huizinga'},\n",
       "   {'authorId': '2276187117', 'name': 'Shantanu Jain'},\n",
       "   {'authorId': '2171110177', 'name': 'Shawn Jain'},\n",
       "   {'authorId': '2151094350', 'name': 'Joanne Jang'},\n",
       "   {'authorId': '2253471334', 'name': 'Angela Jiang'},\n",
       "   {'authorId': '2275172062', 'name': 'Roger Jiang'},\n",
       "   {'authorId': '2275752035', 'name': 'Haozhun Jin'},\n",
       "   {'authorId': '2275203081', 'name': 'Denny Jin'},\n",
       "   {'authorId': '2275250083', 'name': 'Shino Jomoto'},\n",
       "   {'authorId': '2275247096', 'name': 'B. Jonn'},\n",
       "   {'authorId': '35450887', 'name': 'Heewoo Jun'},\n",
       "   {'authorId': '2403754', 'name': 'Tomer Kaftan'},\n",
       "   {'authorId': '2275230678', 'name': 'Lukasz Kaiser'},\n",
       "   {'authorId': '2275169038', 'name': 'Ali Kamali'},\n",
       "   {'authorId': '3151440', 'name': 'I. Kanitscheider'},\n",
       "   {'authorId': '2844898', 'name': 'N. Keskar'},\n",
       "   {'authorId': '2152264064', 'name': 'Tabarak Khan'},\n",
       "   {'authorId': '2275246102', 'name': 'Logan Kilpatrick'},\n",
       "   {'authorId': '2260346092', 'name': 'Jong Wook Kim'},\n",
       "   {'authorId': '2149054292', 'name': 'Christina Kim'},\n",
       "   {'authorId': '2275296777', 'name': 'Yongjik Kim'},\n",
       "   {'authorId': '2275112980', 'name': 'Hendrik Kirchner'},\n",
       "   {'authorId': '51131802', 'name': 'J. Kiros'},\n",
       "   {'authorId': '2146257375', 'name': 'Matthew Knight'},\n",
       "   {'authorId': '1485556711', 'name': 'Daniel Kokotajlo'},\n",
       "   {'authorId': '2275246094', 'name': 'Lukasz Kondraciuk'},\n",
       "   {'authorId': '1666171360', 'name': 'Andrew Kondrich'},\n",
       "   {'authorId': '2275252322', 'name': 'Aris Konstantinidis'},\n",
       "   {'authorId': '2275245594', 'name': 'Kyle Kosic'},\n",
       "   {'authorId': '2064404342', 'name': 'Gretchen Krueger'},\n",
       "   {'authorId': '2275229877', 'name': 'Vishal Kuo'},\n",
       "   {'authorId': '2275247085', 'name': 'Michael Lampe'},\n",
       "   {'authorId': '2275246287', 'name': 'Ikai Lan'},\n",
       "   {'authorId': '2274915115', 'name': 'Teddy Lee'},\n",
       "   {'authorId': '2990741', 'name': 'J. Leike'},\n",
       "   {'authorId': '52152632', 'name': 'Jade Leung'},\n",
       "   {'authorId': '2275256930', 'name': 'Daniel Levy'},\n",
       "   {'authorId': '2275285124', 'name': 'Chak Li'},\n",
       "   {'authorId': '2275176375', 'name': 'Rachel Lim'},\n",
       "   {'authorId': '2275759230', 'name': 'Molly Lin'},\n",
       "   {'authorId': '2253840098', 'name': 'Stephanie Lin'},\n",
       "   {'authorId': '1380985420', 'name': 'Ma-teusz Litwin'},\n",
       "   {'authorId': '2275248327', 'name': 'Theresa Lopez'},\n",
       "   {'authorId': '2257272397', 'name': 'Ryan Lowe'},\n",
       "   {'authorId': '2275245628', 'name': 'Patricia Lue'},\n",
       "   {'authorId': '119341078', 'name': 'A. Makanju'},\n",
       "   {'authorId': '2275245649', 'name': 'Kim Malfacini'},\n",
       "   {'authorId': '46430291', 'name': 'Sam Manning'},\n",
       "   {'authorId': '14113256', 'name': 'Todor Markov'},\n",
       "   {'authorId': '2275245336', 'name': 'Yaniv Markovski'},\n",
       "   {'authorId': '2114362965', 'name': 'Bianca Martin'},\n",
       "   {'authorId': '2275231822', 'name': 'Katie Mayer'},\n",
       "   {'authorId': '2275247045', 'name': 'Andrew Mayne'},\n",
       "   {'authorId': '39593364', 'name': 'Bob McGrew'},\n",
       "   {'authorId': '2047820455', 'name': 'S. McKinney'},\n",
       "   {'authorId': '3028785', 'name': 'C. McLeavey'},\n",
       "   {'authorId': '2274772421', 'name': 'Paul McMillan'},\n",
       "   {'authorId': '2275234856', 'name': 'Jake McNeil'},\n",
       "   {'authorId': '2275210659', 'name': 'David Medina'},\n",
       "   {'authorId': '2275132306', 'name': 'Aalok Mehta'},\n",
       "   {'authorId': '10698483', 'name': 'Jacob Menick'},\n",
       "   {'authorId': '2275246330', 'name': 'Luke Metz'},\n",
       "   {'authorId': '2275252694', 'name': 'Andrey Mishchenko'},\n",
       "   {'authorId': '2051714782', 'name': 'Pamela Mishkin'},\n",
       "   {'authorId': '2275245453', 'name': 'Vinnie Monaco'},\n",
       "   {'authorId': '1404556973', 'name': 'Evan Morikawa'},\n",
       "   {'authorId': '3407880', 'name': 'Daniel P. Mossing'},\n",
       "   {'authorId': '2275154456', 'name': 'Tong Mu'},\n",
       "   {'authorId': '2117715631', 'name': 'Mira Murati'},\n",
       "   {'authorId': '147746767', 'name': 'O. Murk'},\n",
       "   {'authorId': '2275246116', 'name': \"David M'ely\"},\n",
       "   {'authorId': '3422774', 'name': 'Ashvin Nair'},\n",
       "   {'authorId': '7406311', 'name': 'Reiichiro Nakano'},\n",
       "   {'authorId': '2057426488', 'name': 'Rajeev Nayak'},\n",
       "   {'authorId': '2072676', 'name': 'Arvind Neelakantan'},\n",
       "   {'authorId': '2273886618', 'name': 'Richard Ngo'},\n",
       "   {'authorId': '2275115983', 'name': 'Hyeonwoo Noh'},\n",
       "   {'authorId': '2228518120', 'name': 'Ouyang Long'},\n",
       "   {'authorId': '1435765036', 'name': \"Cullen O'Keefe\"},\n",
       "   {'authorId': '2713380', 'name': 'J. Pachocki'},\n",
       "   {'authorId': '34800652', 'name': 'Alex Paino'},\n",
       "   {'authorId': '2275244652', 'name': 'Joe Palermo'},\n",
       "   {'authorId': '2275246178', 'name': 'Ashley Pantuliano'},\n",
       "   {'authorId': '50213542', 'name': 'Giambattista Parascandolo'},\n",
       "   {'authorId': '2275245818', 'name': 'Joel Parish'},\n",
       "   {'authorId': '2275245435', 'name': 'Emy Parparita'},\n",
       "   {'authorId': '2274774915', 'name': 'Alexandre Passos'},\n",
       "   {'authorId': '2068123790', 'name': 'Mikhail Pavlov'},\n",
       "   {'authorId': '2275125663', 'name': 'Andrew Peng'},\n",
       "   {'authorId': '2275245529', 'name': 'Adam Perelman'},\n",
       "   {'authorId': '2275250075', 'name': 'Filipe de Avila Belbute Peres'},\n",
       "   {'authorId': '2136008481', 'name': 'Michael Petrov'},\n",
       "   {'authorId': '1463773776', 'name': 'Henrique Pondé de Oliveira Pinto'},\n",
       "   {'authorId': '2275246346', 'name': 'Michael Pokorny'},\n",
       "   {'authorId': '2275246814', 'name': 'Michelle Pokrass'},\n",
       "   {'authorId': '144401061', 'name': 'Vitchyr H. Pong'},\n",
       "   {'authorId': '2275150061', 'name': 'Tolly Powell'},\n",
       "   {'authorId': '146162186', 'name': 'Alethea Power'},\n",
       "   {'authorId': '2151088845', 'name': 'Boris Power'},\n",
       "   {'authorId': '2275243930', 'name': 'Elizabeth Proehl'},\n",
       "   {'authorId': '2285654208', 'name': 'Raul Puri'},\n",
       "   {'authorId': '38909097', 'name': 'Alec Radford'},\n",
       "   {'authorId': '2275178294', 'name': 'Jack W. Rae'},\n",
       "   {'authorId': '2261024614', 'name': 'Aditya Ramesh'},\n",
       "   {'authorId': '2275225165', 'name': 'Cameron Raymond'},\n",
       "   {'authorId': '2275252438', 'name': 'Francis Real'},\n",
       "   {'authorId': '2275252095', 'name': 'Kendra Rimbach'},\n",
       "   {'authorId': '2275207240', 'name': 'Carl Ross'},\n",
       "   {'authorId': '11150265', 'name': 'Bob Rotsted'},\n",
       "   {'authorId': '2275250007', 'name': 'Henri Roussez'},\n",
       "   {'authorId': '2260406867', 'name': 'Nick Ryder'},\n",
       "   {'authorId': '47204843', 'name': 'M. Saltarelli'},\n",
       "   {'authorId': '2275246803', 'name': 'Ted Sanders'},\n",
       "   {'authorId': '2852106', 'name': 'Shibani Santurkar'},\n",
       "   {'authorId': '144864359', 'name': 'Girish Sastry'},\n",
       "   {'authorId': '2275265666', 'name': 'Heather Schmidt'},\n",
       "   {'authorId': '2252874293', 'name': 'David Schnurr'},\n",
       "   {'authorId': '47971768', 'name': 'John Schulman'},\n",
       "   {'authorId': '2196579', 'name': 'Daniel Selsam'},\n",
       "   {'authorId': '2275244711', 'name': 'Kyla Sheppard'},\n",
       "   {'authorId': '102475503', 'name': 'Toki Sherbakov'},\n",
       "   {'authorId': '2275246834', 'name': 'Jessica Shieh'},\n",
       "   {'authorId': '118335789', 'name': 'S. Shoker'},\n",
       "   {'authorId': '67311962', 'name': 'Pranav Shyam'},\n",
       "   {'authorId': '2700360', 'name': 'Szymon Sidor'},\n",
       "   {'authorId': '2064673055', 'name': 'Eric Sigler'},\n",
       "   {'authorId': '2151735251', 'name': 'Maddie Simens'},\n",
       "   {'authorId': '2275252299', 'name': 'Jordan Sitkin'},\n",
       "   {'authorId': '2117680841', 'name': 'Katarina Slama'},\n",
       "   {'authorId': '103422608', 'name': 'Ian Sohl'},\n",
       "   {'authorId': '2901424', 'name': 'Benjamin Sokolowsky'},\n",
       "   {'authorId': '2307592658', 'name': 'Yang Song'},\n",
       "   {'authorId': '2275245668', 'name': 'Natalie Staudacher'},\n",
       "   {'authorId': '9927844', 'name': 'F. Such'},\n",
       "   {'authorId': '2275252251', 'name': 'Natalie Summers'},\n",
       "   {'authorId': '1701686', 'name': 'I. Sutskever'},\n",
       "   {'authorId': '2275750817', 'name': 'Jie Tang'},\n",
       "   {'authorId': '145950540', 'name': 'N. Tezak'},\n",
       "   {'authorId': '2151289331', 'name': 'Madeleine Thompson'},\n",
       "   {'authorId': '2275252092', 'name': 'Phil Tillet'},\n",
       "   {'authorId': '2267339677', 'name': 'Amin Tootoonchian'},\n",
       "   {'authorId': '2275249879', 'name': 'Elizabeth Tseng'},\n",
       "   {'authorId': '2275249709', 'name': 'Preston Tuggle'},\n",
       "   {'authorId': '2275244171', 'name': 'Nick Turley'},\n",
       "   {'authorId': '2065005836', 'name': 'Jerry Tworek'},\n",
       "   {'authorId': '2275203310', 'name': \"Juan Felipe Cer'on Uribe\"},\n",
       "   {'authorId': '2275244586', 'name': 'Andrea Vallone'},\n",
       "   {'authorId': '2275245661', 'name': 'Arun Vijayvergiya'},\n",
       "   {'authorId': '153387869', 'name': 'Chelsea Voss'},\n",
       "   {'authorId': '2275245962', 'name': 'Carroll L. Wainwright'},\n",
       "   {'authorId': '2275528432', 'name': 'Justin Jay Wang'},\n",
       "   {'authorId': '2275540420', 'name': 'Alvin Wang'},\n",
       "   {'authorId': '2275189326', 'name': 'Ben Wang'},\n",
       "   {'authorId': '2170081200', 'name': 'Jonathan Ward'},\n",
       "   {'authorId': '2253952872', 'name': 'Jason Wei'},\n",
       "   {'authorId': '2275244218', 'name': 'CJ Weinmann'},\n",
       "   {'authorId': '2275245663', 'name': 'Akila Welihinda'},\n",
       "   {'authorId': '2930640', 'name': 'P. Welinder'},\n",
       "   {'authorId': '2275139180', 'name': 'Jiayi Weng'},\n",
       "   {'authorId': '2065741038', 'name': 'Lilian Weng'},\n",
       "   {'authorId': '2275252154', 'name': 'Matt Wiethoff'},\n",
       "   {'authorId': '2275249733', 'name': 'Dave Willner'},\n",
       "   {'authorId': '2059411355', 'name': 'Clemens Winter'},\n",
       "   {'authorId': '2275244177', 'name': 'Samuel Wolrich'},\n",
       "   {'authorId': '2275225207', 'name': 'Hannah Wong'},\n",
       "   {'authorId': '2275245771', 'name': 'Lauren Workman'},\n",
       "   {'authorId': '2275299848', 'name': 'Sherwin Wu'},\n",
       "   {'authorId': '2274911253', 'name': 'Jeff Wu'},\n",
       "   {'authorId': '2307456650', 'name': 'Michael Wu'},\n",
       "   {'authorId': '2275190169', 'name': 'Kai Xiao'},\n",
       "   {'authorId': '2275452480', 'name': 'Tao Xu'},\n",
       "   {'authorId': '2275310096', 'name': 'Sarah Yoo'},\n",
       "   {'authorId': '2275593618', 'name': 'Kevin Yu'},\n",
       "   {'authorId': '2275194186', 'name': 'Qim-ing Yuan'},\n",
       "   {'authorId': '2563432', 'name': 'Wojciech Zaremba'},\n",
       "   {'authorId': '49629836', 'name': 'Rowan Zellers'},\n",
       "   {'authorId': '2262080679', 'name': 'Chong Zhang'},\n",
       "   {'authorId': '2275288889', 'name': 'Marvin Zhang'},\n",
       "   {'authorId': '2275545682', 'name': 'Shengjia Zhao'},\n",
       "   {'authorId': '2275257857', 'name': 'Tianhao Zheng'},\n",
       "   {'authorId': '2275201537', 'name': 'Juntang Zhuang'},\n",
       "   {'authorId': '2275245715', 'name': 'William Zhuk'},\n",
       "   {'authorId': '2368067', 'name': 'Barret Zoph'}]},\n",
       " {'paperId': 'a1e7b7a560b493c235eed2429cfbb9c12324ff4d',\n",
       "  'title': 'Scaling Adversarial Training to Large Perturbation Bounds',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2210.09852',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2210.09852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '1398341975', 'name': 'Sravanti Addepalli'},\n",
       "   {'authorId': '151444035', 'name': 'Samyak Jain'},\n",
       "   {'authorId': '1602820179', 'name': 'Gaurang Sriramanan'},\n",
       "   {'authorId': '144682140', 'name': 'R. Venkatesh Babu'}]},\n",
       " {'paperId': '426b0ee8c723aa6086402f743d5cbb447622d9b6',\n",
       "  'title': 'When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2210.07540',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2210.07540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '145566197', 'name': 'Yi Mo'},\n",
       "   {'authorId': '1492154834', 'name': 'Dongxian Wu'},\n",
       "   {'authorId': '2115568564', 'name': 'Yifei Wang'},\n",
       "   {'authorId': '2527106', 'name': 'Yiwen Guo'},\n",
       "   {'authorId': '2115869684', 'name': 'Yisen Wang'}]},\n",
       " {'paperId': 'c570475cab4c8d0662144c4d414c17e776d39409',\n",
       "  'title': 'A Light Recipe to Train Robust Vision Transformers',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2209.07399',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2209.07399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2175939276', 'name': 'Edoardo Debenedetti'},\n",
       "   {'authorId': '3482535', 'name': 'Vikash Sehwag'},\n",
       "   {'authorId': '143615345', 'name': 'Prateek Mittal'}]},\n",
       " {'paperId': '35c0800e657faa18cf3fc3629bdbeafbb976b006',\n",
       "  'title': 'Are Transformers More Robust Than CNNs?',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2111.05464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '48442730', 'name': 'Yutong Bai'},\n",
       "   {'authorId': '10407760', 'name': 'Jieru Mei'},\n",
       "   {'authorId': '145081362', 'name': 'A. Yuille'},\n",
       "   {'authorId': '3011497', 'name': 'Cihang Xie'}]},\n",
       " {'paperId': '9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342',\n",
       "  'title': 'Improving Robustness using Generated Data',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2110.09468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2071666', 'name': 'Sven Gowal'},\n",
       "   {'authorId': '8478422', 'name': 'Sylvestre-Alvise Rebuffi'},\n",
       "   {'authorId': '8792285', 'name': 'Olivia Wiles'},\n",
       "   {'authorId': '3205302', 'name': 'Florian Stimberg'},\n",
       "   {'authorId': '2792016', 'name': 'D. A. Calian'},\n",
       "   {'authorId': '144467964', 'name': 'Timothy Mann'}]},\n",
       " {'paperId': '3c2622daa8a658d5c85ea9869cb460a70b0f878d',\n",
       "  'title': 'Towards Transferable Adversarial Attacks on Vision Transformers',\n",
       "  'abstract': 'Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance.',\n",
       "  'openAccessPdf': {'url': 'https://ojs.aaai.org/index.php/AAAI/article/download/20169/19928',\n",
       "   'status': 'GOLD',\n",
       "   'license': None,\n",
       "   'disclaimer': 'Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2109.04176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'},\n",
       "  'authors': [{'authorId': None, 'name': 'Zhipeng Wei'},\n",
       "   {'authorId': '2108536365', 'name': 'Jingjing Chen'},\n",
       "   {'authorId': '2126058635', 'name': 'Micah Goldblum'},\n",
       "   {'authorId': '3099139', 'name': 'Zuxuan Wu'},\n",
       "   {'authorId': '1962083', 'name': 'T. Goldstein'},\n",
       "   {'authorId': '1717861', 'name': 'Yu-Gang Jiang'}]},\n",
       " {'paperId': '0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9',\n",
       "  'title': 'On Improving Adversarial Transferability of Vision Transformers',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2106.04169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '40894826', 'name': 'Muzammal Naseer'},\n",
       "   {'authorId': '48430646', 'name': 'Kanchana Ranasinghe'},\n",
       "   {'authorId': '2111180748', 'name': 'Salman Siddique Khan'},\n",
       "   {'authorId': '2358803', 'name': 'F. Khan'},\n",
       "   {'authorId': '29905643', 'name': 'F. Porikli'}]},\n",
       " {'paperId': '43e51c1bfd69df518e2907f7a955e485985ba423',\n",
       "  'title': 'On the Robustness of Vision Transformers to Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2104.02610',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2104.02610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '145460048', 'name': 'Kaleel Mahmood'},\n",
       "   {'authorId': '9461237', 'name': 'Rigel Mahmood'},\n",
       "   {'authorId': '144534186', 'name': 'Marten van Dijk'}]},\n",
       " {'paperId': 'd2a3bb6356d439146cd8d8e72dc728a1e3d93e7f',\n",
       "  'title': 'Understanding Robustness of Transformers for Image Classification',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2103.14586',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.14586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '1798880', 'name': 'Srinadh Bhojanapalli'},\n",
       "   {'authorId': '38534744', 'name': 'Ayan Chakrabarti'},\n",
       "   {'authorId': '1752652', 'name': 'Daniel Glasner'},\n",
       "   {'authorId': '2108467824', 'name': 'Daliang Li'},\n",
       "   {'authorId': '2465270', 'name': 'Thomas Unterthiner'},\n",
       "   {'authorId': '2799898', 'name': 'Andreas Veit'}]},\n",
       " {'paperId': '152d454e54112742d5f9e6de64f25387c84bcbbf',\n",
       "  'title': 'Evaluating the Robustness of Geometry-Aware Instance-Reweighted Adversarial Training',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.01914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2702886', 'name': 'Dorjan Hitaj'},\n",
       "   {'authorId': '1403242401', 'name': 'Giulio Pagnotta'},\n",
       "   {'authorId': '11269472', 'name': 'I. Masi'},\n",
       "   {'authorId': '1686516', 'name': 'L. Mancini'}]},\n",
       " {'paperId': 'ad7ddcc14984caae308c397f1a589aae75d4ab71',\n",
       "  'title': 'Training data-efficient image transformers & distillation through attention',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2012.12877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2113243762', 'name': 'Hugo Touvron'},\n",
       "   {'authorId': '51021910', 'name': 'M. Cord'},\n",
       "   {'authorId': '3271933', 'name': 'Matthijs Douze'},\n",
       "   {'authorId': '1403239967', 'name': 'Francisco Massa'},\n",
       "   {'authorId': '3469062', 'name': 'Alexandre Sablayrolles'},\n",
       "   {'authorId': '2065248680', 'name': \"Herv'e J'egou\"}]},\n",
       " {'paperId': '5fc631cc1dbb5383a7751c39d3f8f3548e8d6ed2',\n",
       "  'title': 'Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2011.14969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '1602820179', 'name': 'Gaurang Sriramanan'},\n",
       "   {'authorId': '1398341975', 'name': 'Sravanti Addepalli'},\n",
       "   {'authorId': '30553248', 'name': 'Arya Baburaj'},\n",
       "   {'authorId': '144682140', 'name': 'R. Venkatesh Babu'}]},\n",
       " {'paperId': '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a',\n",
       "  'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2010.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2841331', 'name': 'Alexey Dosovitskiy'},\n",
       "   {'authorId': '39611591', 'name': 'Lucas Beyer'},\n",
       "   {'authorId': '144629422', 'name': 'Alexander Kolesnikov'},\n",
       "   {'authorId': '3319373', 'name': 'Dirk Weissenborn'},\n",
       "   {'authorId': '2743563', 'name': 'Xiaohua Zhai'},\n",
       "   {'authorId': '2465270', 'name': 'Thomas Unterthiner'},\n",
       "   {'authorId': '2274215058', 'name': 'Mostafa Dehghani'},\n",
       "   {'authorId': '46352821', 'name': 'Matthias Minderer'},\n",
       "   {'authorId': '2280399', 'name': 'G. Heigold'},\n",
       "   {'authorId': '1802148', 'name': 'S. Gelly'},\n",
       "   {'authorId': '39328010', 'name': 'Jakob Uszkoreit'},\n",
       "   {'authorId': '2815290', 'name': 'N. Houlsby'}]},\n",
       " {'paperId': '99a599d8fe56529f47e78243ed61250190f96196',\n",
       "  'title': 'Geometry-aware Instance-reweighted Adversarial Training',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2010.01736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '47539929', 'name': 'Jingfeng Zhang'},\n",
       "   {'authorId': '2143475848', 'name': 'Jianing Zhu'},\n",
       "   {'authorId': '47537639', 'name': 'Gang Niu'},\n",
       "   {'authorId': '2087238859', 'name': 'Bo Han'},\n",
       "   {'authorId': '67154907', 'name': 'Masashi Sugiyama'},\n",
       "   {'authorId': '1744045', 'name': 'M. Kankanhalli'}]},\n",
       " {'paperId': '473a854a939eca4bf39420ff496f8e24e223d460',\n",
       "  'title': 'Perceptual Adversarial Robustness: Defense Against Unseen Threat Models',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2006.12655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '114339785', 'name': 'Cassidy Laidlaw'},\n",
       "   {'authorId': '144190575', 'name': 'Sahil Singla'},\n",
       "   {'authorId': '34389431', 'name': 'S. Feizi'}]},\n",
       " {'paperId': '90abbc2cf38462b954ae1b772fac9532e2ccd8b0',\n",
       "  'title': 'Language Models are Few-Shot Learners',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2005.14165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '31035595', 'name': 'Tom B. Brown'},\n",
       "   {'authorId': '2056658938', 'name': 'Benjamin Mann'},\n",
       "   {'authorId': '39849748', 'name': 'Nick Ryder'},\n",
       "   {'authorId': '2065894334', 'name': 'Melanie Subbiah'},\n",
       "   {'authorId': '152724169', 'name': 'J. Kaplan'},\n",
       "   {'authorId': '6515819', 'name': 'Prafulla Dhariwal'},\n",
       "   {'authorId': '2072676', 'name': 'Arvind Neelakantan'},\n",
       "   {'authorId': '67311962', 'name': 'Pranav Shyam'},\n",
       "   {'authorId': '144864359', 'name': 'Girish Sastry'},\n",
       "   {'authorId': '119609682', 'name': 'Amanda Askell'},\n",
       "   {'authorId': '144517868', 'name': 'Sandhini Agarwal'},\n",
       "   {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'},\n",
       "   {'authorId': '2064404342', 'name': 'Gretchen Krueger'},\n",
       "   {'authorId': '103143311', 'name': 'T. Henighan'},\n",
       "   {'authorId': '48422824', 'name': 'R. Child'},\n",
       "   {'authorId': '1992922591', 'name': 'A. Ramesh'},\n",
       "   {'authorId': '2052152920', 'name': 'Daniel M. Ziegler'},\n",
       "   {'authorId': '49387725', 'name': 'Jeff Wu'},\n",
       "   {'authorId': '2059411355', 'name': 'Clemens Winter'},\n",
       "   {'authorId': '144239765', 'name': 'Christopher Hesse'},\n",
       "   {'authorId': '2108828435', 'name': 'Mark Chen'},\n",
       "   {'authorId': '2064673055', 'name': 'Eric Sigler'},\n",
       "   {'authorId': '1380985420', 'name': 'Ma-teusz Litwin'},\n",
       "   {'authorId': '145565184', 'name': 'Scott Gray'},\n",
       "   {'authorId': '1490681878', 'name': 'Benjamin Chess'},\n",
       "   {'authorId': '2115193883', 'name': 'Jack Clark'},\n",
       "   {'authorId': '133740015', 'name': 'Christopher Berner'},\n",
       "   {'authorId': '52238703', 'name': 'Sam McCandlish'},\n",
       "   {'authorId': '38909097', 'name': 'Alec Radford'},\n",
       "   {'authorId': '1701686', 'name': 'I. Sutskever'},\n",
       "   {'authorId': '2698777', 'name': 'Dario Amodei'}]},\n",
       " {'paperId': '764eff31d9596033859895d9513b838d2c57a6fb',\n",
       "  'title': 'Improving Adversarial Robustness Requires Revisiting Misclassified Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '', 'status': None, 'license': None},\n",
       "  'authors': [{'authorId': '1919541', 'name': 'Yisen Wang'},\n",
       "   {'authorId': '1838855', 'name': 'Difan Zou'},\n",
       "   {'authorId': '2882166', 'name': 'Jinfeng Yi'},\n",
       "   {'authorId': '145148600', 'name': 'J. Bailey'},\n",
       "   {'authorId': '9576855', 'name': 'Xingjun Ma'},\n",
       "   {'authorId': '9937103', 'name': 'Quanquan Gu'}]},\n",
       " {'paperId': '9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5',\n",
       "  'title': 'Adversarial Weight Perturbation Helps Robust Generalization',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '', 'status': None, 'license': None},\n",
       "  'authors': [{'authorId': '1492154834', 'name': 'Dongxian Wu'},\n",
       "   {'authorId': '3085483', 'name': 'Shutao Xia'},\n",
       "   {'authorId': '1919541', 'name': 'Yisen Wang'}]},\n",
       " {'paperId': '18939eadc9c4460c8385e0591cde214a1ead067b',\n",
       "  'title': 'Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2003.01690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '39171784', 'name': 'Francesco Croce'},\n",
       "   {'authorId': '143610806', 'name': 'Matthias Hein'}]},\n",
       " {'paperId': '58c143069444c7dff4be53531a47efefc40be497',\n",
       "  'title': 'On Adaptive Attacks to Adversarial Example Defenses',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2002.08347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2444919', 'name': 'Florian Tramèr'},\n",
       "   {'authorId': '2483738', 'name': 'Nicholas Carlini'},\n",
       "   {'authorId': '40634590', 'name': 'Wieland Brendel'},\n",
       "   {'authorId': '143826246', 'name': 'A. Madry'}]},\n",
       " {'paperId': 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5',\n",
       "  'title': 'On Evaluating Adversarial Robustness',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1902.06705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'},\n",
       "   {'authorId': '38939786', 'name': 'Anish Athalye'},\n",
       "   {'authorId': '1967156', 'name': 'Nicolas Papernot'},\n",
       "   {'authorId': '40634590', 'name': 'Wieland Brendel'},\n",
       "   {'authorId': '19237612', 'name': 'Jonas Rauber'},\n",
       "   {'authorId': '2754804', 'name': 'Dimitris Tsipras'},\n",
       "   {'authorId': '153440022', 'name': 'I. Goodfellow'},\n",
       "   {'authorId': '143826246', 'name': 'A. Madry'},\n",
       "   {'authorId': '145714153', 'name': 'Alexey Kurakin'}]},\n",
       " {'paperId': '6c405d4b5dc41a86be05acd59c06ed19daf01d14',\n",
       "  'title': 'Theoretically Principled Trade-off between Robustness and Accuracy',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1901.08573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '40975176', 'name': 'Hongyang Zhang'},\n",
       "   {'authorId': '29001000', 'name': 'Yaodong Yu'},\n",
       "   {'authorId': '2784735', 'name': 'Jiantao Jiao'},\n",
       "   {'authorId': '143977260', 'name': 'E. Xing'},\n",
       "   {'authorId': '1701847', 'name': 'L. Ghaoui'},\n",
       "   {'authorId': '1694621', 'name': 'Michael I. Jordan'}]},\n",
       " {'paperId': '8b9127bee0f7d109da2672ba06d0f39a5a60335a',\n",
       "  'title': 'Thermometer Encoding: One Hot Way To Resist Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '', 'status': None, 'license': None},\n",
       "  'authors': [{'authorId': '47619311', 'name': 'Jacob Buckman'},\n",
       "   {'authorId': '39788470', 'name': 'Aurko Roy'},\n",
       "   {'authorId': '2402716', 'name': 'Colin Raffel'},\n",
       "   {'authorId': '153440022', 'name': 'I. Goodfellow'}]},\n",
       " {'paperId': '651adaa058f821a890f2c5d1053d69eb481a8352',\n",
       "  'title': 'Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1802.00420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '38939786', 'name': 'Anish Athalye'},\n",
       "   {'authorId': '2483738', 'name': 'Nicholas Carlini'},\n",
       "   {'authorId': '145394689', 'name': 'D. Wagner'}]},\n",
       " {'paperId': 'c468bbde6a22d961829e1970e6ad5795e05418d1',\n",
       "  'title': 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/1801.03924',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1801.03924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2844849', 'name': 'Richard Zhang'},\n",
       "   {'authorId': '2094770', 'name': 'Phillip Isola'},\n",
       "   {'authorId': '1763086', 'name': 'Alexei A. Efros'},\n",
       "   {'authorId': '2177801', 'name': 'Eli Shechtman'},\n",
       "   {'authorId': '39231399', 'name': 'Oliver Wang'}]},\n",
       " {'paperId': '9a089c56eec68df722b2a5a52727143aacdc2532',\n",
       "  'title': 'Mitigating adversarial effects through randomization',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1711.01991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '3011497', 'name': 'Cihang Xie'},\n",
       "   {'authorId': None, 'name': 'Jianyu Wang'},\n",
       "   {'authorId': '2852303', 'name': 'Zhishuai Zhang'},\n",
       "   {'authorId': '145888238', 'name': 'Zhou Ren'},\n",
       "   {'authorId': '145081362', 'name': 'A. Yuille'}]},\n",
       " {'paperId': 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd',\n",
       "  'title': 'PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1710.10766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '115504645', 'name': 'Yang Song'},\n",
       "   {'authorId': '3307885', 'name': 'Taesup Kim'},\n",
       "   {'authorId': '2388416', 'name': 'Sebastian Nowozin'},\n",
       "   {'authorId': '2490652', 'name': 'Stefano Ermon'},\n",
       "   {'authorId': '1684887', 'name': 'Nate Kushman'}]},\n",
       " {'paperId': '7aa38b85fa8cba64d6a4010543f6695dbf5f1386',\n",
       "  'title': 'Towards Deep Learning Models Resistant to Adversarial Attacks',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1706.06083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '143826246', 'name': 'A. Madry'},\n",
       "   {'authorId': '17775913', 'name': 'Aleksandar Makelov'},\n",
       "   {'authorId': '152772922', 'name': 'Ludwig Schmidt'},\n",
       "   {'authorId': '2754804', 'name': 'Dimitris Tsipras'},\n",
       "   {'authorId': '2869958', 'name': 'Adrian Vladu'}]},\n",
       " {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "  'title': 'Attention is All you Need',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1706.03762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '40348417', 'name': 'Ashish Vaswani'},\n",
       "   {'authorId': '1846258', 'name': 'Noam M. Shazeer'},\n",
       "   {'authorId': '3877127', 'name': 'Niki Parmar'},\n",
       "   {'authorId': '39328010', 'name': 'Jakob Uszkoreit'},\n",
       "   {'authorId': '145024664', 'name': 'Llion Jones'},\n",
       "   {'authorId': '19177000', 'name': 'Aidan N. Gomez'},\n",
       "   {'authorId': '40527594', 'name': 'Lukasz Kaiser'},\n",
       "   {'authorId': '3443442', 'name': 'Illia Polosukhin'}]},\n",
       " {'paperId': 'df40ce107a71b770c9d0354b78fdd8989da80d2f',\n",
       "  'title': 'Towards Evaluating the Robustness of Neural Networks',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/1608.04644',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1608.04644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'},\n",
       "   {'authorId': '145394689', 'name': 'D. Wagner'}]},\n",
       " {'paperId': '975b6ec05f04662a967af8c7504b7f552a0ee0bd',\n",
       "  'title': 'Defensive Distillation is Not Robust to Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1607.04311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'},\n",
       "   {'authorId': '145394689', 'name': 'D. Wagner'}]},\n",
       " {'paperId': 'bee044c8e8903fb67523c1f8c105ab4718600cdb',\n",
       "  'title': 'Explaining and Harnessing Adversarial Examples',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1412.6572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '153440022', 'name': 'I. Goodfellow'},\n",
       "   {'authorId': '1789737', 'name': 'Jonathon Shlens'},\n",
       "   {'authorId': '2574060', 'name': 'Christian Szegedy'}]},\n",
       " {'paperId': 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad',\n",
       "  'title': 'Intriguing properties of neural networks',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1312.6199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2574060', 'name': 'Christian Szegedy'},\n",
       "   {'authorId': '2563432', 'name': 'Wojciech Zaremba'},\n",
       "   {'authorId': '1701686', 'name': 'I. Sutskever'},\n",
       "   {'authorId': '143627859', 'name': 'Joan Bruna'},\n",
       "   {'authorId': '1761978', 'name': 'D. Erhan'},\n",
       "   {'authorId': '153440022', 'name': 'I. Goodfellow'},\n",
       "   {'authorId': '2276554', 'name': 'R. Fergus'}]},\n",
       " {'paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e',\n",
       "  'title': 'ImageNet: A large-scale hierarchical image database',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'http://www.image-net.org/papers/imagenet_cvpr09.pdf',\n",
       "   'status': 'CLOSED',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2009.5206848?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2009.5206848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '153302678', 'name': 'Jia Deng'},\n",
       "   {'authorId': '144847596', 'name': 'Wei Dong'},\n",
       "   {'authorId': '2166511', 'name': 'R. Socher'},\n",
       "   {'authorId': '2040091191', 'name': 'Li-Jia Li'},\n",
       "   {'authorId': '94451829', 'name': 'K. Li'},\n",
       "   {'authorId': '48004138', 'name': 'Li Fei-Fei'}]},\n",
       " {'paperId': 'f2e7d8f6bed2bae7311d3e1788d6872387e8bb86',\n",
       "  'title': 'Do Perceptually Aligned Gradients Imply Adversarial Robustness?',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2207.11378',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2207.11378?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2207.11378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2061747201', 'name': 'Roy Ganz'},\n",
       "   {'authorId': '2047309422', 'name': 'Bahjat Kawar'},\n",
       "   {'authorId': '1753908', 'name': 'Michael Elad'}]},\n",
       " {'paperId': '0def290ae38abb4a04e35e0bcdc86b71d237f494',\n",
       "  'title': 'On the Adversarial Robustness of Vision Transformers',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.15670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '1380245502', 'name': 'Rulin Shao'},\n",
       "   {'authorId': '2987927', 'name': 'Zhouxing Shi'},\n",
       "   {'authorId': '2882166', 'name': 'Jinfeng Yi'},\n",
       "   {'authorId': '153191489', 'name': 'Pin-Yu Chen'},\n",
       "   {'authorId': '1793529', 'name': 'Cho-Jui Hsieh'}]},\n",
       " {'paperId': 'c8b25fab5608c3e033d34b4483ec47e68ba109b7',\n",
       "  'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2103.14030',\n",
       "   'status': 'GREEN',\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2103.14030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '2109371439', 'name': 'Ze Liu'},\n",
       "   {'authorId': '51091819', 'name': 'Yutong Lin'},\n",
       "   {'authorId': '2112823372', 'name': 'Yue Cao'},\n",
       "   {'authorId': '1823518756', 'name': 'Han Hu'},\n",
       "   {'authorId': '2107995927', 'name': 'Yixuan Wei'},\n",
       "   {'authorId': '2148904543', 'name': 'Zheng Zhang'},\n",
       "   {'authorId': '145676588', 'name': 'Stephen Lin'},\n",
       "   {'authorId': '2261753424', 'name': 'B. Guo'}]},\n",
       " {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1810.04805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '39172707', 'name': 'Jacob Devlin'},\n",
       "   {'authorId': '1744179', 'name': 'Ming-Wei Chang'},\n",
       "   {'authorId': '2544107', 'name': 'Kenton Lee'},\n",
       "   {'authorId': '3259253', 'name': 'Kristina Toutanova'}]},\n",
       " {'paperId': 'e225dd59ef4954db21479cdcbee497624b2d6d0f',\n",
       "  'title': 'Countering Adversarial Images using Input Transformations',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': {'url': '',\n",
       "   'status': None,\n",
       "   'license': None,\n",
       "   'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/1711.00117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       "  'authors': [{'authorId': '144993411', 'name': 'Chuan Guo'},\n",
       "   {'authorId': '2139712', 'name': 'Mayank Rana'},\n",
       "   {'authorId': '5723508', 'name': 'Moustapha Cissé'},\n",
       "   {'authorId': '1803520', 'name': 'L. Maaten'}]},\n",
       " {'paperId': None,\n",
       "  'title': 'Authorized licensed use limited to the terms of the',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': None,\n",
       "  'authors': []},\n",
       " {'paperId': None,\n",
       "  'title': 'Restrictions apply',\n",
       "  'abstract': None,\n",
       "  'openAccessPdf': None,\n",
       "  'authors': []}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'f09e3845b9857b0c1a251bdf0f572eaa1519cc2f',\n",
       " 'title': 'Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks',\n",
       " 'abstract': \"Attackers can deceive neural networks by adding human imperceptive perturbations to their input data; this reveals the vulnerability and weak robustness of current deep-learning networks. Many attack techniques have been proposed to evaluate the model's robustness. Gradient-based attacks suffer from severely overestimating the robustness. This paper identifies that the relative error in calculated gradients caused by floating-point errors, including floating-point underflow and rounding errors, is a fundamental reason why gradient-based attacks fail to accurately assess the model's robustness. Although it is hard to eliminate the relative error in the gradients, we can control its effect on the gradient-based attacks. Correspondingly, we propose an efficient loss function by minimizing the detrimental impact of the floating-point errors on the attacks. Experimental results show that it is more efficient and reliable than other loss functions when examined across a wide range of defence mechanisms.\",\n",
       " 'openAccessPdf': {'url': '',\n",
       "  'status': 'CLOSED',\n",
       "  'license': None,\n",
       "  'disclaimer': 'Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52729.2023.00395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52729.2023.00395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'},\n",
       " 'authors': [{'authorId': '2117164874', 'name': 'Yunrui Yu'},\n",
       "  {'authorId': '2153074991', 'name': 'Chengjie Xu'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.post(\n",
    "    'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "    params={'fields': 'abstract'},  # 'title,paperId,externalIds,abstract,references.title,references.paperId,\n",
    "    json={'ids': ['f09e3845b9857b0c1a251bdf0f572eaa1519cc2f']}\n",
    ")\n",
    "res = r.json()[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadQueryParametersException",
     "evalue": "Unrecognized or unsupported fields: [s, r, t, b, a, c]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadQueryParametersException\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_paper_references\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m7433da608f60204cf0845fbd26cb83982e891875\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mabstract\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.items[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcitedPaper\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/SemanticScholar.py:284\u001b[39m, in \u001b[36mSemanticScholar.get_paper_references\u001b[39m\u001b[34m(self, paper_id, fields, limit)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03mGet details about a paper's references\u001b[39;00m\n\u001b[32m    264\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m \u001b[33;03m       (must be <= 1000).\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    283\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m results = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_AsyncSemanticScholar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_paper_references\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/AsyncSemanticScholar.py:383\u001b[39m, in \u001b[36mAsyncSemanticScholar.get_paper_references\u001b[39m\u001b[34m(self, paper_id, fields, limit)\u001b[39m\n\u001b[32m    380\u001b[39m base_url = \u001b[38;5;28mself\u001b[39m.api_url + \u001b[38;5;28mself\u001b[39m.BASE_PATH_GRAPH\n\u001b[32m    381\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/paper/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/references\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m PaginatedResults.create(\n\u001b[32m    384\u001b[39m         requester=\u001b[38;5;28mself\u001b[39m._requester,\n\u001b[32m    385\u001b[39m         data_type=Reference,\n\u001b[32m    386\u001b[39m         url=url,\n\u001b[32m    387\u001b[39m         fields=fields,\n\u001b[32m    388\u001b[39m         limit=limit\n\u001b[32m    389\u001b[39m     )\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/PaginatedResults.py:54\u001b[39m, in \u001b[36mPaginatedResults.create\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m     46\u001b[39m             *args, \n\u001b[32m     47\u001b[39m             **kwargs\n\u001b[32m     48\u001b[39m         ):\n\u001b[32m     50\u001b[39m     obj = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m     51\u001b[39m         *args,\n\u001b[32m     52\u001b[39m         **kwargs\n\u001b[32m     53\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m obj._async_get_next_page()\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/PaginatedResults.py:148\u001b[39m, in \u001b[36mPaginatedResults._async_get_next_page\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoMorePagesException(\u001b[33m'\u001b[39m\u001b[33mNo more pages to fetch.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    146\u001b[39m \u001b[38;5;28mself\u001b[39m._build_params()\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_data()\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._update_params(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/PaginatedResults.py:135\u001b[39m, in \u001b[36mPaginatedResults._request_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Union[\u001b[38;5;28mdict\u001b[39m, List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requester.get_data_async(\n\u001b[32m    136\u001b[39m         \u001b[38;5;28mself\u001b[39m._url,\n\u001b[32m    137\u001b[39m         \u001b[38;5;28mself\u001b[39m._parameters,\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m._headers\n\u001b[32m    139\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/ApiRequester.py:90\u001b[39m, in \u001b[36mApiRequester.get_data_async\u001b[39m\u001b[34m(self, url, parameters, headers, payload)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03mGet data from Semantic Scholar API\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m:rtype: :class:`dict` or :class:`List` of :class:`dict`\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retry:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_data_async(\n\u001b[32m     91\u001b[39m         url, parameters, headers, payload)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_data_async.retry_with(\n\u001b[32m     93\u001b[39m         stop=stop_after_attempt(\u001b[32m1\u001b[39m)\n\u001b[32m     94\u001b[39m     )(\u001b[38;5;28mself\u001b[39m, url, parameters, headers, payload)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/semanticscholar/ApiRequester.py:129\u001b[39m, in \u001b[36mApiRequester._get_data_async\u001b[39m\u001b[34m(self, url, parameters, headers, payload)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m r.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m    128\u001b[39m     data = r.json()\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadQueryParametersException(data[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m r.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mHTTP status 403 Forbidden.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mBadQueryParametersException\u001b[39m: Unrecognized or unsupported fields: [s, r, t, b, a, c]"
     ]
    }
   ],
   "source": [
    "sch.get_paper_references(paper_id='7433da608f60204cf0845fbd26cb83982e891875', fields=['title', 'paperId', 'externalIds', 'abstract'], limit=1).items[0]['citedPaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06692b85bc9424aab6163075cbc39da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tot = []\n",
    "for paper_id in tqdm(df.paperId.iloc[:50]):\n",
    "    res = get_referenced_papers(sch, paper_id)\n",
    "    tot.extend(res)\n",
    "\n",
    "df2 = pd.DataFrame(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalPaperId</th>\n",
       "      <th>paperId</th>\n",
       "      <th>abstract</th>\n",
       "      <th>externalIds.ArXiv</th>\n",
       "      <th>externalIds.DBLP</th>\n",
       "      <th>externalIds.DOI</th>\n",
       "      <th>externalIds.CorpusId</th>\n",
       "      <th>externalIds.MAG</th>\n",
       "      <th>externalIds.PubMed</th>\n",
       "      <th>externalIds.PubMedCentral</th>\n",
       "      <th>externalIds.ACL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>9b91b3031ea159e4964d18b2ce703168660ecf46</td>\n",
       "      <td>None</td>\n",
       "      <td>2401.11605</td>\n",
       "      <td>journals/corr/abs-2401-11605</td>\n",
       "      <td>10.48550/arXiv.2401.11605</td>\n",
       "      <td>267069338.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>0e8f1bb91bb4502966fa5e91e0610832dfe4240e</td>\n",
       "      <td>None</td>\n",
       "      <td>2401.08639</td>\n",
       "      <td>journals/corr/abs-2401-08639</td>\n",
       "      <td>10.48550/arXiv.2401.08639</td>\n",
       "      <td>267028569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>b994cf51a8c7cf5c13358a6110d7304d6d04c881</td>\n",
       "      <td>None</td>\n",
       "      <td>2310.18605</td>\n",
       "      <td>journals/corr/abs-2310-18605</td>\n",
       "      <td>10.48550/arXiv.2310.18605</td>\n",
       "      <td>264590159.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>5003fdf35af631d4cb17fd3c1ce2469f665064f1</td>\n",
       "      <td>None</td>\n",
       "      <td>2305.08891</td>\n",
       "      <td>journals/corr/abs-2305-08891</td>\n",
       "      <td>10.1109/WACV57701.2024.00532</td>\n",
       "      <td>258714883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>f9570989919338079088270a9cf1a7afc8db8093</td>\n",
       "      <td>None</td>\n",
       "      <td>2304.14108</td>\n",
       "      <td>journals/corr/abs-2304-14108</td>\n",
       "      <td>10.48550/arXiv.2304.14108</td>\n",
       "      <td>258352812.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>736973165f98105fec3729b7db414ae4d80fcbeb</td>\n",
       "      <td>None</td>\n",
       "      <td>2212.09748</td>\n",
       "      <td>journals/corr/abs-2212-09748</td>\n",
       "      <td>10.1109/ICCV51070.2023.00387</td>\n",
       "      <td>254854389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>e2e34dc10482795a94e401c343a78cb333960996</td>\n",
       "      <td>None</td>\n",
       "      <td>2211.13874</td>\n",
       "      <td>journals/corr/abs-2211-13874</td>\n",
       "      <td>10.1109/CVPR52729.2023.00043</td>\n",
       "      <td>254018271.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>7c8979a99c1a9b214fc6762ae8e73ee4b39749c0</td>\n",
       "      <td>None</td>\n",
       "      <td>2210.12867</td>\n",
       "      <td>conf/nips/PokleGK22</td>\n",
       "      <td>10.48550/arXiv.2210.12867</td>\n",
       "      <td>253098319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9</td>\n",
       "      <td>None</td>\n",
       "      <td>2210.08402</td>\n",
       "      <td>conf/nips/SchuhmannBVGWCC22</td>\n",
       "      <td>10.48550/arXiv.2210.08402</td>\n",
       "      <td>252917726.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>4530c25da949bb2185c50663158ef19d52e3c6b5</td>\n",
       "      <td>None</td>\n",
       "      <td>2206.00927</td>\n",
       "      <td>conf/nips/0011ZB0L022</td>\n",
       "      <td>10.48550/arXiv.2206.00927</td>\n",
       "      <td>249282317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            originalPaperId  \\\n",
       "0  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "1  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "2  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "3  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "4  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "5  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "6  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "7  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "8  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "9  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "\n",
       "                                    paperId abstract externalIds.ArXiv  \\\n",
       "0  9b91b3031ea159e4964d18b2ce703168660ecf46     None        2401.11605   \n",
       "1  0e8f1bb91bb4502966fa5e91e0610832dfe4240e     None        2401.08639   \n",
       "2  b994cf51a8c7cf5c13358a6110d7304d6d04c881     None        2310.18605   \n",
       "3  5003fdf35af631d4cb17fd3c1ce2469f665064f1     None        2305.08891   \n",
       "4  f9570989919338079088270a9cf1a7afc8db8093     None        2304.14108   \n",
       "5  736973165f98105fec3729b7db414ae4d80fcbeb     None        2212.09748   \n",
       "6  e2e34dc10482795a94e401c343a78cb333960996     None        2211.13874   \n",
       "7  7c8979a99c1a9b214fc6762ae8e73ee4b39749c0     None        2210.12867   \n",
       "8  e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9     None        2210.08402   \n",
       "9  4530c25da949bb2185c50663158ef19d52e3c6b5     None        2206.00927   \n",
       "\n",
       "               externalIds.DBLP               externalIds.DOI  \\\n",
       "0  journals/corr/abs-2401-11605     10.48550/arXiv.2401.11605   \n",
       "1  journals/corr/abs-2401-08639     10.48550/arXiv.2401.08639   \n",
       "2  journals/corr/abs-2310-18605     10.48550/arXiv.2310.18605   \n",
       "3  journals/corr/abs-2305-08891  10.1109/WACV57701.2024.00532   \n",
       "4  journals/corr/abs-2304-14108     10.48550/arXiv.2304.14108   \n",
       "5  journals/corr/abs-2212-09748  10.1109/ICCV51070.2023.00387   \n",
       "6  journals/corr/abs-2211-13874  10.1109/CVPR52729.2023.00043   \n",
       "7           conf/nips/PokleGK22     10.48550/arXiv.2210.12867   \n",
       "8   conf/nips/SchuhmannBVGWCC22     10.48550/arXiv.2210.08402   \n",
       "9         conf/nips/0011ZB0L022     10.48550/arXiv.2206.00927   \n",
       "\n",
       "   externalIds.CorpusId externalIds.MAG externalIds.PubMed  \\\n",
       "0           267069338.0             NaN                NaN   \n",
       "1           267028569.0             NaN                NaN   \n",
       "2           264590159.0             NaN                NaN   \n",
       "3           258714883.0             NaN                NaN   \n",
       "4           258352812.0             NaN                NaN   \n",
       "5           254854389.0             NaN                NaN   \n",
       "6           254018271.0             NaN                NaN   \n",
       "7           253098319.0             NaN                NaN   \n",
       "8           252917726.0             NaN                NaN   \n",
       "9           249282317.0             NaN                NaN   \n",
       "\n",
       "  externalIds.PubMedCentral externalIds.ACL  \n",
       "0                       NaN             NaN  \n",
       "1                       NaN             NaN  \n",
       "2                       NaN             NaN  \n",
       "3                       NaN             NaN  \n",
       "4                       NaN             NaN  \n",
       "5                       NaN             NaN  \n",
       "6                       NaN             NaN  \n",
       "7                       NaN             NaN  \n",
       "8                       NaN             NaN  \n",
       "9                       NaN             NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.abstract.isna()].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/2576 papers have abstract (7%)\n"
     ]
    }
   ],
   "source": [
    "print(f'{(~df2.abstract.isna()).sum()}/{len(df2)} papers have abstract ({(1-df2.abstract.isna().sum()/len(df2))*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sch.get_paper_references(paper_id=paper_id, fields=['title', 'paperId', 'externalIds', 'abstract'], limit=1).items[0]['citedPaper']\n",
    "df2 = pd.DataFrame(res)\n",
    "df2.abstract.isna().sum()/len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing crossref API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "      <th>referenced_works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "      <td>['https://openalex.org/W1504980292', 'https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D ...</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "      <td>['https://openalex.org/W1861492603', 'https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models ...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Insights from the Use of Previously Unseen Neu...</td>\n",
       "      <td>https://doi.org/10.48550/arxiv.2404.02189</td>\n",
       "      <td>https://openalex.org/W4393967825</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Efficient local correlation volume for unsuper...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00049</td>\n",
       "      <td>https://openalex.org/W4402904316</td>\n",
       "      <td>['https://openalex.org/W1513100184', 'https://...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fpath  \\\n",
       "0  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "1  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "3  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "4  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "\n",
       "                                               title  \\\n",
       "0                       Fixed Point Diffusion Models   \n",
       "1  BEVNeXt: Reviving Dense BEV Frameworks for 3D ...   \n",
       "2  Leveraging Pre-trained Multi-task Deep Models ...   \n",
       "3  Insights from the Use of Previously Unseen Neu...   \n",
       "4  Efficient local correlation volume for unsuper...   \n",
       "\n",
       "                                             doi  \\\n",
       "0              https://doi.org/10.1063/1.2121687   \n",
       "1   https://doi.org/10.1109/cvpr52733.2024.01901   \n",
       "2  https://doi.org/10.1109/cvprw63382.2024.00473   \n",
       "3      https://doi.org/10.48550/arxiv.2404.02189   \n",
       "4  https://doi.org/10.1109/cvprw63382.2024.00049   \n",
       "\n",
       "                               oaid  \\\n",
       "0  https://openalex.org/W2000456051   \n",
       "1  https://openalex.org/W4402727763   \n",
       "2  https://openalex.org/W4402916217   \n",
       "3  https://openalex.org/W4393967825   \n",
       "4  https://openalex.org/W4402904316   \n",
       "\n",
       "                                    referenced_works  \n",
       "0  ['https://openalex.org/W1504980292', 'https://...  \n",
       "1  ['https://openalex.org/W1861492603', 'https://...  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  ['https://openalex.org/W1513100184', 'https://...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-ids+refs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexed': {'date-parts': [[2025, 2, 21]],\n",
       "  'date-time': '2025-02-21T10:19:07Z',\n",
       "  'timestamp': 1740133147234,\n",
       "  'version': '3.37.3'},\n",
       " 'reference-count': 8,\n",
       " 'publisher': 'Institute of Electrical and Electronics Engineers (IEEE)',\n",
       " 'issue': '3',\n",
       " 'license': [{'start': {'date-parts': [[2011, 6, 1]],\n",
       "    'date-time': '2011-06-01T00:00:00Z',\n",
       "    'timestamp': 1306886400000},\n",
       "   'content-version': 'vor',\n",
       "   'delay-in-days': 0,\n",
       "   'URL': 'https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html'}],\n",
       " 'funder': [{'DOI': '10.13039/100000015',\n",
       "   'name': 'U.S. Department of Energy',\n",
       "   'doi-asserted-by': 'publisher',\n",
       "   'award': ['DEFG0210ER41650'],\n",
       "   'id': [{'id': '10.13039/100000015',\n",
       "     'id-type': 'DOI',\n",
       "     'asserted-by': 'publisher'}]}],\n",
       " 'content-domain': {'domain': [], 'crossmark-restriction': False},\n",
       " 'short-container-title': ['IEEE Trans. Appl. Supercond.'],\n",
       " 'published-print': {'date-parts': [[2011, 6]]},\n",
       " 'abstract': \"<jats:p>A 1.3 GHz test cavity has been designed to test wafer samples of superconducting materials. This mushroom shaped cavity, operating in TE<jats:sub>01</jats:sub>mode, creates a unique distribution of surface fields. The surface magnetic field on the sample wafer is 3.75 times greater than elsewhere on the Niobium cavity surface. This field design is made possible through dielectrically loading the cavity by locating a hemisphere of ultra-pure sapphire just above the sample wafer. The sapphire pulls the fields away from the walls so the maximum field the Nb surface sees is 25% of the surface field on the sample. In this manner, it should be possible to drive the sample wafer well beyond the BCS limit for Niobium while still maintaining a respectable Q. The sapphire's purity must be tested for its loss tangent and dielectric constant to finalize the design of the mushroom test cavity. A sapphire loaded CEBAF cavity has been constructed and tested. The results on the dielectric constant and loss tangent will be presented.</jats:p>\",\n",
       " 'DOI': '10.1109/tasc.2010.2088091',\n",
       " 'type': 'journal-article',\n",
       " 'created': {'date-parts': [[2010, 11, 23]],\n",
       "  'date-time': '2010-11-23T21:02:28Z',\n",
       "  'timestamp': 1290546148000},\n",
       " 'page': '1903-1907',\n",
       " 'source': 'Crossref',\n",
       " 'is-referenced-by-count': 1,\n",
       " 'title': ['Ultra-Gradient Test Cavity for Testing SRF Wafer Samples'],\n",
       " 'prefix': '10.1109',\n",
       " 'volume': '21',\n",
       " 'author': [{'given': 'Nathaniel J.',\n",
       "   'family': 'Pogue',\n",
       "   'sequence': 'first',\n",
       "   'affiliation': []},\n",
       "  {'given': 'Peter M.',\n",
       "   'family': 'McIntyre',\n",
       "   'sequence': 'additional',\n",
       "   'affiliation': []},\n",
       "  {'given': 'Akhdiyor I.',\n",
       "   'family': 'Sattarov',\n",
       "   'sequence': 'additional',\n",
       "   'affiliation': []},\n",
       "  {'given': 'Charles',\n",
       "   'family': 'Reece',\n",
       "   'sequence': 'additional',\n",
       "   'affiliation': []}],\n",
       " 'member': '263',\n",
       " 'reference': [{'key': 'ref4',\n",
       "   'doi-asserted-by': 'publisher',\n",
       "   'DOI': '10.1088/0022-3727/27/10/033'},\n",
       "  {'journal-title': 'A sapphire loaded cavity for surface impedance measurementsdesign construction and commissioning',\n",
       "   'year': '0',\n",
       "   'author': 'phillips',\n",
       "   'key': 'ref3'},\n",
       "  {'key': 'ref6', 'doi-asserted-by': 'publisher', 'DOI': '10.1063/1.1663581'},\n",
       "  {'key': 'ref5',\n",
       "   'doi-asserted-by': 'publisher',\n",
       "   'DOI': '10.1103/PhysRevB.38.6538'},\n",
       "  {'key': 'ref8',\n",
       "   'doi-asserted-by': 'publisher',\n",
       "   'DOI': '10.1088/0957-0233/10/5/308'},\n",
       "  {'journal-title': 'Personal communication',\n",
       "   'year': '0',\n",
       "   'author': 'wang',\n",
       "   'key': 'ref7'},\n",
       "  {'key': 'ref2',\n",
       "   'doi-asserted-by': 'publisher',\n",
       "   'DOI': '10.1109/PAC.2005.1591773'},\n",
       "  {'key': 'ref1', 'doi-asserted-by': 'publisher', 'DOI': '10.1063/1.2162264'}],\n",
       " 'container-title': ['IEEE Transactions on Applied Superconductivity'],\n",
       " 'original-title': [],\n",
       " 'link': [{'URL': 'http://ieeexplore.ieee.org/iel5/77/5776774/05640689.pdf?arnumber=5640689',\n",
       "   'content-type': 'application/pdf',\n",
       "   'content-version': 'vor',\n",
       "   'intended-application': 'text-mining'},\n",
       "  {'URL': 'http://xplorestaging.ieee.org/ielx5/77/5776774/05640689.pdf?arnumber=5640689',\n",
       "   'content-type': 'unspecified',\n",
       "   'content-version': 'vor',\n",
       "   'intended-application': 'similarity-checking'}],\n",
       " 'deposited': {'date-parts': [[2021, 10, 10]],\n",
       "  'date-time': '2021-10-10T23:47:16Z',\n",
       "  'timestamp': 1633909636000},\n",
       " 'score': 1,\n",
       " 'resource': {'primary': {'URL': 'http://ieeexplore.ieee.org/document/5640689/'}},\n",
       " 'subtitle': [],\n",
       " 'short-title': [],\n",
       " 'issued': {'date-parts': [[2011, 6]]},\n",
       " 'references-count': 8,\n",
       " 'journal-issue': {'issue': '3',\n",
       "  'published-print': {'date-parts': [[2011, 6]]}},\n",
       " 'URL': 'https://doi.org/10.1109/tasc.2010.2088091',\n",
       " 'relation': {},\n",
       " 'ISSN': ['1051-8223', '1558-2515'],\n",
       " 'issn-type': [{'type': 'print', 'value': '1051-8223'},\n",
       "  {'type': 'electronic', 'value': '1558-2515'}],\n",
       " 'subject': [],\n",
       " 'published': {'date-parts': [[2011, 6]]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi = '10.1109/TASC.2010.2088091'\n",
    "crossref_url = f'https://api.crossref.org/works/{doi}'\n",
    "response = requests.get(crossref_url)\n",
    "data = response.json()\n",
    "data['message']['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'message-type': 'work-list',\n",
       " 'message-version': '1.0.0',\n",
       " 'message': {'facets': {},\n",
       "  'total-results': 2004128,\n",
       "  'items': [{'indexed': {'date-parts': [[2025, 4, 16]],\n",
       "     'date-time': '2025-04-16T08:30:45Z',\n",
       "     'timestamp': 1744792245500,\n",
       "     'version': '3.28.0'},\n",
       "    'reference-count': 73,\n",
       "    'publisher': 'IEEE',\n",
       "    'license': [{'start': {'date-parts': [[2024, 6, 16]],\n",
       "       'date-time': '2024-06-16T00:00:00Z',\n",
       "       'timestamp': 1718496000000},\n",
       "      'content-version': 'stm-asf',\n",
       "      'delay-in-days': 0,\n",
       "      'URL': 'https://doi.org/10.15223/policy-029'},\n",
       "     {'start': {'date-parts': [[2024, 6, 16]],\n",
       "       'date-time': '2024-06-16T00:00:00Z',\n",
       "       'timestamp': 1718496000000},\n",
       "      'content-version': 'stm-asf',\n",
       "      'delay-in-days': 0,\n",
       "      'URL': 'https://doi.org/10.15223/policy-037'}],\n",
       "    'content-domain': {'domain': [], 'crossmark-restriction': False},\n",
       "    'published-print': {'date-parts': [[2024, 6, 16]]},\n",
       "    'DOI': '10.1109/cvpr52733.2024.01901',\n",
       "    'type': 'proceedings-article',\n",
       "    'created': {'date-parts': [[2024, 9, 16]],\n",
       "     'date-time': '2024-09-16T17:34:53Z',\n",
       "     'timestamp': 1726508093000},\n",
       "    'page': '20113-20123',\n",
       "    'source': 'Crossref',\n",
       "    'is-referenced-by-count': 10,\n",
       "    'title': ['BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection'],\n",
       "    'prefix': '10.1109',\n",
       "    'author': [{'given': 'Zhenxin',\n",
       "      'family': 'Li',\n",
       "      'sequence': 'first',\n",
       "      'affiliation': [{'name': 'School of CS, Fudan University,Shanghai Key Lab of Intell. Info. Processing'}]},\n",
       "     {'given': 'Shiyi',\n",
       "      'family': 'Lan',\n",
       "      'sequence': 'additional',\n",
       "      'affiliation': [{'name': 'NVIDIA'}]},\n",
       "     {'given': 'Jose M.',\n",
       "      'family': 'Alvarez',\n",
       "      'sequence': 'additional',\n",
       "      'affiliation': [{'name': 'NVIDIA'}]},\n",
       "     {'given': 'Zuxuan',\n",
       "      'family': 'Wu',\n",
       "      'sequence': 'additional',\n",
       "      'affiliation': [{'name': 'School of CS, Fudan University,Shanghai Key Lab of Intell. Info. Processing'}]}],\n",
       "    'member': '263',\n",
       "    'reference': [{'key': 'ref1',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.3390/rs14153824'},\n",
       "     {'key': 'ref2',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52688.2022.00116'},\n",
       "     {'key': 'ref3',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR42600.2020.01164'},\n",
       "     {'key': 'ref4',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/TCSVT.2017.2740321'},\n",
       "     {'key': 'ref5',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-030-58452-8_13'},\n",
       "     {'key': 'ref6',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.01296'},\n",
       "     {'key': 'ref7',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.02076'},\n",
       "     {'key': 'ref8',\n",
       "      'article-title': 'Vision transformer adapter for dense predictions',\n",
       "      'author': 'Chen',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'ICLR'},\n",
       "     {'key': 'ref9',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR.2009.5206848'},\n",
       "     {'key': 'ref10',\n",
       "      'article-title': 'An image is worth 16x16 words: Transform-ers for image recognition at scale',\n",
       "      'author': 'Dosovitskiy',\n",
       "      'year': '2021',\n",
       "      'journal-title': 'ICLR'},\n",
       "     {'key': 'ref11',\n",
       "      'first-page': '8458',\n",
       "      'article-title': 'Em-bracing single stride 3d object detector with sparse transformer',\n",
       "      'author': 'Fan',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'CVPR'},\n",
       "     {'key': 'ref12',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/TPAMI.2019.2938758'},\n",
       "     {'key': 'ref13',\n",
       "      'article-title': 'Exploring recurrent long-term temporal fusion for multi-view 3d perception',\n",
       "      'author': 'Han',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref14',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR.2016.90'},\n",
       "     {'key': 'ref15',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV48922.2021.01499'},\n",
       "     {'key': 'ref16',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.01712'},\n",
       "     {'key': 'ref17',\n",
       "      'article-title': 'Bevdet4d: Exploit temporal cues in multi-camera 3d object detection',\n",
       "      'author': 'Huang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref18',\n",
       "      'article-title': 'Bevpoolv2: A cutting-edge implementation of bevdet toward deployment',\n",
       "      'author': 'Huang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref19',\n",
       "      'article-title': 'Bevdet: High-performance multi-camera 3d object detection in bird-eye-view',\n",
       "      'author': 'Huang',\n",
       "      'year': '2021',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref20',\n",
       "      'article-title': 'Leveraging vision-centric multimodal expertise for 3d object detection',\n",
       "      'author': 'Huang',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref21',\n",
       "      'article-title': 'Far3d: Expanding the horizon for surround-view 3d object detection',\n",
       "      'author': 'Jiang',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref22',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.02073'},\n",
       "     {'key': 'ref23',\n",
       "      'article-title': 'Efficient inference in fully connected crfs with gaussian edge potentials',\n",
       "      'volume': '24',\n",
       "      'author': 'Krähenbuhl',\n",
       "      'year': '2011',\n",
       "      'journal-title': 'NeurIPS'},\n",
       "     {'key': 'ref24',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV48922.2021.00339'},\n",
       "     {'key': 'ref25',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.02274'},\n",
       "     {'key': 'ref26',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPRW.2019.00103'},\n",
       "     {'key': 'ref27',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/IROS55552.2023.10341778'},\n",
       "     {'key': 'ref28',\n",
       "      'article-title': 'Bevstereo: Enhancing depth estimation in multi-view 3d object detection with dynamic temporal stereo',\n",
       "      'author': 'Li',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'AAAI'},\n",
       "     {'key': 'ref29',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1609/aaai.v37i2.25233'},\n",
       "     {'key': 'ref30',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-031-20077-9_1'},\n",
       "     {'key': 'ref31',\n",
       "      'article-title': 'Fb-occ: 3d occupancy prediction based on forward-backward view transformation',\n",
       "      'author': 'Li',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref32',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.00637'},\n",
       "     {'key': 'ref33',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-319-10602-1_48'},\n",
       "     {'key': 'ref34',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR.2017.106'},\n",
       "     {'key': 'ref35',\n",
       "      'article-title': 'Sparse4d: Multi-view 3d object detection with sparse spatial-temporal fusion',\n",
       "      'author': 'Lin',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref36',\n",
       "      'article-title': 'Sparse4d v2: Recurrent temporal fusion with sparse model',\n",
       "      'author': 'Lin',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref37',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR.2015.7299152'},\n",
       "     {'key': 'ref38',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.01703'},\n",
       "     {'key': 'ref39',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR.2014.97'},\n",
       "     {'key': 'ref40',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-031-19812-0_31'},\n",
       "     {'key': 'ref41',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.00302'},\n",
       "     {'key': 'ref42',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52688.2022.01167'},\n",
       "     {'key': 'ref43',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICRA48891.2023.10160968'},\n",
       "     {'key': 'ref44',\n",
       "      'article-title': 'Decoupled weight decay regularization',\n",
       "      'author': 'Loshchilov',\n",
       "      'year': '2017',\n",
       "      'journal-title': 'ICLR'},\n",
       "     {'key': 'ref45',\n",
       "      'article-title': 'Bev-seg: Birds eye view semantic segmentation using geometry and semantic point cloud',\n",
       "      'author': 'Ng',\n",
       "      'year': '2020',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref46',\n",
       "      'article-title': 'Scene transformer: A unified architecture for predicting multiple agent trajectories',\n",
       "      'author': 'Ngiam',\n",
       "      'year': '2021',\n",
       "      'journal-title': 'ICLR'},\n",
       "     {'key': 'ref47',\n",
       "      'article-title': 'Dinov2: Learning robust visual features without supervision',\n",
       "      'author': 'Oquab',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref48',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV48922.2021.00313'},\n",
       "     {'key': 'ref49',\n",
       "      'article-title': 'Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection',\n",
       "      'author': 'Park',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'ICLR'},\n",
       "     {'key': 'ref50',\n",
       "      'first-page': '194',\n",
       "      'article-title': 'Lift, splat, shoot:Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d',\n",
       "      'author': 'Philion',\n",
       "      'year': '2020',\n",
       "      'journal-title': 'ECCV'},\n",
       "     {'key': 'ref51',\n",
       "      'first-page': '3749',\n",
       "      'article-title': 'Dort: Modeling dynamic objects in recurrent for multi-camera 3d object detection and tracking',\n",
       "      'author': 'Qing',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'CoRL'},\n",
       "     {'key': 'ref52',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV.2019.00852'},\n",
       "     {'journal-title': 'ECCV, 2022',\n",
       "      'article-title': 'Pillarnet: High-performance pillar-based 3d object detection',\n",
       "      'author': 'Shi',\n",
       "      'key': 'ref53'},\n",
       "     {'key': 'ref54',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-031-20080-9_25'},\n",
       "     {'key': 'ref55',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/TIV.2023.3274536'},\n",
       "     {'key': 'ref56',\n",
       "      'article-title': 'Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving',\n",
       "      'author': 'Tian',\n",
       "      'year': '2023',\n",
       "      'journal-title': 'NeurIPS'},\n",
       "     {'key': 'ref57',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.00772'},\n",
       "     {'key': 'ref58',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.48550/ARXIV.1706.03762'},\n",
       "     {'key': 'ref59',\n",
       "      'article-title': 'Focal-petr: Em-bracing foreground for efficient multi-camera 3d object detection',\n",
       "      'author': 'Wang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref60',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.00335'},\n",
       "     {'key': 'ref61',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCVW54120.2021.00107'},\n",
       "     {'key': 'ref62',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.01385'},\n",
       "     {'key': 'ref63',\n",
       "      'first-page': '180',\n",
       "      'article-title': 'Detr3d: 3d object detection from multi-view images via 3d-to-2d queries',\n",
       "      'author': 'Wang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'CoRL'},\n",
       "     {'key': 'ref64',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV51070.2023.01675'},\n",
       "     {'key': 'ref65',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.01710'},\n",
       "     {'key': 'ref66',\n",
       "      'article-title': 'Quality matters: Embracing quality clues for robust 3d multi-object tracking',\n",
       "      'author': 'Yang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref67',\n",
       "      'first-page': '1992',\n",
       "      'article-title': 'Deepinteraction: 3d object detection via modality interaction',\n",
       "      'volume': '35',\n",
       "      'author': 'Yang',\n",
       "      'year': '2022',\n",
       "      'journal-title': 'NeurIPS'},\n",
       "     {'key': 'ref68',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR46437.2021.01161'},\n",
       "     {'key': 'ref69',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/ICCV.2015.179'},\n",
       "     {'key': 'ref70',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1007/978-3-031-19839-7_29'},\n",
       "     {'key': 'ref71',\n",
       "      'doi-asserted-by': 'publisher',\n",
       "      'DOI': '10.1109/CVPR52729.2023.01713'},\n",
       "     {'key': 'ref72',\n",
       "      'article-title': 'Class-balanced grouping and sampling for point cloud 3d object detection',\n",
       "      'author': 'Zhu',\n",
       "      'year': '2019',\n",
       "      'journal-title': 'arXiv preprint'},\n",
       "     {'key': 'ref73',\n",
       "      'article-title': 'Deformable detr: Deformable transformers for end-to-end object detection',\n",
       "      'author': 'Zhu',\n",
       "      'year': '2020',\n",
       "      'journal-title': 'ICLR'}],\n",
       "    'event': {'name': '2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)',\n",
       "     'start': {'date-parts': [[2024, 6, 16]]},\n",
       "     'location': 'Seattle, WA, USA',\n",
       "     'end': {'date-parts': [[2024, 6, 22]]}},\n",
       "    'container-title': ['2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'],\n",
       "    'link': [{'URL': 'http://xplorestaging.ieee.org/ielx8/10654794/10654797/10657020.pdf?arnumber=10657020',\n",
       "      'content-type': 'unspecified',\n",
       "      'content-version': 'vor',\n",
       "      'intended-application': 'similarity-checking'}],\n",
       "    'deposited': {'date-parts': [[2024, 9, 20]],\n",
       "     'date-time': '2024-09-20T05:30:08Z',\n",
       "     'timestamp': 1726810208000},\n",
       "    'score': 69.701775,\n",
       "    'resource': {'primary': {'URL': 'https://ieeexplore.ieee.org/document/10657020/'}},\n",
       "    'issued': {'date-parts': [[2024, 6, 16]]},\n",
       "    'references-count': 73,\n",
       "    'URL': 'https://doi.org/10.1109/cvpr52733.2024.01901',\n",
       "    'published': {'date-parts': [[2024, 6, 16]]}}],\n",
       "  'items-per-page': 1,\n",
       "  'query': {'start-index': 0, 'search-terms': None}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_paper = df.iloc[1].title\n",
    "crossref_url = f'https://api.crossref.org/works?query.bibliographic={name_of_paper}&rows=1'\n",
    "response = requests.get(crossref_url)\n",
    "data = response.json()\n",
    "data.get('message', {}).get('items', [None])[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oaid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>type</th>\n",
       "      <th>topic</th>\n",
       "      <th>domain</th>\n",
       "      <th>field</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2194775991</td>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</td>\n",
       "      <td>article</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               oaid                                         title  \\\n",
       "0  https://openalex.org/W2194775991  Deep Residual Learning for Image Recognition   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        abstract  \\\n",
       "0  Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.   \n",
       "\n",
       "      type                                 topic             domain             field                                 subfield  \n",
       "0  article  Advanced Neural Network Applications  Physical Sciences  Computer Science  Computer Vision and Pattern Recognition  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_df = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata/openalex-refs-abstracts.csv')\n",
    "ref_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:37<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import trange\n",
    "\n",
    "fields_of_interest = ','.join(['DOI', 'title', 'type','references-count', 'reference', 'abstract'])\n",
    "\n",
    "all_raw = []\n",
    "for i in trange(200):\n",
    "    name_of_paper = ref_df.iloc[i].title if i > 0 else 'Ultra-Gradient Test Cavity for Testing SRF Wafer Samples'\n",
    "    crossref_url = f'https://api.crossref.org/works?query.bibliographic={name_of_paper}&rows=2&select={fields_of_interest}'\n",
    "    response = requests.get(crossref_url)\n",
    "    data = response.json()\n",
    "    raw = data.get('message', {}).get('items', [None])[0]  # FIXME: Do proper relevance comparison as advised\n",
    "    all_raw.append(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,   0,  29,   0,  30,  14,   0,   0,  24,  10,   0,  91,  92,\n",
       "        68,  23,  92,  28,  66, 100,  36,   0,   0,   0,  52,  97,   0,\n",
       "         0,  33, 236,  24,   0,  86,  24,  55,  41,  39,  70,  31,  35,\n",
       "        41, 125,  87,  13,  26,   0,  17,  58,  43,  73,  67,  62,  51,\n",
       "        27,   0,  62,  48,  35,  28,  89,  57,  71,  81,  83,   0, 152,\n",
       "        53,   0, 100, 136,  61,   0,   0, 121,  42, 117,  16,  55,  65,\n",
       "        54,  22,  37,  76,   8,  28,  24,  56,  50,  25,   0,   0,   0,\n",
       "        16,  26,  21, 106,  38,  29,  39,   0,  58,   0,  76,   0,   0,\n",
       "        63,  48,   0,   0,  38,  33,  18,  44,   9,  60,   0,  53,  19,\n",
       "         0,  47,  95,   0,  39,   0,   0,  64,   0,  21,  60,  79,   1,\n",
       "         0, 287,   0,   0,  55, 168,  12,   0,  33,   0,   0,  61,  53,\n",
       "        54,  36,  38,  62,  43,  56,  61,  30,  81,  60,  62,   0,  40,\n",
       "        47,  21, 210,   0,   0,  43,  36,  51,  44,  48,   0,  60,  24,\n",
       "         0,  47,  77,  56,  57,  19,  45,  48,  27,  59,  64,  38,  50,\n",
       "         0,  49,  22,   0,  28,  35,  53,  23,  64,  63,  41,  15,  59,\n",
       "        31,   0,  35,  32,   0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_cnts = pd.Series([raw.get('references-count', 0) for raw in all_raw])\n",
    "ref_cnts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOI': '10.21123/bsj.2024.11089',\n",
       " 'title': ['Improved Deep Perceptual Hashing Algorithm (IDP-HA), Information Retrieval System, Microsoft Common Objects In Context (MS COCO), Remote Cloud Computing, Computer science, information systems',\n",
       "  'نظام محسّن لاسترجاع المعلومات قائم على خوارزمية التجزئة الإدراكية العميقة للحوسبة السحابية عن بعد'],\n",
       " 'type': 'journal-article',\n",
       " 'references-count': 0,\n",
       " 'abstract': '<jats:p>يمكن أن يعزى نمو استرجاع المعلومات والخدمات المرتبطة بها إلى التقدم التقني. وفي الوقت نفسه، تتأثر الطرق التقليدية لاسترجاع المعلومات بقيود الأداء والدقة وقابلية التوسع. يعد نظام استرجاع المعلومات للحوسبة السحابية البعيدة الذي يعتمد على خوارزمية التجزئة الإدراكية العميقة المحسنة (IDP-HA) أحد الحلول التي تم تطويرها لحل هذه القيود. تُستخدم الأنظمة على نطاق واسع نظرًا لقدرتها على التعرف على الأنماط المعقدة في البيانات. لا تزال دقة قياس تشابه المعلومات غير متوفرة بسبب التعقيد الكامن في البيانات وطرق القياس. يستخدم أسلوب التجزئة الإدراكي العميق أطر عمل الشبكة العصبية العميقة (DNN) لاستخراج الميزات الهرمية من الصور المدخلة من مجموعة بيانات Microsoft Common Objects in context (MS COCO). يعد مرشح Gaussian (GF) أداة تستخدم في المعالجة المسبقة للصور الفردية لرؤى الكمبيوتر المختلفة. وبعد ذلك، تقوم هذه الطريقة بإنشاء أرقام التجزئة الرقمية عن طريق وصف العناصر المرئية للصور باستخدام آلية العتبة. هدفها الأساسي هو تحسين مقياس التشابه للحفاظ على التشابه الإدراكي وضمان تشابه رموز التجزئة للصور القابلة للمقارنة بصريًا. يتم تقليل استخدام الذاكرة باستخدام دالة التجزئة كخطوة أولى في إنشاء اتصال بين قاعدة البيانات والاستعلام. يجد هذا النهج تطبيقات في أنظمة استرجاع الصور القائمة على المحتوى، واسترجاع الصور، وتجميع الصور، واكتشاف النسخ. وبشكل عام، فهو يوفر إطارًا قويًا لإنتاج تمثيلات صور مدمجة وذات أهمية دلالية. تم تحسين IDP-HA للحوسبة السحابية عن بعد لتعزيز متوسط الاستدعاء ومتوسط الدقة ومتوسط قياس F1 ومتوسط توقيت الاستعلام لعمليات استرداد البيانات. تعمل هذه الطريقة على تقليل زمن الوصول وزيادة كفاءة النظام عن طريق إنشاء تمثيلات ثنائية مدمجة لبيانات الوسائط المتعددة. يمكن أن يكون الاسترجاع القائم على التشابه البصري موثوقًا وطبيعيًا حيث يتم الحفاظ على التشابه الإدراكي.</jats:p>'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_raw[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"<jats:p>A 1.3 GHz test cavity has been designed to test wafer samples of superconducting materials. This mushroom shaped cavity, operating in TE<jats:sub>01</jats:sub>mode, creates a unique distribution of surface fields. The surface magnetic field on the sample wafer is 3.75 times greater than elsewhere on the Niobium cavity surface. This field design is made possible through dielectrically loading the cavity by locating a hemisphere of ultra-pure sapphire just above the sample wafer. The sapphire pulls the fields away from the walls so the maximum field the Nb surface sees is 25% of the surface field on the sample. In this manner, it should be possible to drive the sample wafer well beyond the BCS limit for Niobium while still maintaining a respectable Q. The sapphire's purity must be tested for its loss tangent and dielectric constant to finalize the design of the mushroom test cavity. A sapphire loaded CEBAF cavity has been constructed and tested. The results on the dielectric constant and loss tangent will be presented.</jats:p>\",\n",
       "       '',\n",
       "       '<jats:p>Classical Monte Carlo simulations have been carried out for liquid water in the NPT ensemble at 25\\u2009°C and 1 atm using six of the simpler intermolecular potential functions for the water dimer: Bernal–Fowler (BF), SPC, ST2, TIPS2, TIP3P, and TIP4P. Comparisons are made with experimental thermodynamic and structural data including the recent neutron diffraction results of Thiessen and Narten. The computed densities and potential energies are in reasonable accord with experiment except for the original BF model, which yields an 18% overestimate of the density and poor structural results. The TIPS2 and TIP4P potentials yield oxygen–oxygen partial structure functions in good agreement with the neutron diffraction results. The accord with the experimental OH and HH partial structure functions is poorer; however, the computed results for these functions are similar for all the potential functions. Consequently, the discrepancy may be due to the correction terms needed in processing the neutron data or to an effect uniformly neglected in the computations. Comparisons are also made for self-diffusion coefficients obtained from molecular dynamics simulations. Overall, the SPC, ST2, TIPS2, and TIP4P models give reasonable structural and thermodynamic descriptions of liquid water and they should be useful in simulations of aqueous solutions. The simplicity of the SPC, TIPS2, and TIP4P functions is also attractive from a computational standpoint.</jats:p>',\n",
       "       '<jats:p>يمكن أن يعزى نمو استرجاع المعلومات والخدمات المرتبطة بها إلى التقدم التقني. وفي الوقت نفسه، تتأثر الطرق التقليدية لاسترجاع المعلومات بقيود الأداء والدقة وقابلية التوسع. يعد نظام استرجاع المعلومات للحوسبة السحابية البعيدة الذي يعتمد على خوارزمية التجزئة الإدراكية العميقة المحسنة (IDP-HA) أحد الحلول التي تم تطويرها لحل هذه القيود. تُستخدم الأنظمة على نطاق واسع نظرًا لقدرتها على التعرف على الأنماط المعقدة في البيانات. لا تزال دقة قياس تشابه المعلومات غير متوفرة بسبب التعقيد الكامن في البيانات وطرق القياس. يستخدم أسلوب التجزئة الإدراكي العميق أطر عمل الشبكة العصبية العميقة (DNN) لاستخراج الميزات الهرمية من الصور المدخلة من مجموعة بيانات Microsoft Common Objects in context (MS COCO). يعد مرشح Gaussian (GF) أداة تستخدم في المعالجة المسبقة للصور الفردية لرؤى الكمبيوتر المختلفة. وبعد ذلك، تقوم هذه الطريقة بإنشاء أرقام التجزئة الرقمية عن طريق وصف العناصر المرئية للصور باستخدام آلية العتبة. هدفها الأساسي هو تحسين مقياس التشابه للحفاظ على التشابه الإدراكي وضمان تشابه رموز التجزئة للصور القابلة للمقارنة بصريًا. يتم تقليل استخدام الذاكرة باستخدام دالة التجزئة كخطوة أولى في إنشاء اتصال بين قاعدة البيانات والاستعلام. يجد هذا النهج تطبيقات في أنظمة استرجاع الصور القائمة على المحتوى، واسترجاع الصور، وتجميع الصور، واكتشاف النسخ. وبشكل عام، فهو يوفر إطارًا قويًا لإنتاج تمثيلات صور مدمجة وذات أهمية دلالية. تم تحسين IDP-HA للحوسبة السحابية عن بعد لتعزيز متوسط الاستدعاء ومتوسط الدقة ومتوسط قياس F1 ومتوسط توقيت الاستعلام لعمليات استرداد البيانات. تعمل هذه الطريقة على تقليل زمن الوصول وزيادة كفاءة النظام عن طريق إنشاء تمثيلات ثنائية مدمجة لبيانات الوسائط المتعددة. يمكن أن يكون الاسترجاع القائم على التشابه البصري موثوقًا وطبيعيًا حيث يتم الحفاظ على التشابه الإدراكي.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>A new Lagrangian formulation is introduced. It can be used to make molecular dynamics (MD) calculations on systems under the most general, externally applied, conditions of stress. In this formulation the MD cell shape and size can change according to dynamical equations given by this Lagrangian. This new MD technique is well suited to the study of structural transformations in solids under external stress and at finite temperature. As an example of the use of this technique we show how a single crystal of Ni behaves under uniform uniaxial compressive and tensile loads. This work confirms some of the results of static (i.e., zero temperature) calculations reported in the literature. We also show that some results regarding the stress-strain relation obtained by static calculations are invalid at finite temperature. We find that, under compressive loading, our model of Ni shows a bifurcation in its stress-strain relation; this bifurcation provides a link in configuration space between cubic and hexagonal close packing. It is suggested that such a transformation could perhaps be observed experimentally under extreme conditions of shock.</jats:p>',\n",
       "       '', '', '',\n",
       "       '<jats:p>Deep networks for classification problems may be enhanced by training with proper regularization exploiting intrinsic class similarities.</jats:p>',\n",
       "       '', '', '', '', '',\n",
       "       '<jats:p>The ability of simple potential functions to reproduce accurately the density of liquid water from −37 to 100\\u200a°C at 1 to 10\\u200a000 atm has been further explored. The result is the five-site TIP5P model, which yields significantly improved results; the average error in the density over the 100° temperature range from −37.5 to 62.5\\u200a°C at 1 atm is only 0.006 g cm−3. Classical Monte Carlo statistical mechanics calculations have been performed to optimize the parameters, especially the position of the negative charges along the lone-pair directions. Initial calculations with 216 molecules in the NPT ensemble at 1 atm focused on finding a model that reproduced the shape of the liquid density curve as a function of temperature. Calculations performed for 512 molecules with the final TIP5P model demonstrate that the density maximum near 4\\u200a°C at 1 atm is reproduced, while high-quality structural and thermodynamic results are maintained. Attainment of high precision for the low-temperature runs required sampling for more than 1 billion Monte Carlo configurations. In addition, the dielectric constant was computed from the response to an applied electric field; the result is 81.5±1.5 at 25\\u200a°C and the experimental curve is mirrored from 0–100\\u200a°C at 1 atm. The TIP5P model is also found to perform well as a function of pressure; the density of liquid water at 25\\u200a°C is reproduced with an average error of ∼2% over the range from 1 to 10\\u200a000 atm, and the shift of the temperature of maximum density to lower temperature with increasing pressure is also obtained.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>A re-parameterization of the standard TIP4P water model for use with Ewald techniques is introduced, providing an overall global improvement in water properties relative to several popular nonpolarizable and polarizable water potentials. Using high precision simulations, and careful application of standard analytical corrections, we show that the new TIP4P-Ew potential has a density maximum at ∼1\\u200a°C, and reproduces experimental bulk-densities and the enthalpy of vaporization, ΔHvap, from −37.5 to 127\\u200a°C at 1 atm with an absolute average error of less than 1%. Structural properties are in very good agreement with x-ray scattering intensities at temperatures between 0 and 77\\u200a°C and dynamical properties such as self-diffusion coefficient are in excellent agreement with experiment. The parameterization approach used can be easily generalized to rehabilitate any water force field using available experimental data over a range of thermodynamic points.</jats:p>',\n",
       "       '', '', '', '',\n",
       "       '<jats:p>Fire is a serious security threat that can lead to casualties, property damage, and environmental damage. However, despite the availability of object detection algorithms, challenges persist in detecting fires and smoke. These challenges include slow convergence speed, poor performance in detecting small targets, and high computational cost limiting deployments. In this paper, Fire smoke and human detection based on ConvNeXt and Mixed encoder (FCM-DETR), an end-to-end object detection algorithm based on Deformable DETR, is proposed. Firstly, we introduce ConvNeXt to take the place of Resnet, which greatly reduces the amount of computation and improves the ability to extract irregular flame features. Secondly, to effectively process multi-scale features, the original encoder is decoupled into two modules. Then Mixed encoder, an innovative encoder structure, is proposed, resulting in excellent performance of multi-scale fire and smoke features fusion. What can’t be overlooked is that the encoder block is compatible with any DETR-based models. Finally, the convergence speed is accelerated and the mean of average precision is improved by applying a novel loss function called Powerful IoU v2. The experimental results indicate that our model achieves the best detection accuracy compared to other models, with mAP reaching 66.7%, mAPs and Accuracyfire achieving impressive 50.2% and 98.05%, respectively.</jats:p>',\n",
       "       '<jats:p>The ability of several water models to predict the properties of ices is discussed. The emphasis is put on the results for the densities and the coexistence curves between the different ice forms. It is concluded that none of the most commonly used rigid models is satisfactory. A new model specifically designed to cope with solid-phase properties is proposed. The parameters have been obtained by fitting the equation of state and selected points of the melting lines and of the coexistence lines involving different ice forms. The phase diagram is then calculated for the new potential. The predicted melting temperature of hexagonal ice (Ih) at 1bar is 272.2K. This excellent value does not imply a deterioration of the rest of the properties. In fact, the predictions for both the densities and the coexistence curves are better than for TIP4P, which previously yielded the best estimations of the ice properties.</jats:p>',\n",
       "       '<jats:p>An overview is provided on the development and status of potential energy functions that are used in atomic-level statistical mechanics and molecular dynamics simulations of water and of organic and biomolecular systems. Some topics that are considered are the form of force fields, their parameterization and performance, simulations of organic liquids, computation of free energies of hydration, universal extension for organic molecules, and choice of atomic charges. The discussion of water models covers some history, performance issues, and special topics such as nuclear quantum effects.</jats:p>',\n",
       "       '', '', '', '', '', '',\n",
       "       '<jats:p>We have performed long molecular dynamics simulations of water using four popular water models, namely simple point charge (SPC), extended simple point charge (SPC/E), and the three point (TIP3P) and four point (TIP4P) transferable intermolecular potentials. System sizes of 216 and 820 molecules were used to study the dependence of properties on the system size. All systems were simulated at 300 K with and without reaction fields and with two different cutoff radii, in order to study the impact of the cutoff treatment on density, energy, dynamic, and dielectric properties. Furthermore we generated two special-purpose water models based on the SPC and TIP4P models, for use with a reaction field. The atomic charges and the Lennard-Jones C12 parameter were optimized to reproduce the correct energy and pressure using the weak coupling algorithm for parameters. Indeed, in simulations without parameter coupling of both new models the density and potential energy were found to be close to the experimental values. The other properties of these models that we called SPC/RF and TIP4P/RF (where RF stands for reaction field) are evaluated and discussed.</jats:p>',\n",
       "       '<jats:p>Thermodynamic integration along a path that coincides with the saturation line is proposed as an efficient means for evaluation of phase equilibria by molecular simulation. The technique allows coexistence to be determined by just one simulation, without ever attempting or performing particle insertions. Prior knowledge of one coexistence point is required to start the procedure. Integration then advances from this state according to the Clapeyron formula—a first-order ordinary differential equation that prescribes how the pressure must change with temperature to maintain coexistence. The method is unusual in the context of thermodynamic integration in that the path is not known at the outset of the process; results from each simulation determine the course that the integration subsequently takes. Predictor–corrector methods among standard numerical techniques are shown to be particularly well suited for this type of integration. A typical integration step along the saturation line proceeds as follows: An increment in the temperature is chosen, and the saturation pressure at the new temperature is ‘‘predicted’’ from previous data (the initial coexistence datum and/or previous simulations). Simultaneous but independent NPT simulations of the coexisting phases are initiated at the said conditions. Averages taken throughout the simulations are repeatedly used to ‘‘correct’’ the estimate of the pressure to convergence. Thus strictly the pressure is not fixed during the simulation. Vapor–liquid coexistence of the van der Waals model is first used to study the numerical integration method without the complications of molecular simulation. In a second application the phase envelope of the Lennard-Jones model fluid is computed, and many variations of the technique are examined. Overall, the results are remarkably consistent and in agreement with previous simulation studies. Difficulty is encountered upon approach of the critical point, but, by artificially coupling the simulation volumes, the method remains effective in this regime so long as a suitably small integration step is employed. Many extensions and improvements of the technique are discussed.</jats:p>',\n",
       "       '', '', '', '', '', '', '',\n",
       "       '<jats:p>The melting temperature of ice Ih for several commonly used models of water (SPC, SPC/E,TIP3P,TIP4P, TIP4P/Ew, and TIP5P) is obtained from computer simulations at p=1bar. Since the melting temperature of ice Ih for the TIP4P model is now known [E. Sanz, C. Vega, J. L. F. Abascal, and L. G. MacDowell, Phys. Rev. Lett.\\u200892, 255701 (2004)], it is possible to use the Gibbs–Duhem methodology [D. Kofke, J. Chem. Phys.\\u200898, 4149 (1993)] to evaluate the melting temperature of ice Ih for other potential models of water. We have found that the melting temperatures of ice Ih for SPC, SPC/E, TIP3P, TIP4P, TIP4P/Ew, and TIP5P models are T=190K, 215K, 146K, 232K, 245K, and 274K, respectively. The relative stability of ice Ih with respect to ice II for these models has also been considered. It turns out that for SPC, SPC/E, TIP3P, and TIP5P the stable phase at the normal melting point is ice II (so that ice Ih is not a thermodynamically stable phase for these models). For TIP4P and TIP4P/Ew, ice Ih is the stable solid phase at the standard melting point. The location of the negative charge along the H–O–H bisector appears as a critical factor in the determination of the relative stability between the Ih and II ice forms. The methodology proposed in this paper can be used to investigate the effect upon a coexistence line due to a change in the potential parameters.</jats:p>',\n",
       "       '<jats:p>The five-site transferable interaction potential (TIP5P) for water [M. W. Mahoney and W. L. Jorgensen, J. Chem. Phys. 112, 8910 (2000)] is most accurate at reproducing experimental data when used with a simple spherical cutoff for the long-ranged electrostatic interactions. When used with other methods for treating long-ranged interactions, the model is considerably less accurate. With small modifications, a new TIP5P-like potential can be made which is very accurate for liquid water when used with Ewald sums, a more physical and increasingly more commonly used method for treating long-ranged electrostatic interactions. The new model demonstrates a density maximum near 4\\u200a°C, like the TIP5P model, and otherwise is similar to the TIP5P model for thermodynamic, dielectric, and dynamical properties of liquid water over a range of temperatures and densities. An analysis of this and other commonly used water models reveals how the quadrupole moment of a model can influence the dielectric response of liquid water.</jats:p>',\n",
       "       '', '', '',\n",
       "       '<jats:p>The self-diffusion coefficient of water is reported to −31°C, where the activation energy reaches 11 kcal/mole compared with 4.5 kcal/mole at 25°C. The similarity of the temperature dependence of the fraction of broken hydrogen bonds, as inferred from Raman and infrared data, and the diffusion coefficient over the entire liquid range forms the basis of empirical support of the dominant role of hydrogen bonding in the fundamental diffusion mechanism.</jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '<jats:p>We report a new, high-quality x-ray scattering experiment on pure ambient water using a synchrotron beam line at the Advanced Light Source at Lawrence Berkeley National Laboratory. Several factors contribute to the improved quality of our intensity curves including use of a highly monochromatic source, a well-characterized polarization correction, a Compton scattering correction that includes electron correlation, and more accurate intensities using a modern charge coupled device (CCD) detector. We provide a comprehensive description of the data processing that we have used for correcting systematic errors, and we provide an estimate of our remaining random errors. The resulting error estimates of our data are smaller then the discrepancies between data sets collected in past x-ray experiments. We find that the older x-ray curves support a family of gOO(r)’s that exhibit a smaller first peak (∼2.2), while the current data is better fit with a family of gOO(r)’s with a first peak height of 2.8, and systematic shifts in all peak positions to smaller r.</jats:p>',\n",
       "       '<jats:p>We examine five different popular rigid water models (SPC, SPCE, TIP3P, TIP4P, and TIP5P) using molecular dynamics simulations in order to investigate the hydrophobic hydration and interaction of apolar Lennard-Jones solutes as a function of temperature in the range between 275 and 375 K along the 0.1 MPa isobar. For all investigated models and state points we calculate the excess chemical potential for the noble gases and methane employing the Widom particle insertion technique. All water models exhibit too small hydration entropies, but show a clear hierarchy. TIP3P shows poorest agreement with experiment, whereas TIP5P is closest to the experimental data at lower temperatures and SPCE is closest at higher temperatures. As a first approximation, this behavior can be rationalized as a temperature shift with respect to the solvation behavior found in real water. A rescaling procedure inspired by the information theory model of Hummer et al. [Chem. Phys. 258, 349 (2000)] suggests that the different solubility curves for the different models and real water can be largely explained on the basis of the different density curves at constant pressure. In addition, the models that give a good representation of the water structure at ambient conditions (TIP5P, SPCE, and TIP4P) show considerably better agreement with the experimental data than the ones which exhibit less structured O–O correlation functions (SPC and TIP3P). In the second part of the paper we calculate the hydrophobic interaction between xenon particles directly from a series of 60 ns simulation runs. We find that the temperature dependence of the association is to a large extent related to the strength of the solvation entropy. Nevertheless, differences between the models seem to require a more detailed molecular picture. The TIP5P model shows by far the strongest temperature dependence. The suggested density rescaling is also applied to the chemical potential in the xenon–xenon contact-pair configuration, indicating the presence of a temperature where the hydrophobic interaction turns into purely repulsive. The predicted association for xenon in real water suggests the presence of a strong variation with temperature, comparable to the behavior found for TIP5P water. Comparing different water models and experimental data we conclude that a proper description of density effects is an important requirement for a water model to account correctly for the correct description of the hydrophobic effects. A water model exhibiting a density maximum at the correct temperature is desirable.</jats:p>',\n",
       "       '<jats:p>An intermolecular potential model of H2O with six interaction sites is proposed. The model is developed for the simulation of ice and water near the melting point. Parameters in the potential are determined to reproduce the real melting point of ice, and densities of ice and water near the melting point, which are predicted by calculating derivatives of the free energies and volumes of ice and water against potential parameters. Free energy calculations are carried out for several ice structures and water, and the results are compared with those obtained in four- and five-site models, which are currently in use. It is shown that, only in the present six-site model, the proton-disordered hexagonal ice is the stable structure at the melting point, as in real ice. The melting point of the proton-disordered hexagonal ice at 1 atm is estimated to be 271±9\\u2009K in the present model, which is in good agreement with the real melting point of 273.15 K. Moreover, results of Monte Carlo simulations of ice and water show that the present six-site model reproduces well the real structural and thermodynamic properties of ice and water near the melting point.</jats:p>',\n",
       "       '<jats:p>Different approaches to improve the simple point charge model for liquid water (SPC) were investigated. This led to a whole series of new water models with additional van der Waals interaction sites at the hydrogen atoms, modified partial charges and modified geometries. The properties of these models are analyzed and discussed. Particular emphasis has been put on the study of the dependence and sensitivity of water properties on the model parameters. We found that a simultaneous improvement of the dielectric permittivity and the diffusion coefficient is difficult to attain for a rigid, nonpolarizable three interaction site model. Nevertheless, two of the models presented here, SPC/A and SPC/L, show good agreement with experimental data on water and have been characterized in more detail. We conclude that SPC/L represents the overall properties of water better than SPC. Especially, it shows excellent dielectric properties, an improved shear viscosity and a slightly lower diffusion coefficient.</jats:p>',\n",
       "       '<jats:p>The charge-on-spring method is used to develop a rigid, three-site, polarizable water model, a noniterative and a self-consistent version. In this method, the polarizability is taken into account by a variable separation of charges on selected polarizable centers. One of the pair of polarization charges resides on a polarizable center, while the other one is treated as an additional particle attached to the polarizable center by a parabolic restraint potential. The separation is calculated in response to the instantaneous electric field. We parametrized two models which are based on noniterative and self-consistent versions of the method, respectively. We computed several liquid-phase and gas-phase properties and compared with data available from experiment and ab initio calculations. The condensed-phase properties of both models are in reasonable accord with experiment, apart from discrepancies in electrostatic properties consistent with a slightly too large liquid-state dipole.</jats:p>',\n",
       "       '<jats:p>Water exhibits many unusual properties that are essential for the existence of life. Water completely changes its character from ambient to supercritical conditions in a way that makes it possible to sustain life at extreme conditions, leading to conjectures that life may have originated in deep-sea vents. Molecular simulation can be very useful in exploring biological and chemical systems, particularly at extreme conditions for which experiments are either difficult or impossible; however this scenario entails an accurate molecular model for water applicable over a wide range of state conditions. Here, we present a Gaussian charge polarizable model (GCPM) based on the model developed earlier by Chialvo and Cummings [Fluid Phase Equilib.\\u2008150, 73 (1998)] which is, to our knowledge, the first that satisfies the water monomer and dimer properties, and simultaneously yields very accurate predictions of dielectric, structural, vapor-liquid equilibria, and transport properties, over the entire fluid range. This model would be appropriate for simulating biological and chemical systems at both ambient and extreme conditions. The particularity of the GCPM model is the use of Gaussian distributions instead of points to represent the partial charges on the water molecules. These charge distributions combined with a dipole polarizability and a Buckingham exp-6 potential are found to play a crucial role for the successful and simultaneous predictions of a variety of water properties. This work not only aims at presenting an accurate model for water, but also at proposing strategies to develop classical accurate models for the predictions of structural, dynamic, and thermodynamic properties.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>The properties of two improved versions of charge-on-spring (COS) polarizable water models (COS/G2 and COS/G3) that explicitly include nonadditive polarization effects are reported. In COS models, the polarization is represented via a self-consistently induced dipole moment consisting of a pair of separated charges. A previous polarizable water model (COS/B2), upon which the improved versions are based, was developed by Yu, Hansson, and van Gunsteren [J. Chem. Phys. 118, 221 (2003)]. To improve the COS/B2 model, which overestimated the dielectric permittivity, one additional virtual atomic site was used to reproduce the water monomer quadrupole moments besides the water monomer dipole moment in the gas phase. The molecular polarizability, residing on the virtual atomic site, and Lennard-Jones parameters for oxygen-oxygen interactions were varied to reproduce the experimental values for the heat of vaporization and the density of liquid water at room temperature and pressure. The improved models were used to study the properties of liquid water at various thermodynamic states as well as gaseous water clusters and ice. Overall, good agreement is obtained between simulated properties and those derived from experiments and ab initio calculations. The COS/G2 and COS/G3 models may serve as simple, classical, rigid, polarizable water models for the study of organic solutes and biopolymers. Due to its simplicity, COS type of polarization can straightforwardly be used to introduce explicit polarization into (bio)molecular force fields.</jats:p>',\n",
       "       '<jats:p>As a test of the accuracy of the several proposed effective pair potentials for water, we have examined how well they reproduce the structures and properties of the proton-ordered ices II, VIII, and IX, and those of a hypothetical proton ordered form of ice I which is closely related to ice Ih. Despite leading to a consistent underestimate of all the ice densities by roughly 21%, the ab\\u2008initio pair potential calculated by Matsuoka, Clementi, and Yoshimine succeeds better than any other in predicting the hydrogen bonding topology and the O–O–O angles characteristic of the various ice lattices. Some of the proposed water–water potentials predict the existence of ice polymorphs entirely different from known forms of ice, and/or give rise to structures with bifurcated hydrogen bonds. Using our new results, and others in the literature, we give a detailed discussion of the features of the MCY, ST2, and RSL2 potentials, each considered a representative of a class of potentials.</jats:p>',\n",
       "       '', '',\n",
       "       '<jats:p>Mobile robots lack a driver or a pilot and, thus, should be able to detect obstacles autonomously. This paper reviews various image-based obstacle detection techniques employed by unmanned vehicles such as Unmanned Surface Vehicles (USVs), Unmanned Aerial Vehicles (UAVs), and Micro Aerial Vehicles (MAVs). More than 110 papers from 23 high-impact computer science journals, which were published over the past 20 years, were reviewed. The techniques were divided into monocular and stereo. The former uses a single camera, while the latter makes use of images taken by two synchronised cameras. Monocular obstacle detection methods are discussed in appearance-based, motion-based, depth-based, and expansion-based categories. Monocular obstacle detection approaches have simple, fast, and straightforward computations. Thus, they are more suited for robots like MAVs and compact UAVs, which usually are small and have limited processing power. On the other hand, stereo-based methods use pair(s) of synchronised cameras to generate a real-time 3D map from the surrounding objects to locate the obstacles. Stereo-based approaches have been classified into Inverse Perspective Mapping (IPM)-based and disparity histogram-based methods. Whether aerial or terrestrial, disparity histogram-based methods suffer from common problems: computational complexity, sensitivity to illumination changes, and the need for accurate camera calibration, especially when implemented on small robots. In addition, until recently, both monocular and stereo methods relied on conventional image processing techniques and, thus, did not meet the requirements of real-time applications. Therefore, deep learning networks have been the centre of focus in recent years to develop fast and reliable obstacle detection solutions. However, we observed that despite significant progress, deep learning techniques also face difficulties in complex and unknown environments where objects of varying types and shapes are present. The review suggests that detecting narrow and small, moving obstacles and fast obstacle detection are the most challenging problem to focus on in future studies.</jats:p>',\n",
       "       '', '', '',\n",
       "       '<jats:p>The temperature dependence of the thermodynamic and dynamical properties of liquid water using the polarizable fluctuating charge (FQ) model is presented. The properties of ice Ih, both for a perfect lattice with no thermal disorder and at a temperature of 273 K, are also presented. In contrast to nonpolarizable models, the FQ model has a density maximum of water near 277 K. For ice, the model has a dipole moment of the perfect lattice of 3.05 Debye, in good agreement with a recent induction model calculation. The simulations at 273 K and the correct density find that thermal motion decreases the average dipole moment to 2.96 D. The liquid state dipole moment is less than the ice value and decreases with temperature.</jats:p>',\n",
       "       '<jats:p>We carry out extensive molecular dynamics simulations in order to evaluate the thermodynamic equation of state of the extended simple point charge model of water (customarily described by the acronym SPC/E) over a wide range of temperature and density, with emphasis on the supercooled region. We thereby determine the location of the temperature of maximum density (TMD) line and the liquid spinodal line. In particular, we find that the experimental TMD line lies between the TMD lines of the SPC/E and ST2 models of water, so perhaps the behavior of these two models of simulated water “bracket” the behavior of real water. As temperature decreases, we find (i) that maxima appear in isotherms of the isothermal compressibility as a function of density, (ii) that isotherms of the internal energy as a function of volume display negative curvature and (iii) that the pressure of the liquid–vapor spinodal decreases. We compare the results to corresponding behavior found from simulations of the ST2 model of water and find that the behavior of SPC/E, when shifted to higher values of temperature and pressure (ΔP≈50\\u2009MPa and ΔT≈80\\u2009K), approximates that of ST2. We discuss the implications of our results for the hypothesis that a critical point occurs in the phase diagram of supercooled water. Finally, we argue that the results of our simulations are not inconsistent with the possibility that C′ exists for SPC/E water.</jats:p>',\n",
       "       '<jats:p>With the objective of improving the effective pair potentials for water, we develop a potential model that employs diffuse charges, in addition to the usual point charges, on the oxygen and hydrogen atoms, to account for charge penetration effects. The potential has better transferability from the liquid to gaseous phases since, unlike many existing models, it does not require an enhanced dipole moment. As a result it accurately reproduces the structural and thermodynamic properties of water over a wide range of conditions. Moreover, by allowing for electronic polarization when evaluating the total dipole moment of the simulated fluid, the model leads to the correct value of the dielectric constant for virtually any state point. At room temperature the calculation produces an average dipole moment of 3.09 D, in accord with recent theoretical and experimental evaluations. This supports the idea that induction effects in water are more important than previously expected.</jats:p>',\n",
       "       '<jats:p>Simulations of water using the exnteded simple point charge (SPC/E) model at temperatures between 190 and 330 K were performed using molecular dynamics techniques. A maximum in the density at 1 bar pressure was found to occur at 235 K. The energies and diffusivities are also reported. The SPC/E-modeled water exhibits a glass transition ∼177 K. No crystallization events were observed during the course of the long simulations.</jats:p>',\n",
       "       '<jats:p>We present an analysis of the morphology of the water dimer potential energy surface (PES) obtained from ab initio electronic structure calculations and perform a quantitative comparison with the results from various water potentials. In order to characterize the morphology of the PES we have obtained minimum energy paths (MEPs) as a function of the intermolecular O–O separation by performing constrained optimizations under various symmetries (Cs,\\u2008Ci,\\u2008C2, and C2v). These constitute a primitive map of the dimer PES and aid in providing an account for some of its salient features such as the energetic stabilization of “doubly hydrogen-bonded” configurations for R(O–O)&amp;lt;2.66 Å. Among the various interaction potentials that are examined, it is found that the family of anisotropic site potential (ASP) models agrees better with the ab initio results in reproducing the geometries along the symmetry-constrained MEPs. It is demonstrated that the models that produce closest agreement with the morphology of the ab initio PES, tend to better reproduce the experimental data for the second virial coefficients. We finally comment on the functional forms of simple water models and discuss how effects such as charge overlap can be incorporated into such models.</jats:p>',\n",
       "       '<jats:p>Water exhibits a maximum in density at normal pressure at 4° above its melting point. The reproduction of this maximum is a stringent test for potential models used commonly in simulations of water. The relation between the melting temperature and the temperature of maximum density for these potential models is unknown mainly due to our ignorance about the melting temperature of these models. Recently we have determined the melting temperature of ice Ih for several commonly used models of water (SPC, SPC/E, TIP3P, TIP4P, TIP4P/Ew, and TIP5P). In this work we locate the temperature of maximum density for these models. In this way the relative location of the temperature of maximum density with respect to the melting temperature is established. For SPC, SPC/E, TIP3P, TIP4P, and TIP4P/Ew the maximum in density occurs at about 21–37K above the melting temperature. In all these models the negative charge is located either on the oxygen itself or on a point along the H–O–H bisector. For the TIP5P and TIP5P-E models the maximum in density occurs at about 11K above the melting temperature. The location of the negative charge appears as a geometrical crucial factor to the relative position of the temperature of maximum density with respect to the melting temperature.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>Scene change detection (SCD) is a task to identify changes of interest between bi-temporal images acquired at different times. A critical idea of SCD is how to identify interesting changes while overcoming noisy changes induced by camera motion or environment variation, such as viewpoint, dynamic changes and outdoor conditions. The noisy changes cause corresponding pixel pairs to have spatial difference (position relation) and temporal difference (intensity relation). Due to the limitation of local receptive field, it is difficult for traditional models based on convolutional neural network (CNN) to establish long-range relations for the semantic changes. In order to address the above challenges, we explore the potential of a transformer in SCD and propose a transformer-based SCD architecture (TransCD). From the intuition that a SCD model should be able to model both interesting and noisy changes, we incorporate a siamese vision transformer (SViT) in a feature difference SCD framework. Our motivation is that SViT is able to establish global semantic relations and model long-range context, which is more robust to noisy changes. In addition, different from the pure CNN-based models with high computational complexity, the proposed model is more efficient and has fewer parameters. Extensive experiments on the CDNet-2014 dataset demonstrate that the proposed TransCD (SViT-E1-D1-32) outperforms the state-of-the-art SCD models and achieves 0.9361 in terms of the F1 score with an improvement of 7.31%.</jats:p>',\n",
       "       '', '', '', '', '', '',\n",
       "       '<jats:p>We have calculated molecular multipole moments for water molecules in clusters and in ice Ih by partitioning the charge density obtained from first principles calculations. Various schemes for dividing the electronic charge density among the water molecules were used. They include Bader’s zero flux surfaces and Voronoi partitioning schemes. A comparison was also made with an induction model including dipole, dipole-quadrupole, quadrupole-quadrupole polarizability and first hyperpolarizability as well as fixed octopole and hexadecapole moments. We have found that the different density partitioning schemes lead to widely different values for the molecular multipoles, illustrating how poorly defined molecular multipoles are in clusters and condensed environments. For instance, the magnitude of the molecular dipole moment in ice Ih ranges between 2.3 D and 3.1 D depending on the partitioning scheme used. Within each scheme, though, the value for the molecular dipole moment in ice is larger than in the hexamer. The magnitude of the molecular dipole moment in the clusters shows a monotonic increase from the gas phase value to the one in ice Ih, with the molecular dipole moment in the water ring hexamer being smaller than the one in ice Ih for all the partitioning schemes used.</jats:p>',\n",
       "       '<jats:p>Parameterization of the five-site model (TIP5P) for water [M. W. Mahoney and W. L. Jorgensen, J. Chem. Phys. 112, 8910 (2000)] has been examined by several computer simulation methods accounting properly for long-range forces. The structural and thermodynamic properties at a pressure of 1 atm over the temperature range (−25\\u200a°C,+75\\u200a°C) and the vapor–liquid coexistence have been determined. It is shown that the simple spherical cutoff method used in the original simulations to find optimized parameters of this five-site model yields results that differ from those obtained by both the Ewald summation and reaction field methods. Consequently, the pivot property to which the parameters were adjusted, the location of the density maximum at 1 atm, does not agree with experimental values. The equilibrium properties then show only a fair agreement with experimental data and are uniformly inferior to those of the four-site TIP4P water over the entire coexistence range.</jats:p>',\n",
       "       '', '',\n",
       "       \"<jats:p>Restricted by the ability of depth perception, all Multi-view 3D object detection methods fall into the bottleneck of depth accuracy. By constructing temporal stereo, depth estimation is quite reliable in indoor scenarios. However, there are two difficulties in directly integrating temporal stereo into outdoor multi-view 3D object detectors: 1) The construction of temporal stereos for all views results in high computing costs. 2) Unable to adapt to challenging outdoor scenarios. In this study, we propose an effective method for creating temporal stereo by dynamically determining the center and range of the temporal stereo. The most confident center is found using the EM algorithm. Numerous experiments on nuScenes have shown the BEVStereo's ability to deal with complex outdoor scenarios that other stereo-based methods are unable to handle.  For the first time, a stereo-based approach shows superiority in scenarios like a static ego vehicle and moving objects. BEVStereo achieves the new state-of-the-art in the camera-only track of nuScenes dataset while maintaining memory efficiency.  Codes have been released.</jats:p>\",\n",
       "       '<jats:p>We present here the phase diagram for one of the most popular water models, the four-point transferable intermolecular potential (TIP4P) model. We show that TIP4P model, does indeed provide a qualitatively correct description of the phase diagram of water. The melting line of the five-point transferable intermolecular potential (TIP5P) at low pressures is also presented.</jats:p>',\n",
       "       '<jats:p>A method of free energy calculation is proposed, which enables to cover a wide range of pressure and temperature. The free energies of proton-disordered hexagonal ice (ice Ih) and liquid water are calculated for the TIP4P [J. Chem. Phys. 79, 926 (1983)] model and the TIP5P [J. Chem. Phys. 112, 8910 (2000)] model. From the calculated free energy curves, we determine the melting point of the proton-disordered hexagonal ice at 0.1 MPa (atmospheric pressure), 50 MPa, 100 MPa, and 200 MPa. The melting temperatures at atmospheric pressure for the TIP4P ice and the TIP5P ice are found to be about Tm=229\\u2009K and Tm=268\\u2009K, respectively. The melting temperatures decrease as the pressure is increased, a feature consistent with the pressure dependence of the melting point for realistic proton-disordered hexagonal ice. We also calculate the thermal expansivity of the model ices. Negative thermal expansivity is observed at the low temperature region for the TIP4P ice, but not for the TIP5P ice at the ambient pressure.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>The method of flexible constraints was implemented in a Monte Carlo code to perform numerical simulations of liquid water and ice Ih in the constant number of molecules, volume, and temperature and constant pressure, instead of volume ensembles, using the polarizable and flexible mobile charge densities in harmonic oscillators (MCDHO) model. The structural and energetic results for the liquid at T=298\\u2009K and ρ=997\\u2009kg\\u200am−3 were in good agreement with those obtained from molecular dynamics. The density obtained at P=1\\u2009atm with flexible constraints, ρ=1008\\u2009kg\\u200am−3, was slightly lower than with the classical sampling of the intramolecular vibrations, ρ=1010\\u2009kg\\u200am−3. The comparison of the structures and energies found for water hexamers and for ice Ih with six standard empirical models to those obtained with MCDHO, show this latter to perform better in describing water far from ambient conditions: the MCDHO minimum lattice energy, density, and lattice constants were in good agreement with experiment. The average ∠HOH of the water molecule in ice was predicted to be slightly larger than in the liquid, yet 1.2% smaller than the experimental value.</jats:p>',\n",
       "       '<jats:title>Abstract</jats:title><jats:p>A new method for performing molecular dynamics simulations with fluctuating charge polarizable potentials is introduced. In fluctuating charge models, polarizability is treated by allowing the partial charges to be variables, with values that are coupled to charges on the same molecule as well as those on other molecules. The charges can be efficiently propagated in a molecular dynamics simulation using extended Lagrangian dynamics. By making a coordinate change from the charge variables to a set of normal mode charge coordinates for each molecule, a new method is constructed in which the normal mode charge variables uncouple from those on the same molecule. The method is applied to the TIP4P‐FQ model of water and compared to other methods for implementing the dynamics. The methods are compared using different molecular dynamics time steps. © 2005 Wiley Periodicals, Inc. J Comput Chem 26: 699–707, 2005</jats:p>',\n",
       "       '<jats:p>Monte Carlo computer simulations of ice VII and ice VIII phases have been undertaken using the four-point transferable intermolecular potential model of water. By following thermodynamic paths similar to those used experimentally, ice is decompressed resulting in an amorphous phase. These phases are compared to the high density amorphous phase formed upon compression of ice Ih and are found to have very similar structures. By cooling liquid water along the water/Ih melting line a high density amorphous phase was also generated.</jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '<jats:p>Traditional learning-based multi-view stereo (MVS) methods usually need to find the correct depth value from a large number of depth candidates, which leads to huge memory consumption and slow inference. To address these problems, we propose a probabilistic depth sampling in the learning-based PatchMatch framework, i.e., sampling a small number of depth candidates from a single-view probability distribution, which achieves the purpose of saving computational resources. Furthermore, to overcome the difficulty of obtaining ground-truth depth for outdoor large-scale scenes, we also propose a self-supervised training pipeline based on knowledge distillation, which involves self-supervised teacher training and student training based on knowledge distillation. Extensive experiments show that our approach outperforms other recent learning-based MVS methods on DTU, Tanks and Temples, and ETH3D datasets.</jats:p>',\n",
       "       '', '', '', '', '',\n",
       "       '<jats:p>We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance on the KITTI and Sintel datasets. In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count.</jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       \"<jats:p>In this research, we propose a new 3D object detector with a trustworthy depth estimation,  dubbed BEVDepth, for camera-based Bird's-Eye-View~(BEV) 3D object detection. Our work is based on a key observation -- depth estimation in recent approaches is surprisingly inadequate given the fact that depth is essential to camera 3D detection. Our BEVDepth resolves this by leveraging explicit depth supervision. A camera-awareness depth estimation module is also introduced to facilitate the depth predicting capability. Besides, we design a novel Depth Refinement Module to counter the side effects carried by imprecise feature unprojection. Aided by customized Efficient Voxel Pooling and multi-frame mechanism, BEVDepth achieves the new state-of-the-art 60.9% NDS on the challenging nuScenes test set while maintaining high efficiency. For the first time, the NDS score of a camera model reaches 60%. Codes have been released.</jats:p>\",\n",
       "       '', '', '', '',\n",
       "       '<jats:p>In this work, we propose a novel defense system against adversarial examples leveraging the unique power of Generative Adversarial Networks (GANs) to generate new adversarial examples for model retraining. To do so, we develop an automated pipeline using combination of pre-trained convolutional neural network and an external GAN, that is, Pix2Pix conditional GAN, to determine the transformations between adversarial examples and clean data, and to automatically synthesize new adversarial examples. These adversarial examples are employed to strengthen the model, attack, and defense in an iterative pipeline. Our simulation results demonstrate the success of the proposed method.</jats:p>',\n",
       "       '',\n",
       "       '<jats:p>A lot of theoretical and empirical evidence shows that the flatter local minima tend to improve generalization. Adversarial Weight Perturbation (AWP) is an emerging technique to efficiently and effectively find such minima. In AMP we minimize the loss w.r.t. a bounded worst-case perturbation of the model parameters thereby favoring local minima with a small loss in a neighborhood around them.\\nThe benefits of AWP, and more generally the connections between flatness and generalization, have been extensively studied for i.i.d. data such as images. In this paper, we extensively study this phenomenon for graph data. Along the way, we first derive a generalization bound for non-i.i.d. node classification tasks. Then we identify a vanishing-gradient issue with all existing formulations of AWP and we propose a new Weighted Truncated AWP (WT-AWP) to alleviate this issue. We show that regularizing graph neural networks with WT-AWP consistently improves both natural and robust generalization across many different graph learning tasks and models.</jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '<jats:p>Recently 3D object detection from surround-view images has made notable advancements with its low deployment cost. However, most works have primarily focused on close perception range while leaving long-range detection less explored. Expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. To address these limitations, this paper proposes a novel sparse query-based framework, dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D adaptive queries that complement the 3D global queries. To efficiently capture discriminative features across different views and scales for long-range objects, we introduce a perspective-aware aggregation module. Additionally, we propose a range-modulated 3D denoising approach to address query error propagation and mitigate convergence issues in long-range tasks. Significantly, Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset, covering a wide range of 150 meters, surpassing several LiDAR-based approaches. The code is available at https://github.com/megvii-research/Far3D.</jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '<jats:p>Abstract. The goal of the CO3D (Constellation Optique 3D) mission is the full-automatic production of a worldwide accurate DEM. CO3D is also a constellation of a new generation of low-cost optical satellites. The DEM accuracy is expected to be one meter in relative height and two meters in absolute height with a one-meter grid space. Each of the four satellites of the constellation will provide images with 0.50 m resolution in red, green, blue bands. A NIR (Near-InfraRed) band will also be available with a resolution close to 1 m. The satellites resource will be shared by, on one hand, the French institutions (government, scientists concerned by global Earth monitoring) who will have dedicated access and preferred price conditions, and on the other hand commercial customers interested in 2D and 3D products. The launch of the constellation is expected mid-2023 and 90 % of the DEM worldwide production should be reached by the end of 2025.Starting from an overview of the system characteristics and its main innovations, this paper presents the expected performance, the 2D and 3D products that should be available for the end-users and finally how they should be qualified.\\n                    </jats:p>',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = pd.Series([raw.get('abstract', '') for raw in all_raw])\n",
    "abstracts.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
