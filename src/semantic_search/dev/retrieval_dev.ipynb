{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))  # /src\n",
    "\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/python_envs/dsl-research-assistant/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import misc\n",
    "reload(misc)\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 73809 vectors\n"
     ]
    }
   ],
   "source": [
    "embedding_model = LocalEmbeddingModel(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='cuda',\n",
    "    batch_size=8\n",
    ")\n",
    "document_store = FAISSDocumentStore(\n",
    "    embedding_model=embedding_model,\n",
    "    db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/chunk1-txt-all',\n",
    ")\n",
    "\n",
    "if not document_store.load_index():\n",
    "    document_store.create_index('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er-efficient visual instruction model. arXiv 2304.15010 , 2023. 1, 3, 4\n",
      "- [9] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role...\n"
     ]
    }
   ],
   "source": [
    "print(document_store.search(\"visual\", top_k=5)[0]['chunk_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata for CVPR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show wider dataframes\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text in columns\n",
    "pd.set_option('display.width', 1000)         # Set the display width\n",
    "pd.set_option('display.max_columns', 20)     # Show more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06f8ad35e094fadbead424dc90ffb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>fname</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 fpath                                                                                                            fname                                                                                                                      title                                            doi                              oaid\n",
       "0                                                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                             Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                                                               Fixed Point Diffusion Models              https://doi.org/10.1063/1.2121687  https://openalex.org/W2000456051\n",
       "1                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                             Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                                                             BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection   https://doi.org/10.1109/cvpr52733.2024.01901  https://openalex.org/W4402727763\n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild  https://doi.org/10.1109/cvprw63382.2024.00473  https://openalex.org/W4402916217"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pyalex\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "df = pd.DataFrame([(str(fpath), fpath.name) for fpath in Path(raw_dir).glob(\"*.txt\")], columns=['fpath', 'fname'])\n",
    "\n",
    "def get_title(fpath: str):\n",
    "    doc_text = Path(fpath).read_text(encoding=\"utf-8\")\n",
    "    title_match = re.search(r'## ([^\\n#]+)', doc_text)\n",
    "    return title_match.group(1) if title_match else None\n",
    "\n",
    "def get_metadata(title: str):\n",
    "    search_results = pyalex.Works().search(title).select(['id', 'doi', 'referenced_works']).get(page=1, per_page=1)\n",
    "    return (search_results[0]['doi'], search_results[0]['id'], search_results[0]['referenced_works']) if search_results else (None, None, None)\n",
    "\n",
    "df['title'] = df['fpath'].apply(get_title)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(get_metadata, df['title'].values),\n",
    "        total=len(df)\n",
    "    ))\n",
    "df['doi', 'oaid', 'referenced_works'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(results, columns=['doi', 'oaid', 'referenced_works'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store/Load paper metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/paper-metadata.csv', index=False)\n",
    "df = pd.read_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/paper-metadata.csv')\n",
    "\n",
    "def parse_referenced_works(x):\n",
    "    if pd.isna(x) or x is None:\n",
    "        return []\n",
    "    elif isinstance(x, str):\n",
    "        # Remove brackets and split by commas, then strip quotes and whitespace\n",
    "        if x.startswith('[') and x.endswith(']'):\n",
    "            # Example: \"['https://openalex.org/W10789807', 'https://openalex.org/W109508954']\"\n",
    "            items = x[1:-1].split(',')\n",
    "            return [item.strip().strip(\"'\\\"\") for item in items if item.strip()]\n",
    "        return [x]  # If it's a string but not a list format, treat as single item\n",
    "    return []\n",
    "\n",
    "df['referenced_works'] = df['referenced_works'].apply(parse_referenced_works)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total references and references within dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references stats: min=0, max=621, mean=27.32\n",
      "References in dataset stats: min=0, max=3, mean=0.21\n"
     ]
    }
   ],
   "source": [
    "# Count total references and references within the dataset\n",
    "def count_references(row):\n",
    "    total_refs = len(row['referenced_works'])\n",
    "    dataset_oaids = set(df['oaid'].dropna())\n",
    "    refs_in_dataset = sum(1 for ref in row['referenced_works'] if ref in dataset_oaids)\n",
    "    return total_refs, refs_in_dataset\n",
    "\n",
    "ref_counts = df.apply(count_references, axis=1, result_type='expand')\n",
    "df['total_references'] = ref_counts[0]\n",
    "df['references_in_dataset'] = ref_counts[1]\n",
    "\n",
    "\n",
    "print(f\"Total references stats: min={df['total_references'].min()}, max={df['total_references'].max()}, mean={df['total_references'].mean():.2f}\")\n",
    "print(f\"References in dataset stats: min={df['references_in_dataset'].min()}, max={df['references_in_dataset'].max()}, mean={df['references_in_dataset'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "      <th>referenced_works</th>\n",
       "      <th>total_references</th>\n",
       "      <th>references_in_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "      <td>[https://openalex.org/W1504980292, https://ope...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D ...</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "      <td>[https://openalex.org/W1861492603, https://ope...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models ...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Insights from the Use of Previously Unseen Neu...</td>\n",
       "      <td>https://doi.org/10.48550/arxiv.2404.02189</td>\n",
       "      <td>https://openalex.org/W4393967825</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Efficient local correlation volume for unsuper...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00049</td>\n",
       "      <td>https://openalex.org/W4402904316</td>\n",
       "      <td>[https://openalex.org/W1513100184, https://ope...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>uTRAND: Unsupervised Anomaly Detection in Traf...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00759</td>\n",
       "      <td>https://openalex.org/W4402904312</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Test-Time Adaptation with SaLIP: A Cascade of ...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00526</td>\n",
       "      <td>https://openalex.org/W4402916510</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Towards Understanding and Improving Adversaria...</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.02336</td>\n",
       "      <td>https://openalex.org/W4402753640</td>\n",
       "      <td>[https://openalex.org/W2517229335, https://ope...</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Adaptive Softassign via Hadamard-Equipped Sink...</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01670</td>\n",
       "      <td>https://openalex.org/W4402753930</td>\n",
       "      <td>[https://openalex.org/W1587878450, https://ope...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Augmented Self-Mask Attention Transformer for ...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00705</td>\n",
       "      <td>https://openalex.org/W4402904140</td>\n",
       "      <td>[https://openalex.org/W2507009361, https://ope...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fpath  \\\n",
       "0  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "1  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "3  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "4  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "5  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "6  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "7  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "8  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "9  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "\n",
       "                                               title  \\\n",
       "0                       Fixed Point Diffusion Models   \n",
       "1  BEVNeXt: Reviving Dense BEV Frameworks for 3D ...   \n",
       "2  Leveraging Pre-trained Multi-task Deep Models ...   \n",
       "3  Insights from the Use of Previously Unseen Neu...   \n",
       "4  Efficient local correlation volume for unsuper...   \n",
       "5  uTRAND: Unsupervised Anomaly Detection in Traf...   \n",
       "6  Test-Time Adaptation with SaLIP: A Cascade of ...   \n",
       "7  Towards Understanding and Improving Adversaria...   \n",
       "8  Adaptive Softassign via Hadamard-Equipped Sink...   \n",
       "9  Augmented Self-Mask Attention Transformer for ...   \n",
       "\n",
       "                                             doi  \\\n",
       "0              https://doi.org/10.1063/1.2121687   \n",
       "1   https://doi.org/10.1109/cvpr52733.2024.01901   \n",
       "2  https://doi.org/10.1109/cvprw63382.2024.00473   \n",
       "3      https://doi.org/10.48550/arxiv.2404.02189   \n",
       "4  https://doi.org/10.1109/cvprw63382.2024.00049   \n",
       "5  https://doi.org/10.1109/cvprw63382.2024.00759   \n",
       "6  https://doi.org/10.1109/cvprw63382.2024.00526   \n",
       "7   https://doi.org/10.1109/cvpr52733.2024.02336   \n",
       "8   https://doi.org/10.1109/cvpr52733.2024.01670   \n",
       "9  https://doi.org/10.1109/cvprw63382.2024.00705   \n",
       "\n",
       "                               oaid  \\\n",
       "0  https://openalex.org/W2000456051   \n",
       "1  https://openalex.org/W4402727763   \n",
       "2  https://openalex.org/W4402916217   \n",
       "3  https://openalex.org/W4393967825   \n",
       "4  https://openalex.org/W4402904316   \n",
       "5  https://openalex.org/W4402904312   \n",
       "6  https://openalex.org/W4402916510   \n",
       "7  https://openalex.org/W4402753640   \n",
       "8  https://openalex.org/W4402753930   \n",
       "9  https://openalex.org/W4402904140   \n",
       "\n",
       "                                    referenced_works  total_references  \\\n",
       "0  [https://openalex.org/W1504980292, https://ope...                55   \n",
       "1  [https://openalex.org/W1861492603, https://ope...                73   \n",
       "2                                                 []                 0   \n",
       "3                                                 []                 0   \n",
       "4  [https://openalex.org/W1513100184, https://ope...                28   \n",
       "5                                                 []                 0   \n",
       "6                                                 []                 0   \n",
       "7  [https://openalex.org/W2517229335, https://ope...                31   \n",
       "8  [https://openalex.org/W1587878450, https://ope...                42   \n",
       "9  [https://openalex.org/W2507009361, https://ope...                25   \n",
       "\n",
       "   references_in_dataset  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "5                      0  \n",
       "6                      0  \n",
       "7                      0  \n",
       "8                      0  \n",
       "9                      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Korycki_Class-Incremental_Mixture_of_Gaussians_for_Deep_Continual_Learning_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hoque_IrrNet_Spatio-Temporal_Segmentation_Guided_Classification_for_Irrigation_Mapping_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Zhang_MOHO_Learning_Single-view_Hand-held_Object_Reconstruction_with_Multi-view_Occlusion-Aware_Supervision_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Paissan_Structured_Sparse_Back-propagation_for_Lightweight_On-Device_Continual_Learning_on_Microcontroller_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Halawa_Multi-Task_Multi-Modal_Self-Supervised_Learning_for_Facial_Expression_Recognition_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Loaliyan_Comparative_Analysis_of_Generalization_and_Harmonization_Methods_for_3D_Brain_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hu_Low_Latency_Point_Cloud_Rendering_with_Learned_Splatting_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Ko_Semantic_Line_Combination_Detector_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Duan_Towards_Backward-Compatible_Continual_Learning_of_Image_Compression_CVPR_2024_paper.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "def check_single_doc(fpath: str):\n",
    "    doc_text = fpath.read_text(encoding=\"utf-8\")\n",
    "    abstract_match = re.search(r'## Abstract\\n\\n(.*?)(?=\\n\\n## \\d+\\.)', doc_text, re.DOTALL)\n",
    "    return abstract_match.group(1) if abstract_match else None\n",
    "\n",
    "for fpath in Path(raw_dir).glob(\"*.txt\"):\n",
    "    if check_single_doc(fpath) is None:\n",
    "        print(fpath)\n",
    "\n",
    "# abstracts =[check_single_doc(fpath) for fpath in Path(raw_dir).glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>         1133\n",
       "<class 'NoneType'>       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(str(type(a)) for a in abstracts).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for: 'visual odometry in robotics applications'\n",
      "Rank 1 (Score: 0.7618)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "Visual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 (Score: 0.7586)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: current neural network for (un-) supervised learning of monocular video visual odometry and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5555-556...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 (Score: 0.7444)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: This CVPR Workshop paper is the Open Access version, provided by the Computer Vision Foundation.\n",
      "\n",
      "Except for this watermark, it is identical to the accepted version;\n",
      "\n",
      "the final published version of th...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example search function\n",
    "def search_documents(query, top_k=5):\n",
    "    results = document_store.search(query, top_k=top_k)\n",
    "    print(f\"Search results for: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"Rank {result['rank']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['document_name']}\")\n",
    "        print(f\"Preview: {result['chunk_text']}\")\n",
    "        print(\"-\" * 80)\n",
    "    return results\n",
    "\n",
    "# Test the search\n",
    "search_results = search_documents(\"visual odometry in robotics applications\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'score': np.float32(0.506165),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\\n\\n## 1. Introduction\\n\\nVisual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...'},\n",
       " {'rank': 2,\n",
       "  'score': np.float32(0.47257516),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'ons on robotics , 33(5):1255-1262, 2017. 2, 6\\n- [14] David Nist´ er, Oleg Naroditsky, and James Bergen. Visual\\n\\n- odometry. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Visi...'},\n",
       " {'rank': 3,\n",
       "  'score': np.float32(0.4703958),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'current neural network for (un-) supervised learning of monocular video visual odometry and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5555-556...'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.search(\"visual odometry in robotics applications\"*100000, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing SemanticScholar API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Machinery and Intelligence\n"
     ]
    }
   ],
   "source": [
    "# First, import the client from semanticscholar module\n",
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "# You'll need an instance of the client to request data from the API\n",
    "sch = SemanticScholar()\n",
    "\n",
    "# Get a paper by its ID\n",
    "paper = sch.get_paper('10.1093/mind/lix.236.433')\n",
    "\n",
    "# Print the paper title\n",
    "print(paper.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45271453590192645"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.total_references > 0).sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "df = pd.DataFrame([(str(fpath), fpath.name) for fpath in Path(raw_dir).glob(\"*.txt\")], columns=['fpath', 'fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_title(fpath: str):\n",
    "    doc_text = Path(fpath).read_text(encoding=\"utf-8\")\n",
    "    title_match = re.search(r'## ([^\\n#]+)', doc_text)\n",
    "    if title_match is None:  # Fallback: Extract from file name\n",
    "        return ' '.join(Path(fpath).stem.split('_')[1:-3])\n",
    "    return title_match.group(1)\n",
    "\n",
    "df['title'] = df['fpath'].apply(get_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d56871e2c74525804658407584f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "from semanticscholar.SemanticScholarException import ObjectNotFoundException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "sch = SemanticScholar()\n",
    "\n",
    "def get_paper_metadata(paper_title: str):\n",
    "    try:\n",
    "        paper = sch.search_paper(query=paper_title, match_title=True, fields=['paperId', 'externalIds', 'abstract'])\n",
    "        res = {'paperId': paper['paperId'], 'abstract': paper['abstract']}\n",
    "        res.update({f'externalIds.{k}': v for k, v in paper['externalIds'].items()})\n",
    "        return res\n",
    "    except ObjectNotFoundException:\n",
    "        return {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(get_paper_metadata, df['title'].values[:3]),\n",
    "        total=len(df)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, pd.DataFrame(results)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>fname</th>\n",
       "      <th>title</th>\n",
       "      <th>paperId</th>\n",
       "      <th>abstract</th>\n",
       "      <th>externalIds.ArXiv</th>\n",
       "      <th>externalIds.DOI</th>\n",
       "      <th>externalIds.CorpusId</th>\n",
       "      <th>externalIds.DBLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>fdb679246a2125dad1628081e45efb7a1c80f2c7</td>\n",
       "      <td>We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an im-plicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We con-duct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon-strating substantial improvements in performance and effi-ciency. Compared to the state-of-the-art DiT model [38], FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation quality in situations where sampling computation or time is limited. Our code and pretrained models are available at https://lukemelas.github.io/fixed-point-diffusion-models/.</td>\n",
       "      <td>2401.08741</td>\n",
       "      <td>10.1109/CVPR52733.2024.00901</td>\n",
       "      <td>267027739.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</td>\n",
       "      <td>a23f6180d6908499a8238c06f4fb57bf431a2b43</td>\n",
       "      <td>Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their out-standing abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a “modernized” dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set.</td>\n",
       "      <td>2312.01696</td>\n",
       "      <td>10.1109/CVPR52733.2024.01901</td>\n",
       "      <td>265609098.0</td>\n",
       "      <td>journals/corr/abs-2312-01696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild</td>\n",
       "      <td>9c50996ba35eb605cb9bcd5835103b441cf38e07</td>\n",
       "      <td>This article presents our results for the sixth Affective Behavior Analysis in-the-wild (ABAW) competition. To improve the trustworthiness of facial analysis, we study the possibility of using pre-trained deep models that extract reliable emotional features without the need to fine-tune the neural networks for a downstream task. In particular, we introduce several lightweight models based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN architectures trained in multi-task scenarios to recognize facial expressions, valence, and arousal on static photos. These neural networks extract frame-level features fed into a simple classifier, e.g., linear feed-forward neural network, to predict emotion intensity, compound expressions, and valence/arousal. Experimental results for three tasks from the sixth ABAW challenge demonstrate that our approach lets us significantly improve quality metrics on validation sets compared to existing non-ensemble techniques. As a result, our solutions took second place in the compound expression recognition competition.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1109/CVPRW63382.2024.00473</td>\n",
       "      <td>272915313.0</td>\n",
       "      <td>conf/cvpr/Savchenko22a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Geada_Insights_from_the_Use_of_Previously_Unseen_Neural_Architecture_Search_CVPR_2024_paper.txt</td>\n",
       "      <td>Geada_Insights_from_the_Use_of_Previously_Unseen_Neural_Architecture_Search_CVPR_2024_paper.txt</td>\n",
       "      <td>Insights from the Use of Previously Unseen Neural Architecture Search Datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Khairi_Efficient_Local_Correlation_Volume_for_Unsupervised_Optical_Flow_Estimation_on_CVPRW_2024_paper.txt</td>\n",
       "      <td>Khairi_Efficient_Local_Correlation_Volume_for_Unsupervised_Optical_Flow_Estimation_on_CVPRW_2024_paper.txt</td>\n",
       "      <td>Efficient local correlation volume for unsupervised optical flow estimation on small moving objects in large satellite images</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 fpath                                                                                                            fname                                                                                                                          title                                   paperId  \\\n",
       "0                                                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                             Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                                                                   Fixed Point Diffusion Models  fdb679246a2125dad1628081e45efb7a1c80f2c7   \n",
       "1                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                             Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                                                                 BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection  a23f6180d6908499a8238c06f4fb57bf431a2b43   \n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt      Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild  9c50996ba35eb605cb9bcd5835103b441cf38e07   \n",
       "3                  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Geada_Insights_from_the_Use_of_Previously_Unseen_Neural_Architecture_Search_CVPR_2024_paper.txt                  Geada_Insights_from_the_Use_of_Previously_Unseen_Neural_Architecture_Search_CVPR_2024_paper.txt                                                 Insights from the Use of Previously Unseen Neural Architecture Search Datasets                                       NaN   \n",
       "4       /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Khairi_Efficient_Local_Correlation_Volume_for_Unsupervised_Optical_Flow_Estimation_on_CVPRW_2024_paper.txt       Khairi_Efficient_Local_Correlation_Volume_for_Unsupervised_Optical_Flow_Estimation_on_CVPRW_2024_paper.txt  Efficient local correlation volume for unsupervised optical flow estimation on small moving objects in large satellite images                                       NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract  \\\n",
       "0  We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an im-plicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We con-duct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon-strating substantial improvements in performance and effi-ciency. Compared to the state-of-the-art DiT model [38], FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation quality in situations where sampling computation or time is limited. Our code and pretrained models are available at https://lukemelas.github.io/fixed-point-diffusion-models/.   \n",
       "1                                                                                                                                                                                       Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their out-standing abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a “modernized” dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set.   \n",
       "2                                                                                                                                                                                      This article presents our results for the sixth Affective Behavior Analysis in-the-wild (ABAW) competition. To improve the trustworthiness of facial analysis, we study the possibility of using pre-trained deep models that extract reliable emotional features without the need to fine-tune the neural networks for a downstream task. In particular, we introduce several lightweight models based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN architectures trained in multi-task scenarios to recognize facial expressions, valence, and arousal on static photos. These neural networks extract frame-level features fed into a simple classifier, e.g., linear feed-forward neural network, to predict emotion intensity, compound expressions, and valence/arousal. Experimental results for three tasks from the sixth ABAW challenge demonstrate that our approach lets us significantly improve quality metrics on validation sets compared to existing non-ensemble techniques. As a result, our solutions took second place in the compound expression recognition competition.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
       "\n",
       "  externalIds.ArXiv                externalIds.DOI  externalIds.CorpusId              externalIds.DBLP  \n",
       "0        2401.08741   10.1109/CVPR52733.2024.00901           267027739.0                           NaN  \n",
       "1        2312.01696   10.1109/CVPR52733.2024.01901           265609098.0  journals/corr/abs-2312-01696  \n",
       "2               NaN  10.1109/CVPRW63382.2024.00473           272915313.0        conf/cvpr/Savchenko22a  \n",
       "3               NaN                            NaN                   NaN                           NaN  \n",
       "4               NaN                            NaN                   NaN                           NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to retrieve:\n",
    "Database papers:\n",
    "- paperId, externalIds, abstract, referenceCount\n",
    "\n",
    "Then get references for each of those papers:\n",
    "- paperId, externalIds, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paperId', 'externalIds', 'title', 'abstract', 'openAccessPdf', 'authors'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': '9b91b3031ea159e4964d18b2ce703168660ecf46',\n",
       " 'externalIds': {'ArXiv': '2401.11605',\n",
       "  'DBLP': 'journals/corr/abs-2401-11605',\n",
       "  'DOI': '10.48550/arXiv.2401.11605',\n",
       "  'CorpusId': 267069338},\n",
       " 'corpusId': 267069338,\n",
       " 'publicationVenue': {'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29',\n",
       "  'name': 'International Conference on Machine Learning',\n",
       "  'type': 'conference',\n",
       "  'alternate_names': ['ICML', 'Int Conf Mach Learn'],\n",
       "  'url': 'https://icml.cc/'},\n",
       " 'url': 'https://www.semanticscholar.org/paper/9b91b3031ea159e4964d18b2ce703168660ecf46',\n",
       " 'title': 'Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers',\n",
       " 'abstract': None,\n",
       " 'venue': 'International Conference on Machine Learning',\n",
       " 'year': 2024,\n",
       " 'referenceCount': 60,\n",
       " 'citationCount': 38,\n",
       " 'influentialCitationCount': 7,\n",
       " 'isOpenAccess': False,\n",
       " 'openAccessPdf': {'url': '',\n",
       "  'status': None,\n",
       "  'license': None,\n",
       "  'disclaimer': \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://arxiv.org/abs/2401.11605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"},\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "  {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "  {'category': 'Engineering', 'source': 's2-fos-model'}],\n",
       " 'publicationTypes': ['JournalArticle', 'Conference'],\n",
       " 'publicationDate': '2024-01-21',\n",
       " 'journal': {'volume': 'abs/2401.11605', 'name': 'ArXiv'},\n",
       " 'citationStyles': {'bibtex': '@Article{Crowson2024ScalableHP,\\n author = {Katherine Crowson and Stefan Andreas Baumann and Alex Birch and Tanishq Mathew Abraham and Daniel Z. Kaplan and Enrico Shippole},\\n booktitle = {International Conference on Machine Learning},\\n journal = {ArXiv},\\n title = {Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers},\\n volume = {abs/2401.11605},\\n year = {2024}\\n}\\n'},\n",
       " 'authors': [{'authorId': '2280143905', 'name': 'Katherine Crowson'},\n",
       "  {'authorId': '2280144867', 'name': 'Stefan Andreas Baumann'},\n",
       "  {'authorId': '2280144333', 'name': 'Alex Birch'},\n",
       "  {'authorId': '2280143099', 'name': 'Tanishq Mathew Abraham'},\n",
       "  {'authorId': '2280145134', 'name': 'Daniel Z. Kaplan'},\n",
       "  {'authorId': '2280144537', 'name': 'Enrico Shippole'}]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.items[0]['citedPaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(title: str):\n",
    "    search_results = pyalex.Works().search(title).select(['id', 'doi', 'referenced_works']).get(page=1, per_page=1)\n",
    "    return (search_results[0]['doi'], search_results[0]['id'], search_results[0]['referenced_works']) if search_results else (None, None, None)\n",
    "\n",
    "df['title'] = df['fpath'].apply(get_title)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(get_metadata, df['title'].values),\n",
    "        total=len(df)\n",
    "    ))\n",
    "df['doi', 'oaid', 'referenced_works'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "      <th>referenced_works</th>\n",
       "      <th>total_references</th>\n",
       "      <th>references_in_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "      <td>[https://openalex.org/W1504980292, https://ope...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D ...</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "      <td>[https://openalex.org/W1861492603, https://ope...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models ...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Insights from the Use of Previously Unseen Neu...</td>\n",
       "      <td>https://doi.org/10.48550/arxiv.2404.02189</td>\n",
       "      <td>https://openalex.org/W4393967825</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-res...</td>\n",
       "      <td>Efficient local correlation volume for unsuper...</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00049</td>\n",
       "      <td>https://openalex.org/W4402904316</td>\n",
       "      <td>[https://openalex.org/W1513100184, https://ope...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fpath  \\\n",
       "0  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "1  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "3  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "4  /cluster/home/lcarretero/workspace/dsl/dsl-res...   \n",
       "\n",
       "                                               title  \\\n",
       "0                       Fixed Point Diffusion Models   \n",
       "1  BEVNeXt: Reviving Dense BEV Frameworks for 3D ...   \n",
       "2  Leveraging Pre-trained Multi-task Deep Models ...   \n",
       "3  Insights from the Use of Previously Unseen Neu...   \n",
       "4  Efficient local correlation volume for unsuper...   \n",
       "\n",
       "                                             doi  \\\n",
       "0              https://doi.org/10.1063/1.2121687   \n",
       "1   https://doi.org/10.1109/cvpr52733.2024.01901   \n",
       "2  https://doi.org/10.1109/cvprw63382.2024.00473   \n",
       "3      https://doi.org/10.48550/arxiv.2404.02189   \n",
       "4  https://doi.org/10.1109/cvprw63382.2024.00049   \n",
       "\n",
       "                               oaid  \\\n",
       "0  https://openalex.org/W2000456051   \n",
       "1  https://openalex.org/W4402727763   \n",
       "2  https://openalex.org/W4402916217   \n",
       "3  https://openalex.org/W4393967825   \n",
       "4  https://openalex.org/W4402904316   \n",
       "\n",
       "                                    referenced_works  total_references  \\\n",
       "0  [https://openalex.org/W1504980292, https://ope...                55   \n",
       "1  [https://openalex.org/W1861492603, https://ope...                73   \n",
       "2                                                 []                 0   \n",
       "3                                                 []                 0   \n",
       "4  [https://openalex.org/W1513100184, https://ope...                28   \n",
       "\n",
       "   references_in_dataset  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
