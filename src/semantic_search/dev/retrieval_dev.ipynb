{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))  # /src\n",
    "\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import misc\n",
    "reload(misc)\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1142 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1142/1142 [00:00<00:00, 3451.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 73809 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  14%|█▎        | 1268/9227 [04:44<29:47,  4.45it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m embedding_model = LocalEmbeddingModel(\n\u001b[32m      2\u001b[39m     device=\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     batch_size=\u001b[32m8\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m document_store = FAISSDocumentStore(embedding_model)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mdocument_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/luis/Desktop/ETH/Courses/SS25-DSL/raw-data/challenge10_batch_1/CVPR_2024/Conversions/opencvf-data/txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/src/semantic_search/misc.py:126\u001b[39m, in \u001b[36mFAISSDocumentStore.create_index\u001b[39m\u001b[34m(self, data_dir)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Get embeddings for all chunks\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(document_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Create FAISS index\u001b[39;00m\n\u001b[32m    129\u001b[39m dimension = embeddings.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/src/semantic_search/misc.py:48\u001b[39m, in \u001b[36mLocalEmbeddingModel.get_embeddings\u001b[39m\u001b[34m(self, texts, progress_bar)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m batch_embeddings = model_output.pooler_output\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Normalize embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    575\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    584\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    594\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    525\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ETH/Courses/SS25-DSL/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    436\u001b[39m is_causal = (\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    438\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    450\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "embedding_model = LocalEmbeddingModel(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='mps',\n",
    "    batch_size=8\n",
    ")\n",
    "document_store = FAISSDocumentStore(embedding_model)\n",
    "\n",
    "document_store.create_index('/Users/luis/Desktop/ETH/Courses/SS25-DSL/raw-data/challenge10_batch_1/CVPR_2024/Conversions/opencvf-data/txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 41 vectors\n",
      "Search results for: 'visual odometry in robotics applications'\n",
      "Rank 1 (Score: 0.6195)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "Visual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 (Score: 0.5684)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: This CVPR Workshop paper is the Open Access version, provided by the Computer Vision Foundation.\n",
      "\n",
      "Except for this watermark, it is identical to the accepted version;\n",
      "\n",
      "the final published version of th...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 (Score: 0.5650)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: f the IEEE/CVF international conference on computer vision , pages 3828-3838, 2019. 2, 4, 6\n",
      "- [10] Mohinder S Grewal, Lawrence R Weill, and Angus P Andrews. Global positioning systems, inertial naviga...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model and document store\n",
    "embedding_model = LocalEmbeddingModel()\n",
    "document_store = FAISSDocumentStore(embedding_model)\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"example_data\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if not os.path.exists(f\"{document_store.index_path}.faiss\"):\n",
    "    document_store.create_index(data_dir)\n",
    "else:\n",
    "    document_store.load_index()\n",
    "\n",
    "# Example search function\n",
    "def search_documents(query, top_k=5):\n",
    "    results = document_store.search(query, top_k=top_k)\n",
    "    print(f\"Search results for: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"Rank {result['rank']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['document_name']}\")\n",
    "        print(f\"Preview: {result['chunk_text']}\")\n",
    "        print(\"-\" * 80)\n",
    "    return results\n",
    "\n",
    "# Test the search\n",
    "search_results = search_documents(\"visual odometry in robotics applications\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'score': np.float32(0.506165),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\\n\\n## 1. Introduction\\n\\nVisual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...'},\n",
       " {'rank': 2,\n",
       "  'score': np.float32(0.47257516),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'ons on robotics , 33(5):1255-1262, 2017. 2, 6\\n- [14] David Nist´ er, Oleg Naroditsky, and James Bergen. Visual\\n\\n- odometry. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Visi...'},\n",
       " {'rank': 3,\n",
       "  'score': np.float32(0.4703958),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'current neural network for (un-) supervised learning of monocular video visual odometry and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5555-556...'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.search(\"visual odometry in robotics applications\"*100000, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_out['input_ids'].shape=torch.Size([2, 512])\n",
      "model_output.pooler_output.shape=torch.Size([2, 384]); model_output.last_hidden_state.shape=torch.Size([2, 512, 384])\n",
      "batch_embeddings.shape=torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to('mps')\n",
    "\n",
    "batch_texts = [\"visual\", \"Hello my name is Albert\"*1000]\n",
    "tokenizer_out = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Move tokenizer output to the same device as the model\n",
    "    encoded_input = {k: v.to(model.device) for k, v in tokenizer_out.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "# batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "\n",
    "\n",
    "print(f'{tokenizer_out['input_ids'].shape=}')\n",
    "print(f'{model_output.pooler_output.shape=}; {model_output.last_hidden_state.shape=}')\n",
    "print(f'{batch_embeddings.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.9925, 2.1388], device='mps:0'),\n",
       " tensor([1.2345, 1.3589], device='mps:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(batch_embeddings, p=2, dim=1), torch.norm(model_output.pooler_output, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9884e-01,  4.7030e-01,  7.4832e-02,  3.7633e-02, -1.7654e-01,\n",
       "          2.6234e-03, -4.2638e-01, -2.6524e-01, -2.6501e-01,  9.0265e-02,\n",
       "         -3.4190e-01,  7.7723e-01,  1.7366e-01,  4.8052e-02,  2.9466e-01,\n",
       "         -2.6326e-01, -1.1632e-01, -5.8603e-02,  2.2780e-01,  7.3592e-02,\n",
       "          3.2930e-01,  1.0823e-01,  2.7045e-01,  1.8069e-01, -2.0760e-01,\n",
       "         -4.9486e-01,  6.4207e-02, -4.7424e-01, -4.7247e-01,  9.7740e-01,\n",
       "          2.4827e-01, -2.2980e-01, -4.7967e-01, -3.5014e-01,  5.8310e-01,\n",
       "         -1.6662e-01, -1.4073e-01, -1.6845e-01,  3.4730e-01,  1.9024e-01,\n",
       "          6.7369e-01,  1.2889e-01, -1.7597e-01,  4.0943e-01, -1.5266e-01,\n",
       "          4.6617e-02, -2.3934e-01,  2.7047e-01,  1.4761e-01, -5.4200e-02,\n",
       "          6.3219e-01,  5.4383e-01,  4.8124e-01,  1.5749e-01, -2.4815e-01,\n",
       "         -3.2090e-01, -3.8481e-01,  3.4047e-01, -7.2322e-03, -1.9430e-01,\n",
       "         -7.1731e-01, -2.0531e-01,  1.9530e-01, -2.4815e-01, -1.7804e-01,\n",
       "         -3.7555e-02, -7.6236e-02,  1.9987e-02, -2.2775e-02,  2.3974e-01,\n",
       "         -2.1783e-02, -1.5087e-01,  1.5589e-01,  2.9854e-01, -1.3989e-01,\n",
       "         -2.2885e-01, -1.4880e-01,  1.6819e-01, -5.3511e-01,  5.4603e-01,\n",
       "         -2.2878e-01, -3.2142e-01,  7.3526e-01, -6.1169e-01, -4.8713e-01,\n",
       "         -4.0058e-01, -6.2693e-02,  1.7566e-01,  1.4444e-01, -1.2210e-01,\n",
       "          4.4788e-01,  2.2937e-01, -3.2018e-01,  1.9796e-01, -5.6837e-02,\n",
       "          2.6096e-01, -3.4548e-01,  7.1923e-01, -2.2716e-01, -9.9035e-01,\n",
       "         -1.2675e-01,  1.5544e-01, -3.1953e-01, -1.3235e-01, -3.1740e-02,\n",
       "          3.9417e-01, -6.5593e-01, -7.9141e-02,  4.3583e-02,  3.7308e-01,\n",
       "          2.7178e-01,  1.8019e-01,  2.6099e-01,  6.1881e-01,  3.9481e-04,\n",
       "         -4.7388e-01,  3.0933e-01,  8.2697e-02, -7.9508e-01,  3.5085e-01,\n",
       "          5.1901e-02,  1.0405e-01,  4.7072e-01, -4.0174e-02, -1.4599e-02,\n",
       "          8.3241e-01, -1.6919e-01, -3.4786e-02, -1.4783e-01,  4.3870e-02,\n",
       "         -2.3178e-01, -3.5469e-01, -5.3105e-01, -3.1063e-02,  2.6565e-01,\n",
       "          4.6710e-01,  2.6176e-01, -5.4494e-01,  1.8772e-01,  5.7337e-03,\n",
       "          4.1333e-03, -1.1065e+00, -4.9407e-01,  5.2030e-01,  6.4619e-02,\n",
       "         -5.1163e-01,  6.3805e-01, -1.5576e-01,  3.1930e-01, -2.8927e-01,\n",
       "          7.8576e-02, -1.3393e-01, -1.0509e-01, -1.9956e-01,  2.7379e-01,\n",
       "          1.9384e-01,  2.3538e-01, -1.9192e-01,  2.6308e-01, -7.9439e-01,\n",
       "         -1.5733e-01,  1.5274e-02,  3.7863e-01,  2.7647e-01, -5.3037e-02,\n",
       "          4.0382e-01, -3.1602e-01, -6.9714e-02,  2.1911e-01, -4.3706e-01,\n",
       "          4.2852e-01, -1.2900e-01, -3.7363e-02, -5.1328e-01, -2.7734e-01,\n",
       "         -6.0368e-01,  1.0658e+00, -1.0208e-01,  3.3501e-02, -2.3333e-01,\n",
       "          3.3786e-01, -5.1682e-02, -3.4159e-01,  6.2814e-02, -3.5485e-01,\n",
       "         -3.8236e-01,  7.4997e-02,  1.7587e-01, -1.8268e-01, -9.2884e-01,\n",
       "          6.8764e-02, -3.6725e-01,  2.4847e-01,  2.8082e-01,  2.5929e-02,\n",
       "          2.4082e-01, -4.7885e-01,  1.1303e-01,  8.1795e-01, -2.2024e-01,\n",
       "         -1.1602e-01,  4.2297e-01,  1.2215e-01,  3.8900e-02, -1.8449e-01,\n",
       "         -5.2993e-01, -1.9165e-01, -6.7478e-02,  7.6006e-01,  3.7717e-01,\n",
       "          1.0525e-01,  4.0541e-01, -5.8220e-02, -2.0588e-01, -4.9586e-01,\n",
       "          5.0746e-01,  1.6525e-01,  4.4558e-01,  4.0752e-01, -6.8262e-03,\n",
       "         -4.1747e-01,  2.3782e-01,  4.1318e-01, -5.0221e-02,  5.5453e-01,\n",
       "         -6.4683e-01, -4.4336e-02, -8.6599e-01, -2.4878e-02,  5.9661e-02,\n",
       "         -4.2274e-01,  6.5712e-02,  1.0556e-01, -3.9084e-02, -2.4387e-02,\n",
       "          3.8211e-01,  3.5885e-01, -2.6872e-01,  6.5052e-02, -2.1752e-01,\n",
       "         -7.0258e-01,  4.4557e-01,  2.7316e-01,  2.3383e-01, -1.2112e-01,\n",
       "         -2.8028e-01,  2.9833e-02,  4.4031e-01,  4.9814e-01, -4.9994e-01,\n",
       "         -4.0837e-01, -1.5809e-01,  9.1071e-01,  3.4235e-01, -6.7054e-01,\n",
       "          4.2254e-01,  5.1012e-02,  1.8245e-01, -4.4694e-01, -3.3059e-01,\n",
       "         -1.0062e+00,  3.4386e-01,  7.3352e-02, -2.1532e-01,  6.0052e-03,\n",
       "          2.6653e-01,  4.4150e-02, -8.8653e-01,  2.7187e-01, -4.5934e-01,\n",
       "          2.2709e-01, -2.8417e-02, -1.0572e-01, -4.3965e-01,  5.8809e-01,\n",
       "         -3.5564e-01,  1.3675e-01,  3.2628e-01,  1.5952e-01,  5.9566e-01,\n",
       "          3.3367e-01, -1.7310e-01, -3.7402e-01, -1.8353e-01,  2.3132e-01,\n",
       "          1.4869e-01,  3.4188e-01,  2.2180e-01,  3.5708e-01, -1.3925e-01,\n",
       "          7.5997e-02,  3.3707e-01, -1.3875e-01,  3.9185e-01, -5.1041e-01,\n",
       "         -1.1305e-01,  4.3924e-01,  2.5076e-01,  1.0454e-01,  1.0122e-01,\n",
       "          3.8556e-01, -5.6014e-01,  3.2747e-01, -1.7951e-02, -2.2364e-01,\n",
       "          7.6760e-01, -4.1762e-01, -3.9553e-01,  1.1585e-01, -6.0766e-01,\n",
       "          2.7837e-01,  1.8995e-01, -4.5793e-01,  1.3992e-01,  6.8488e-01,\n",
       "          4.3453e-02,  4.8588e-01, -4.7249e-01,  3.8173e-02, -5.9296e-02,\n",
       "          2.7576e-01,  6.3156e-02, -2.8538e-01,  5.7179e-01, -5.9283e-02,\n",
       "         -1.4260e-01,  1.3862e-02, -6.5636e-01, -1.4035e-02, -2.5684e-01,\n",
       "          2.8075e-02, -2.6262e-01, -1.3177e-01, -2.1191e-01, -3.6589e-01,\n",
       "         -1.0981e-01,  2.6787e-01, -1.0315e-01,  3.2219e-02, -5.2557e-01,\n",
       "         -3.9991e-01, -1.8254e-01,  1.1943e-01, -5.8115e-01,  4.0628e-01,\n",
       "          6.5794e-02,  2.1031e-01, -6.4099e-01, -2.2101e-01, -2.0665e-01,\n",
       "         -8.0474e-01, -1.0776e+00, -5.9463e-02, -1.4398e-01, -1.2949e-01,\n",
       "         -1.0005e-01,  2.4202e-01, -3.2595e-01,  1.8448e-01, -3.5674e-01,\n",
       "          2.6962e-01, -2.9421e-01, -3.3767e-01, -1.5344e-01,  2.3608e-03,\n",
       "         -1.2741e-01, -2.0004e-01,  3.4947e-01,  3.4620e-01,  4.3480e-02,\n",
       "          5.0806e-01, -2.8636e-01, -1.3689e-01, -9.4905e-01, -6.1498e-01,\n",
       "          4.0603e-01, -3.1845e-01, -4.1508e-02,  3.1599e-01, -5.6598e-01,\n",
       "         -5.3515e-01, -2.0336e-01, -3.0881e-01,  1.8943e-01],\n",
       "        [-2.0587e-01,  1.1884e-01, -2.2482e-02, -3.1625e-02,  1.5184e-01,\n",
       "          1.1111e-01, -6.6318e-02, -8.1527e-03,  9.7960e-03,  4.2994e-02,\n",
       "          5.7560e-02,  8.1331e-02, -1.9971e-01,  1.9565e-01,  4.1747e-02,\n",
       "         -7.3740e-02,  2.3067e-04, -2.9291e-01,  3.5360e-01,  1.3715e-01,\n",
       "          2.4998e-02,  9.3959e-05,  1.9917e-01,  1.2124e-01,  1.2885e-01,\n",
       "         -7.2194e-02,  9.9258e-02,  7.4610e-02,  2.3222e-01,  1.3152e-01,\n",
       "          4.8384e-02,  3.2023e-02, -3.8951e-01,  2.2698e-02, -8.9989e-02,\n",
       "         -1.8144e-01,  2.1185e-01,  3.6041e-02, -2.7172e-01,  1.4440e-02,\n",
       "          4.1158e-02,  1.3998e-01, -2.4242e-01,  6.2838e-02, -1.2625e-02,\n",
       "          1.2609e-02, -4.7662e-02, -1.3032e-01, -1.9817e-01,  1.9775e-01,\n",
       "          3.3042e-02,  1.7687e-01, -1.0056e-01, -9.3634e-02,  1.7855e-01,\n",
       "          1.9420e-01,  1.5485e-01,  1.7959e-01,  1.0373e-01,  7.0692e-02,\n",
       "          3.0065e-03,  1.0722e-02,  3.1751e-02, -4.2157e-02, -1.4355e-02,\n",
       "         -2.9310e-02,  1.8820e-02,  3.9800e-02,  1.1325e-01, -7.5900e-02,\n",
       "         -5.1772e-02,  1.8283e-02, -1.0476e-01, -1.5354e-01, -2.6666e-02,\n",
       "         -7.0398e-02, -1.7033e-01,  6.6486e-02, -2.0695e-01, -5.2690e-02,\n",
       "          1.8962e-01,  2.2826e-01,  2.3214e-01,  6.2224e-02,  4.3299e-02,\n",
       "         -3.2569e-02, -1.0966e-01,  2.2863e-02,  1.5360e-01,  1.2007e-01,\n",
       "          8.4911e-02,  3.3060e-02, -4.6195e-02,  1.5625e-01, -2.7462e-01,\n",
       "          3.8053e-02, -1.4659e-01, -1.8058e-01, -4.6884e-02,  7.6885e-02,\n",
       "          3.1301e-02, -1.3089e-01,  4.2419e-02,  3.6658e-03,  3.5888e-02,\n",
       "         -2.9677e-01, -1.3871e-01,  1.2274e-01, -7.3507e-02, -3.3545e-02,\n",
       "          1.9048e-01,  1.1266e-01,  1.7449e-01,  8.3429e-02, -1.4954e-02,\n",
       "         -2.9363e-03, -2.0285e-01, -1.6782e-01, -4.8077e-02, -9.4123e-02,\n",
       "          1.7272e-01,  7.1688e-02,  1.8527e-01, -4.3023e-02,  9.2012e-02,\n",
       "          2.4761e-02,  9.7265e-03, -9.2604e-02,  1.8236e-01, -1.2253e-01,\n",
       "          1.9825e-01, -1.0383e-01, -1.1231e-02, -1.1480e-01,  2.0789e-03,\n",
       "          8.2009e-02,  6.0314e-02,  1.6293e-01, -1.6574e-01,  1.3492e-01,\n",
       "          8.8088e-02, -1.7688e-01, -2.2786e-02, -3.3470e-02,  7.4289e-02,\n",
       "          1.8882e-01,  3.4039e-03, -4.0784e-01, -8.7583e-02,  1.7337e-01,\n",
       "          1.0171e-01, -7.9978e-02,  1.1346e-01, -4.1529e-02, -1.1694e-01,\n",
       "          2.4527e-01,  1.7961e-01,  1.1824e-02,  1.1071e-01, -1.2859e-01,\n",
       "         -5.1524e-02,  5.7655e-03,  2.6683e-01,  1.2383e-01,  3.1218e-02,\n",
       "          2.9229e-02,  1.5737e-01, -2.9350e-01,  6.2368e-02,  6.7273e-02,\n",
       "          9.0300e-02,  5.6078e-02, -3.6263e-02, -6.3798e-04,  7.9042e-02,\n",
       "          3.2154e-02,  1.1022e-01, -2.2809e-01,  1.1348e-02,  1.2067e-01,\n",
       "          1.1045e-01, -1.3507e-01,  7.2774e-02,  1.4199e-01,  9.0587e-02,\n",
       "         -4.8139e-02, -4.7633e-02,  3.9890e-02,  1.0679e-01,  1.1419e-01,\n",
       "          1.5724e-01, -1.6409e-01, -7.2328e-02,  2.3416e-02,  8.1615e-02,\n",
       "         -1.4907e-02, -2.6557e-01, -1.3267e-01,  2.4046e-02,  2.9170e-01,\n",
       "         -1.4570e-01, -7.8255e-02,  3.5704e-02,  4.5254e-02, -5.4157e-02,\n",
       "          1.3161e-01,  4.8148e-02, -2.5106e-01, -1.0264e-01, -9.0159e-02,\n",
       "          1.1277e-01, -2.3217e-02, -1.2727e-01, -1.6982e-01,  1.1626e-01,\n",
       "         -8.5800e-02,  2.2214e-01,  1.3251e-01, -7.8735e-02, -3.4192e-01,\n",
       "          1.2596e-01,  1.2274e-01,  9.1668e-03, -7.0625e-02, -2.3762e-01,\n",
       "         -8.9100e-02, -2.2451e-01, -1.6834e-01, -2.4798e-01,  5.2500e-02,\n",
       "          5.7830e-02,  9.8765e-03,  3.6162e-02,  1.8292e-02, -7.4141e-02,\n",
       "          2.7676e-01, -8.5787e-02, -4.1892e-02, -1.2623e-01,  3.5767e-02,\n",
       "         -4.8359e-02,  5.1536e-02,  6.9675e-02,  1.4936e-02,  1.9475e-01,\n",
       "         -1.5348e-01,  1.7646e-01,  1.7505e-01,  2.0526e-01, -9.5869e-02,\n",
       "          1.4219e-01, -8.6760e-02, -2.3472e-01, -2.6985e-02, -4.5886e-02,\n",
       "         -5.9408e-02,  1.6406e-01, -5.9937e-02,  9.9868e-02,  9.5796e-02,\n",
       "          2.7451e-01, -1.4064e-01,  1.2275e-01, -2.6540e-01, -7.4734e-04,\n",
       "         -3.7475e-02,  1.6352e-01,  6.6957e-02,  4.2954e-02,  3.7865e-02,\n",
       "         -2.5041e-02,  4.2524e-02, -1.3135e-01, -1.4388e-01,  7.5549e-02,\n",
       "          7.9228e-02,  3.1429e-01, -5.4649e-02, -1.4427e-01,  2.6873e-02,\n",
       "          3.5940e-02,  4.2910e-02,  7.8427e-02,  4.2507e-02,  1.4661e-01,\n",
       "         -2.6442e-01,  6.7110e-02, -6.6934e-02, -9.0239e-02, -1.2341e-02,\n",
       "         -4.3237e-02, -9.6624e-02,  1.7957e-01, -4.9634e-02, -9.0820e-02,\n",
       "          3.1792e-02,  7.9043e-02,  1.1419e-01, -1.9647e-01, -3.6415e-02,\n",
       "         -1.1925e-01, -2.2290e-02,  1.3564e-01, -2.7770e-01, -1.2593e-02,\n",
       "          1.4463e-02,  6.0754e-02, -2.3290e-01, -2.9924e-02, -8.1298e-02,\n",
       "          6.1922e-02,  6.7026e-02,  2.9272e-02,  2.5952e-02, -5.3489e-02,\n",
       "         -1.6306e-01,  1.2351e-01,  1.3645e-01,  1.2007e-01, -1.0548e-01,\n",
       "         -5.7040e-02,  3.9465e-02, -2.0952e-02,  1.5856e-01,  8.2102e-02,\n",
       "          5.8445e-03, -2.6330e-02,  2.8411e-02, -1.1434e-01,  8.8294e-02,\n",
       "          2.1201e-02, -7.5293e-02, -7.2708e-02,  6.8810e-02,  1.0219e-01,\n",
       "         -7.6677e-02,  7.5636e-02,  1.5770e-01,  2.3140e-02, -3.8821e-03,\n",
       "         -9.4342e-03, -5.7408e-02, -1.4787e-01,  4.0308e-03, -3.0649e-02,\n",
       "         -1.8234e-01, -2.3267e-01, -2.2372e-02,  5.3566e-02,  9.9974e-02,\n",
       "          4.7408e-02, -1.1947e-02,  3.7725e-02, -6.9913e-02, -6.4166e-03,\n",
       "          9.4644e-02,  1.0096e-01, -1.4494e-02,  1.5596e-01, -4.8722e-02,\n",
       "          1.0013e-01, -1.2159e-01, -1.9393e-01, -4.8422e-02,  1.7054e-01,\n",
       "         -1.4674e-01, -1.3072e-02,  1.2334e-01,  2.3091e-02,  9.7206e-02,\n",
       "          5.9339e-02,  6.4342e-02, -5.6567e-02,  6.0699e-02, -5.6791e-02,\n",
       "         -7.2247e-03, -9.4754e-02, -2.3054e-02, -2.1775e-01,  1.2777e-02,\n",
       "          4.6845e-02,  1.1517e-01,  8.3323e-02,  1.9126e-01]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.pooler_output - batch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5107,  102,    0,    0,    0,    0],\n",
       "        [ 101, 7592, 2026, 2171, 2003, 4789,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"chunk_store.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index Information:\n",
      "Number of vectors: 41\n",
      "Vector dimension: 384\n",
      "Index type: IndexIDMap\n",
      "\n",
      "Document Metadata:\n",
      "Metadata not directly accessible from raw FAISS index\n",
      "\n",
      "Metadata from chunk store:\n",
      "   doc_id  chunk_id\n",
      "0       0         0\n",
      "1       0         1\n",
      "2       0         2\n",
      "3       0         3\n",
      "4       0         4\n",
      "\n",
      "Note: Direct vector access requires custom implementation\n",
      "Consider using document_store.search() with specific queries to retrieve vectors\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index directly\n",
    "faiss_index = faiss.read_index(\"faiss_document_index.faiss\")\n",
    "\n",
    "# Get basic information about the index\n",
    "num_vectors = faiss_index.ntotal\n",
    "dimension = faiss_index.d\n",
    "\n",
    "print(f\"FAISS Index Information:\")\n",
    "print(f\"Number of vectors: {num_vectors}\")\n",
    "print(f\"Vector dimension: {dimension}\")\n",
    "print(f\"Index type: {type(faiss_index).__name__}\")\n",
    "\n",
    "# Load metadata from external source if needed\n",
    "try:\n",
    "    # Try to load metadata from a separate file if it exists\n",
    "    metadata_df = pd.read_parquet(\"chunk_store.parquet\")\n",
    "    print(\"\\nMetadata from chunk store:\")\n",
    "    print(metadata_df[[\"doc_id\", \"chunk_id\"]].head())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metadata: {str(e)}\")\n",
    "\n",
    "# Get a sample of vectors if possible\n",
    "print(\"\\nNote: Direct vector access requires custom implementation\")\n",
    "print(\"Consider using document_store.search() with specific queries to retrieve vectors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
