{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/python_envs/dsl-research-assistant/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))  # /src\n",
    "\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/python_envs/dsl-research-assistant/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import misc\n",
    "reload(misc)\n",
    "from semantic_search.misc import LocalEmbeddingModel, FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index with 73809 vectors\n"
     ]
    }
   ],
   "source": [
    "embedding_model = LocalEmbeddingModel(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='cuda',\n",
    "    batch_size=8\n",
    ")\n",
    "document_store = FAISSDocumentStore(\n",
    "    embedding_model=embedding_model,\n",
    "    db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/chunk1-txt-all',\n",
    ")\n",
    "\n",
    "if not document_store.load_index():\n",
    "    document_store.create_index('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er-efficient visual instruction model. arXiv 2304.15010 , 2023. 1, 3, 4\n",
      "- [9] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role...\n"
     ]
    }
   ],
   "source": [
    "print(document_store.search(\"visual\", top_k=5)[0]['chunk_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show wider dataframes\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text in columns\n",
    "pd.set_option('display.width', 1000)         # Set the display width\n",
    "pd.set_option('display.max_columns', 20)     # Show more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06f8ad35e094fadbead424dc90ffb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>fname</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>oaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt</td>\n",
       "      <td>Fixed Point Diffusion Models</td>\n",
       "      <td>https://doi.org/10.1063/1.2121687</td>\n",
       "      <td>https://openalex.org/W2000456051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt</td>\n",
       "      <td>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52733.2024.01901</td>\n",
       "      <td>https://openalex.org/W4402727763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt</td>\n",
       "      <td>Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild</td>\n",
       "      <td>https://doi.org/10.1109/cvprw63382.2024.00473</td>\n",
       "      <td>https://openalex.org/W4402916217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 fpath                                                                                                            fname                                                                                                                      title                                            doi                              oaid\n",
       "0                                                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                             Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.txt                                                                                               Fixed Point Diffusion Models              https://doi.org/10.1063/1.2121687  https://openalex.org/W2000456051\n",
       "1                             /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                             Li_BEVNeXt_Reviving_Dense_BEV_Frameworks_for_3D_Object_Detection_CVPR_2024_paper.txt                                                             BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection   https://doi.org/10.1109/cvpr52733.2024.01901  https://openalex.org/W4402727763\n",
       "2  /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Savchenko_Leveraging_Pre-trained_Multi-task_Deep_Models_for_Trustworthy_Facial_Analysis_in_CVPRW_2024_paper.txt  Leveraging Pre-trained Multi-task Deep Models for Trustworthy Facial Analysis in Affective Behaviour Analysis in-the-Wild  https://doi.org/10.1109/cvprw63382.2024.00473  https://openalex.org/W4402916217"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pyalex\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "df = pd.DataFrame([(str(fpath), fpath.name) for fpath in Path(raw_dir).glob(\"*.txt\")], columns=['fpath', 'fname'])\n",
    "\n",
    "def get_title(fpath: str):\n",
    "    doc_text = Path(fpath).read_text(encoding=\"utf-8\")\n",
    "    title_match = re.search(r'## ([^\\n#]+)', doc_text)\n",
    "    return title_match.group(1) if title_match else None\n",
    "\n",
    "def get_metadata(title: str):\n",
    "    search_results = pyalex.Works().search(title).select(['id', 'doi', 'referenced_works']).get(page=1, per_page=1)\n",
    "    return (search_results[0]['doi'], search_results[0]['id'], search_results[0]['referenced_works']) if search_results else (None, None, None)\n",
    "\n",
    "df['title'] = df['fpath'].apply(get_title)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(get_metadata, df['title'].values),\n",
    "        total=len(df)\n",
    "    ))\n",
    "df['doi', 'oaid', 'referenced_works'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(results, columns=['doi', 'oaid', 'referenced_works'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/paper-metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Korycki_Class-Incremental_Mixture_of_Gaussians_for_Deep_Continual_Learning_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hoque_IrrNet_Spatio-Temporal_Segmentation_Guided_Classification_for_Irrigation_Mapping_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Zhang_MOHO_Learning_Single-view_Hand-held_Object_Reconstruction_with_Multi-view_Occlusion-Aware_Supervision_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Paissan_Structured_Sparse_Back-propagation_for_Lightweight_On-Device_Continual_Learning_on_Microcontroller_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Halawa_Multi-Task_Multi-Modal_Self-Supervised_Learning_for_Facial_Expression_Recognition_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Loaliyan_Comparative_Analysis_of_Generalization_and_Harmonization_Methods_for_3D_Brain_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Hu_Low_Latency_Point_Cloud_Rendering_with_Learned_Splatting_CVPRW_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Ko_Semantic_Line_Combination_Detector_CVPR_2024_paper.txt\n",
      "/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt/Duan_Towards_Backward-Compatible_Continual_Learning_of_Image_Compression_CVPR_2024_paper.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_dir = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/Conversions/opencvf-data/txt'\n",
    "\n",
    "def check_single_doc(fpath: str):\n",
    "    doc_text = fpath.read_text(encoding=\"utf-8\")\n",
    "    abstract_match = re.search(r'## Abstract\\n\\n(.*?)(?=\\n\\n## \\d+\\.)', doc_text, re.DOTALL)\n",
    "    return abstract_match.group(1) if abstract_match else None\n",
    "\n",
    "for fpath in Path(raw_dir).glob(\"*.txt\"):\n",
    "    if check_single_doc(fpath) is None:\n",
    "        print(fpath)\n",
    "\n",
    "# abstracts =[check_single_doc(fpath) for fpath in Path(raw_dir).glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>         1133\n",
       "<class 'NoneType'>       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(str(type(a)) for a in abstracts).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for: 'visual odometry in robotics applications'\n",
      "Rank 1 (Score: 0.7618)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "Visual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 (Score: 0.7586)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: current neural network for (un-) supervised learning of monocular video visual odometry and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5555-556...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 (Score: 0.7444)\n",
      "Document: Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt\n",
      "Preview: This CVPR Workshop paper is the Open Access version, provided by the Computer Vision Foundation.\n",
      "\n",
      "Except for this watermark, it is identical to the accepted version;\n",
      "\n",
      "the final published version of th...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example search function\n",
    "def search_documents(query, top_k=5):\n",
    "    results = document_store.search(query, top_k=top_k)\n",
    "    print(f\"Search results for: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"Rank {result['rank']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Document: {result['document_name']}\")\n",
    "        print(f\"Preview: {result['chunk_text']}\")\n",
    "        print(\"-\" * 80)\n",
    "    return results\n",
    "\n",
    "# Test the search\n",
    "search_results = search_documents(\"visual odometry in robotics applications\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'score': np.float32(0.506165),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'omain (real data) while achieving state-of-the-art performance on the KITTI dataset.\\n\\n## 1. Introduction\\n\\nVisual odometry (VO) is a crucial aspect of robotics that enables machines to measure the ego-...'},\n",
       " {'rank': 2,\n",
       "  'score': np.float32(0.47257516),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'ons on robotics , 33(5):1255-1262, 2017. 2, 6\\n- [14] David NistÂ´ er, Oleg Naroditsky, and James Bergen. Visual\\n\\n- odometry. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Visi...'},\n",
       " {'rank': 3,\n",
       "  'score': np.float32(0.4703958),\n",
       "  'document_id': 0,\n",
       "  'document_name': 'Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'document_path': 'example_data/Abouee_Weakly_Supervised_End2End_Deep_Visual_Odometry_CVPRW_2024_paper.txt',\n",
       "  'chunk_text': 'current neural network for (un-) supervised learning of monocular video visual odometry and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5555-556...'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.search(\"visual odometry in robotics applications\"*100000, top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
