{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b059328c7cb948838f6f81caf6753f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a652a4f480e4d2e8be68557c4347dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be659619cd844ea9edccbcb7f5093e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/717k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1024af448e44818f8becec8d6c8040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd93ac4ab7642939be7e3440cd7c407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4299bc9056c74365b8613d000cf0ceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6212578eec4621908475479a269a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store import LocalEmbeddingModel\n",
    "\n",
    "model = LocalEmbeddingModel(\n",
    "    model_name='allenai/specter2_base',  # 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking and encoding: 100%|██████████| 4/4 [00:00<00:00, 585.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 8 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tmp_input = \"Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model. You can concatenate multiple poolings together.\"\n",
    "\n",
    "inputs = [tmp_input*10, tmp_input*3, tmp_input, tmp_input*30]\n",
    "\n",
    "_, all_chunks_encoded = model.chunk_and_encode(inputs, progress_bar=True)\n",
    "\n",
    "# Flatten encoded\n",
    "tmp = {}\n",
    "for single_text_encoded in all_chunks_encoded:\n",
    "    for k, v in single_text_encoded.items():\n",
    "        if k not in tmp:\n",
    "            tmp[k] = []\n",
    "        tmp[k].append(v)\n",
    "encoded_flattened = {k: torch.cat(v) for k, v in tmp.items()}\n",
    "\n",
    "# Get embeddings for all chunks\n",
    "print(f\"Generating embeddings for {len(list(encoded_flattened.values())[0])} chunks...\")\n",
    "embeddings = model.get_embeddings(encoded_flattened, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "_, encoded = model.chunk_and_encode(tmp_input*100)\n",
    "emb = model.get_embeddings(encoded)\n",
    "\n",
    "\n",
    "encoded_input = model.tokenizer(tmp_input*100, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model.model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0] == sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev for new cleaner store setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store import LocalEmbeddingModel, FAISSDocumentStore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded index with 103 vectors\n"
     ]
    }
   ],
   "source": [
    "ds = FAISSDocumentStore(db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/allenai_specter2_base_test2')\n",
    "assert ds.load_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'score': 0.532930805335529,\n",
       "  'doc_id': 'W2772709170',\n",
       "  'doi': '10.48550/arxiv.1712.01815',\n",
       "  'topic': 'Artificial Intelligence in Games'},\n",
       " {'rank': 2,\n",
       "  'score': 0.5323105859180169,\n",
       "  'doc_id': 'W4310561607',\n",
       "  'doi': '10.48550/arxiv.2211.17111',\n",
       "  'topic': 'Parallel Computing and Optimization Techniques'},\n",
       " {'rank': 3,\n",
       "  'score': 0.5287573212579326,\n",
       "  'doc_id': 'W2222512263',\n",
       "  'doi': '10.1002/nav.3800020109',\n",
       "  'topic': 'Optimization and Mathematical Programming'},\n",
       " {'rank': 4,\n",
       "  'score': 0.526998484957759,\n",
       "  'doc_id': 'W2755088640',\n",
       "  'doi': 'nan',\n",
       "  'topic': 'Complex Network Analysis Techniques'},\n",
       " {'rank': 5,\n",
       "  'score': 0.5267845637437224,\n",
       "  'doc_id': 'W2158131535',\n",
       "  'doi': 'nan',\n",
       "  'topic': 'Machine Learning and Algorithms'},\n",
       " {'rank': 6,\n",
       "  'score': 0.5267383869762133,\n",
       "  'doc_id': 'W2151554678',\n",
       "  'doi': '10.1109/4235.585893',\n",
       "  'topic': 'Metaheuristic Optimization Algorithms Research'},\n",
       " {'rank': 7,\n",
       "  'score': 0.5255418443014074,\n",
       "  'doc_id': 'W2964081807',\n",
       "  'doi': '10.1109/cvpr.2018.00907',\n",
       "  'topic': 'Advanced Neural Network Applications'},\n",
       " {'rank': 8,\n",
       "  'score': 0.5223727041560647,\n",
       "  'doc_id': 'W4396941548',\n",
       "  'doi': '10.1109/lra.2024.3401172',\n",
       "  'topic': 'Video Surveillance and Tracking Methods'},\n",
       " {'rank': 9,\n",
       "  'score': 0.5186939505797413,\n",
       "  'doc_id': 'W2913668833',\n",
       "  'doi': 'nan',\n",
       "  'topic': 'Machine Learning and Data Classification'},\n",
       " {'rank': 10,\n",
       "  'score': 0.5169324550185663,\n",
       "  'doc_id': 'W2913668833',\n",
       "  'doi': 'nan',\n",
       "  'topic': 'Machine Learning and Data Classification'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.search('What is the main idea of the paper?', top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = {\n",
    "    'chunk_id': ds.chunk_store['chunk_id'].tolist(),\n",
    "    'doc_id': ds.chunk_store['doc_id'].tolist(),\n",
    "    'text': ds.chunk_store['text'].tolist(),\n",
    "}\n",
    "\n",
    "chunk_store = pd.DataFrame(a).astype({'chunk_id': 'int64'}).set_index('chunk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunk_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2194775991</td>\n",
       "      <td>deeper neural networks are more difficult to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W1686810756</td>\n",
       "      <td>in this work we investigate the effect of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W4385245566</td>\n",
       "      <td>the dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W2108598243</td>\n",
       "      <td>the explosion of image data on the internet ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W2133665775</td>\n",
       "      <td>objective methods for assessing perceptual ima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>W2013439747</td>\n",
       "      <td>this paper discusses the experimental setup of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>W3158642773</td>\n",
       "      <td>abstract despite the rapid development and pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>W3005695990</td>\n",
       "      <td>density estimation aims to predict the spatial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>W1564360503</td>\n",
       "      <td>lately, new methods for the acquisition of tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>W4389574997</td>\n",
       "      <td>we propose the first visible - light tomograph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               doc_id                                               text\n",
       "chunk_id                                                                \n",
       "0         W2194775991  deeper neural networks are more difficult to t...\n",
       "1         W1686810756  in this work we investigate the effect of the ...\n",
       "2         W4385245566  the dominant sequence transduction models are ...\n",
       "3         W2108598243  the explosion of image data on the internet ha...\n",
       "4         W2133665775  objective methods for assessing perceptual ima...\n",
       "...               ...                                                ...\n",
       "1011      W2013439747  this paper discusses the experimental setup of...\n",
       "1012      W3158642773  abstract despite the rapid development and pro...\n",
       "1013      W3005695990  density estimation aims to predict the spatial...\n",
       "1014      W1564360503  lately, new methods for the acquisition of tim...\n",
       "1015      W4389574997  we propose the first visible - light tomograph...\n",
       "\n",
       "[1016 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
