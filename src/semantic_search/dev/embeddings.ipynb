{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dev (tokenisation, embedding and pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b059328c7cb948838f6f81caf6753f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a652a4f480e4d2e8be68557c4347dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be659619cd844ea9edccbcb7f5093e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/717k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1024af448e44818f8becec8d6c8040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd93ac4ab7642939be7e3440cd7c407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4299bc9056c74365b8613d000cf0ceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6212578eec4621908475479a269a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store import LocalEmbeddingModel\n",
    "\n",
    "model = LocalEmbeddingModel(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',  # 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking and encoding: 100%|██████████| 4/4 [00:00<00:00, 585.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 8 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tmp_input = \"Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model. You can concatenate multiple poolings together.\"\n",
    "\n",
    "inputs = [tmp_input*10, tmp_input*3, tmp_input, tmp_input*30]\n",
    "\n",
    "_, all_chunks_encoded = model.chunk_and_encode(inputs, progress_bar=True)\n",
    "\n",
    "# Flatten encoded\n",
    "tmp = {}\n",
    "for single_text_encoded in all_chunks_encoded:\n",
    "    for k, v in single_text_encoded.items():\n",
    "        if k not in tmp:\n",
    "            tmp[k] = []\n",
    "        tmp[k].append(v)\n",
    "encoded_flattened = {k: torch.cat(v) for k, v in tmp.items()}\n",
    "\n",
    "# Get embeddings for all chunks\n",
    "print(f\"Generating embeddings for {len(list(encoded_flattened.values())[0])} chunks...\")\n",
    "embeddings = model.get_embeddings(encoded_flattened, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "_, encoded = model.chunk_and_encode(tmp_input*100)\n",
    "emb = model.get_embeddings(encoded)\n",
    "\n",
    "\n",
    "encoded_input = model.tokenizer(tmp_input*100, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model.model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0] == sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_search.utils import load_metadata\n",
    "\n",
    "df, ref_df = load_metadata(\n",
    "    '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata3',\n",
    "    filter_good_papers=True,\n",
    "    filter_good_references=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oaid</th>\n",
       "      <th>doi</th>\n",
       "      <th>ref_via</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>type</th>\n",
       "      <th>topic</th>\n",
       "      <th>domain</th>\n",
       "      <th>field</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2194775991</td>\n",
       "      <td>10.1109/cvpr.2016.90</td>\n",
       "      <td>openalex_id</td>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>Deeper neural networks are more difficult to t...</td>\n",
       "      <td>article</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          oaid                   doi      ref_via  \\\n",
       "0  W2194775991  10.1109/cvpr.2016.90  openalex_id   \n",
       "\n",
       "                                          title  \\\n",
       "0  Deep Residual Learning for Image Recognition   \n",
       "\n",
       "                                            abstract     type  \\\n",
       "0  Deeper neural networks are more difficult to t...  article   \n",
       "\n",
       "                                  topic             domain             field  \\\n",
       "0  Advanced Neural Network Applications  Physical Sciences  Computer Science   \n",
       "\n",
       "                                  subfield  \n",
       "0  Computer Vision and Pattern Recognition  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.31851532.lcarretero/ipykernel_946913/2934055006.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started(): pt.init(home_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrier-assemblies 5.11 jar-with-dependencies not found, downloading to /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/terrier-assemblies-5.11-jar-with-dependencies.jar4pglkysh.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyterrier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pt.started(): pt.init(home_dir=\u001b[33m'\u001b[39m\u001b[33m/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# import your documents into an index\u001b[39;00m\n\u001b[32m     14\u001b[39m indexer = pt.TerrierIndexer(\u001b[33m'\u001b[39m\u001b[33m/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/terrier_index\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:57\u001b[39m, in \u001b[36mbefore_init.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m started():\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mYou can only call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m before either you start using java or call pt.java.init()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:232\u001b[39m, in \u001b[36mlegacy_init\u001b[39m\u001b[34m(version, mem, packages, jvm_opts, redirect_io, logging, home_dir, boot_packages, tqdm, no_download, helper_version)\u001b[39m\n\u001b[32m    229\u001b[39m     pt.java.mavenresolver.offline()\n\u001b[32m    230\u001b[39m     deprecated_calls.append(\u001b[33m'\u001b[39m\u001b[33mpt.java.mavenresolver.offline()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m deprecated_calls.append(\u001b[33m'\u001b[39m\u001b[33mpt.java.init() # optional, forces java initialisation\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Import other java packages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:96\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/utils.py:94\u001b[39m, in \u001b[36monce.<locals>._once.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has already been run\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# how to handle errors?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m res = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m called = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:126\u001b[39m, in \u001b[36m_init\u001b[39m\u001b[34m(trigger)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# run pre-initialization setup\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, initializer \u001b[38;5;129;01min\u001b[39;00m initializers:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[43minitializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnius_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnius\u001b[39;00m \u001b[38;5;66;03m# noqa: PT100 \u001b[39;00m\n\u001b[32m    129\u001b[39m _started = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/terrier/java.py:50\u001b[39m, in \u001b[36mTerrierJavaInit.pre_init\u001b[39m\u001b[34m(self, jnius_config)\u001b[39m\n\u001b[32m     48\u001b[39m     trJar = pt.java.mavenresolver.get_package_jar(\u001b[33m\"\u001b[39m\u001b[33mcom.github.terrier-org.terrier-core\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mterrier-assemblies\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m5.x-SNAPSHOT\u001b[39m\u001b[33m\"\u001b[39m, artifact=\u001b[33m\"\u001b[39m\u001b[33mjar-with-dependencies\u001b[39m\u001b[33m\"\u001b[39m, force_download=configure[\u001b[33m'\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     trJar = \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmavenresolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_package_jar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTERRIER_PKG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterrier-assemblies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterrier_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjar-with-dependencies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m jnius_config.add_classpath(trJar)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# now the helper classes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/mavenresolver.py:112\u001b[39m, in \u001b[36mget_package_jar\u001b[39m\u001b[34m(orgName, packageName, version, file_path, artifact, force_download)\u001b[39m\n\u001b[32m    109\u001b[39m     mvnUrl = MAVEN_BASE_URL + filelocation\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43mwget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvnUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.HTTPError, urllib.error.URLError) \u001b[38;5;28;01mas\u001b[39;00m he:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == OnlineMode.UNSET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/wget.py:506\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, out, bar)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# get filename for temp file in current directory\u001b[39;00m\n\u001b[32m    505\u001b[39m prefix = detect_filename(url, out)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m (fd, tmpfile) = \u001b[43mtempfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkstemp\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.tmp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m os.close(fd)\n\u001b[32m    508\u001b[39m os.unlink(tmpfile)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/tempfile.py:341\u001b[39m, in \u001b[36mmkstemp\u001b[39m\u001b[34m(suffix, prefix, dir, text)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     flags = _bin_openflags\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/tempfile.py:256\u001b[39m, in \u001b[36m_mkstemp_inner\u001b[39m\u001b[34m(dir, pre, suf, flags, output_type)\u001b[39m\n\u001b[32m    254\u001b[39m _sys.audit(\u001b[33m\"\u001b[39m\u001b[33mtempfile.mkstemp\u001b[39m\u001b[33m\"\u001b[39m, file)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     fd = \u001b[43m_os\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/terrier-assemblies-5.11-jar-with-dependencies.jar4pglkysh.tmp'"
     ]
    }
   ],
   "source": [
    "# Running below in REPL works\n",
    "\n",
    "import os\n",
    "\n",
    "# Set PyTerrier home directory\n",
    "os.environ['PYTERRIER_HOME'] = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev'\n",
    "os.environ['TMPDIR'] = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/tmp'\n",
    "\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "\n",
    "if not pt.started(): pt.init(home_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev')\n",
    "\n",
    "# import your documents into an index\n",
    "# Create a pandas DataFrame with the documents\n",
    "\n",
    "doc_texts = ref_df.abstract\n",
    "\n",
    "docs_df = pd.DataFrame({\n",
    "    'docno': [str(i) for i in range(len(doc_texts))],\n",
    "    'text': doc_texts\n",
    "})\n",
    "\n",
    "# Use IterDictIndexer instead of TerrierIndexer\n",
    "indexer = pt.IterDictIndexer('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/terrier_index')\n",
    "index_ref = indexer.index(docs_df.to_dict('records'))\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")\n",
    "results = bm25.transform(pt.Utils.prepare_qrels(pd.DataFrame({'qid':['q1'], 'query':['your keywords here']})))\n",
    "print(results[['qid', 'docno', 'score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev for new cleaner store setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find javac",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:347\u001b[39m, in \u001b[36mget_jdk_home\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     jdk_home = \u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjavac\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.replace(\u001b[33m'\u001b[39m\u001b[33mbin/javac\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/posixpath.py:415\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[33;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     filename = os.fspath(filename)\n\u001b[32m    416\u001b[39m     path, ok = _joinrealpath(filename[:\u001b[32m0\u001b[39m], filename, strict, {})\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_search\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[32m      3\u001b[39m reload(store)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_search\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISSDocumentStore\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dsl/dsl-research-assistant/src/semantic_search/store/store.py:14\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# from rank_bm25 import BM25Okapi # Removed\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# import pickle # Removed, Pyserini handles its own persistence\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Pyserini imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msearch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlucene\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LuceneSearcher\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindex\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlucene\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexCollection\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/search/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pyserini: Reproducible IR research with sparse and dense representations\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_topics, get_topics_with_reader, get_qrels_file, get_qrels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/search/_base.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autoclass\n\u001b[32m     27\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     28\u001b[39m logging.basicConfig(level=logging.WARNING, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/pyclass.py:31\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# This might happen if the JVM's already been initialized. Just eat the error.\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnius\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autoclass, cast\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Base Java classes\u001b[39;00m\n\u001b[32m     34\u001b[39m JString = autoclass(\u001b[33m'\u001b[39m\u001b[33mjava.lang.String\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/__init__.py:45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjnius\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreflect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# XXX monkey patch methods that cannot be in cython.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Cython doesn't allow to set new attribute on methods it compiled\u001b[39;00m\n\u001b[32m     50\u001b[39m HASHCODE_MAX = \u001b[32m2\u001b[39m ** \u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/reflect.py:19\u001b[39m\n\u001b[32m     14\u001b[39m __all__ = (\u001b[33m'\u001b[39m\u001b[33mautoclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mensureclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprotocol_map\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m log = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mkivy\u001b[39m\u001b[33m'\u001b[39m).getChild(\u001b[34m__name__\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mClass\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mJavaClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetaclass\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetaJavaClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__javaclass__\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjava/lang/Class\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesiredAssertionStatus\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mJavaMethod\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m()Z\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_export_class.pxi:117\u001b[39m, in \u001b[36mjnius.MetaJavaClass.__new__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_export_class.pxi:177\u001b[39m, in \u001b[36mjnius.MetaJavaClass.resolve_class\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_env.pxi:11\u001b[39m, in \u001b[36mjnius.get_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_jvm_dlopen.pxi:95\u001b[39m, in \u001b[36mjnius.get_platform_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_jvm_dlopen.pxi:54\u001b[39m, in \u001b[36mjnius.create_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:60\u001b[39m, in \u001b[36mget_java_setup\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# go hunting for Javac and Java programs, in that order\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     JAVA_HOME = \u001b[43mget_jdk_home\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[32m     63\u001b[39m     JAVA_HOME = get_jre_home(platform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:351\u001b[39m, in \u001b[36mget_jdk_home\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m    347\u001b[39m             jdk_home = realpath(\n\u001b[32m    348\u001b[39m                 which(\u001b[33m'\u001b[39m\u001b[33mjavac\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    349\u001b[39m             ).replace(\u001b[33m'\u001b[39m\u001b[33mbin/javac\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    350\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnable to find javac\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jdk_home \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(jdk_home):\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: Unable to find javac"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search.store import store\n",
    "reload(store)\n",
    "from semantic_search.store.store import FAISSDocumentStore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:32:08.412831: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 12:32:09.511891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 12:32:11.497976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded index with 41220 vectors\n"
     ]
    }
   ],
   "source": [
    "ds = FAISSDocumentStore(db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/hybrid-dev3')\n",
    "assert ds.load_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'id': 'W2964137095',\n",
       "  'doi': '10.5244/c.30.87',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Wide Residual Networks',\n",
       "  'text': 'Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 2,\n",
       "  'id': 'W2194775991',\n",
       "  'doi': '10.1109/cvpr.2016.90',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Deep Residual Learning for Image Recognition',\n",
       "  'text': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 3,\n",
       "  'id': 'W2798581168',\n",
       "  'doi': '10.48550/arxiv.1804.10123',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image Classification',\n",
       "  'text': 'Deep residual networks (ResNets) made a recent breakthrough in deep learning. The core idea of ResNets is to have shortcut connections between layers that allow the network to be much deeper while still being easy to optimize avoiding vanishing gradients. These shortcut connections have interesting side-effects that make ResNets behave differently from other typical network architectures. In this work we use these properties to design a network based on a ResNet but with parameter sharing and with adaptive computation time. The resulting network is much smaller than the original network and can adapt the computational cost to the complexity of the input image.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 4,\n",
       "  'id': 'W2737740651',\n",
       "  'doi': '10.48550/arxiv.1707.04585',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'The Reversible Residual Network: Backpropagation Without Storing Activations',\n",
       "  'text': \"Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.\",\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 5,\n",
       "  'id': 'W2952574409',\n",
       "  'doi': '10.48550/arxiv.1611.04231',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Identity Matters in Deep Learning',\n",
       "  'text': 'An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \\\\emph{batch normalization}, but was also key to the immense success of \\\\emph{residual networks}. In this work, we put the principle of \\\\emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Adversarial Robustness in Machine Learning',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Artificial Intelligence'},\n",
       " {'rank': 6,\n",
       "  'id': 'W2964350391',\n",
       "  'doi': '10.1609/aaai.v31i1.11231',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning',\n",
       "  'text': 'Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Domain Adaptation and Few-Shot Learning',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Artificial Intelligence'},\n",
       " {'rank': 7,\n",
       "  'id': 'W2593110912',\n",
       "  'doi': '10.1109/cvpr.2017.539',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation',\n",
       "  'text': 'Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 8,\n",
       "  'id': 'W2747898905',\n",
       "  'doi': '10.1109/cvpr.2017.298',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Image Super-Resolution via Deep Recursive Residual Network',\n",
       "  'text': 'Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks, recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Advanced Image Processing Techniques',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 9,\n",
       "  'id': 'W4390872537',\n",
       "  'doi': '10.1109/iccv51070.2023.00499',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Poincaré ResNet',\n",
       "  'text': 'This paper introduces an end-to-end residual network that operates entirely on the Poincaré ball model of hyperbolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only performed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard Euclidean networks. In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincaré ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincaré 2D convolutions up to Poincaré residual connections. We identify three roadblocks for training convolutional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initializations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initialization that preserves norms over many layers. (ii) Residual networks rely heavily on batch normalization, which comes with expensive Fréchet mean calculations in hyperbolic space. We introduce Poincaré midpoint batch normalization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincaré layers, the computation graphs of deep learning libraries blow up, limiting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Topological and Geometric Data Analysis',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computational Theory and Mathematics'},\n",
       " {'rank': 10,\n",
       "  'id': 'W3129492101',\n",
       "  'doi': '10.1109/access.2021.3061062',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'A Residual Dense U-Net Neural Network for Image Denoising',\n",
       "  'text': 'In recent years, convolutional neural networks have achieved considerable success in different computer vision tasks, including image denoising. In this work, we present a residual dense neural network (RDUNet) for image denoising based on the densely connected hierarchical network. The encoding and decoding layers of the RDUNet consist of densely connected convolutional layers to reuse the feature maps and local residual learning to avoid the vanishing gradient problem and speed up the learning process. Moreover, global residual learning is adopted such that, instead of directly predicting the denoised image, the model predicts the residual noise of the corrupted image. The algorithm was trained for the case of additive white Gaussian noise and using a wide range of noise levels. Hence, one advantage of the proposal is that the denoising process does not require prior knowledge about the noise level. In order to evaluate the model, we conducted several experiments with natural image databases available online, achieving competitive results compared with state-of-the-art networks for image denoising. For comparison purpose, we use additive Gaussian noise with levels 10, 30, 50. In the case of grayscale images, we achieved PSNR of 34.39, 29.11, 26.99, and SSIM of 0.9297, 0.8193, 0.7491. For color images we obtained PSNR of 36.68, 31.43, 29.12, and SSIM of 0.9600, 0.8961, 0.8465.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Image and Signal Denoising Methods',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.search('Deep residual networks', top_k=10, retrieval_method='hybrid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
