{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMilvus dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_search.utils import load_metadata\n",
    "\n",
    "df, ref_df = load_metadata(\n",
    "    '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata3',\n",
    "    filter_good_papers=True,\n",
    "    filter_good_references=True\n",
    ")\n",
    "ref_df.rename(columns={'oaid': 'id', 'abstract': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_search\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmilvus_store\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MilvusDocumentStore\n\u001b[32m      8\u001b[39m model = LocalEmbeddingModel()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ds = \u001b[43mMilvusDocumentStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dsl/dsl-research-assistant/src/semantic_search/store/milvus_store.py:32\u001b[39m, in \u001b[36mMilvusDocumentStore.__init__\u001b[39m\u001b[34m(self, model, db_dir, collection_name, store_documents)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.doc_store_path: Path | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.milvus_db_path: Path | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dsl/dsl-research-assistant/src/semantic_search/store/milvus_store.py:35\u001b[39m, in \u001b[36mMilvusDocumentStore._setup_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdb_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc_store_path = Path(\u001b[38;5;28mself\u001b[39m.db_dir) / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_documents.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m.milvus_db_path = Path(\u001b[38;5;28mself\u001b[39m.db_dir) / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.db\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/pathlib.py:1116\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1113\u001b[39m \u001b[33;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[32m   1114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[31mFileExistsError\u001b[39m: [Errno 17] File exists: '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db'"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search.store import models, milvus_store\n",
    "reload(models)\n",
    "reload(milvus_store)\n",
    "from semantic_search.store.models import LocalEmbeddingModel\n",
    "from semantic_search.store.milvus_store import MilvusDocumentStore\n",
    "\n",
    "model = LocalEmbeddingModel()\n",
    "\n",
    "ds = MilvusDocumentStore(\n",
    "    model=model, \n",
    "    db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1',\n",
    "    store_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dsl/dsl-research-assistant/src/semantic_search/store/milvus_store.py:68\u001b[39m, in \u001b[36mMilvusDocumentStore.create_index_from_df\u001b[39m\u001b[34m(self, documents, overwrite)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.store_documents:\n\u001b[32m     67\u001b[39m     doc_store_path = Path(\u001b[38;5;28mself\u001b[39m.db_dir) / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_documents.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43mdocuments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_store_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m.document_store = documents\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Chunk and encode all documents at once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/core/frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/io/parquet.py:480\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m impl = get_engine(engine)\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/io/parquet.py:198\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m     merged_metadata = {**existing_metadata, **df_metadata}\n\u001b[32m    196\u001b[39m     table = table.replace_schema_metadata(merged_metadata)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io.BufferedWriter)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[32m    209\u001b[39m ):\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    130\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db'"
     ]
    }
   ],
   "source": [
    "ds.create_index_from_df(ref_df.iloc[:100], overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db',\n",
       " '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/milvus-dev1.db')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.db_dir, ds._client_db_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'text': 'deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than vgg nets [ 40 ] but still having lower complexity. an ensemble of these residual nets achieves 3. 57 % error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar - 10 with 100 and 1000 layers. the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28 % relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions1, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.',\n",
       "  'score': 0.6057924032211304},\n",
       " {'rank': 2,\n",
       "  'text': 'representing features at multiple scales is of great importance for numerous vision tasks. recent advances in backbone convolutional neural networks ( cnns ) continually demonstrate stronger multi - scale representation ability, leading to consistent performance gains on a wide range of applications. however, most existing methods represent the multi - scale features in a layer - wise manner. in this paper, we propose a novel building block for cnns, namely res2net, by constructing hierarchical residual - like connections within one single residual block. the res2net represents multi - scale features at a granular level and increases the range of receptive fields for each network layer. the proposed res2net block can be plugged into the state - of - the - art backbone cnn models, e. g., resnet, resnext, and dla. we evaluate the res2net block on all these models and demonstrate consistent performance gains over baseline models on widely - used datasets, e. g., cifar - 100 and imagenet. further ablation studies and experimental results on representative computer vision tasks, i. e., object detection, class activation mapping, and salient object detection, further verify the superiority of the res2net over the state - of - the -',\n",
       "  'score': 0.48701491951942444},\n",
       " {'rank': 3,\n",
       "  'text': 'depth estimation from single monocular images is a key component in scene understanding. most existing algorithms formulate depth estimation as a regression problem due to the continuous property of depths. however, the depth value of input data can hardly be regressed exactly to the ground - truth value. in this paper, we propose to formulate depth estimation as a pixelwise classification task. specifically, we first discretize the continuous ground - truth depths into several bins and label the bins according to their depth ranges. then, we solve the depth estimation problem as classification by training a fully convolutional deep residual network. compared with estimating the exact depth of a single point, it is easier to estimate its depth range. more importantly, by performing depth classification instead of regression, we can easily obtain the confidence of a depth prediction in the form of probability distribution. with this confidence, we can apply an information gain loss to make use of the predictions that are close to ground - truth during training, as well as fully - connected conditional random fields for post - processing to further improve the performance. we test our proposed method on both indoor and outdoor benchmark rgb - depth datasets and achieve state - of - the - art performance.',\n",
       "  'score': 0.4775729477405548},\n",
       " {'rank': 4,\n",
       "  'text': 'we propose a deep convolutional neural network architecture codenamed inception that achieves the new state of the art for classification and detection in the imagenet large - scale visual recognition challenge 2014 ( ilsvrc14 ). the main hallmark of this architecture is the improved utilization of the computing resources inside the network. by a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. to optimize quality, the architectural decisions were based on the hebbian principle and the intuition of multi - scale processing. one particular incarnation used in our submission for ilsvrc14 is called googlenet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.',\n",
       "  'score': 0.4658282399177551},\n",
       " {'rank': 5,\n",
       "  'text': 'pixel - level labelling tasks, such as semantic segmentation, play a central role in image understanding. recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel - level labelling tasks. one central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. to solve this problem, we introduce a new form of convolutional neural network that combines the strengths of convolutional neural networks ( cnns ) and conditional random fields ( crfs ) - based probabilistic graphical modelling. to this end, we formulate conditional random fields with gaussian pairwise potentials and mean - field approximate inference as recurrent neural networks. this network, called crf - rnn, is then plugged in as a part of a cnn to obtain a deep network that has desirable properties of both cnns and crfs. importantly, our system fully integrates crf modelling with cnns, making it possible to train the whole deep network end - to - end with the usual back - propagation algorithm, avoiding offline post - processing methods for object delineation. we apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging',\n",
       "  'score': 0.4520341157913208},\n",
       " {'rank': 6,\n",
       "  'text': 'recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. in this paper, we embrace this observation and introduce the dense convolutional network ( densenet ), which connects each layer to every other layer in a feed - forward fashion. whereas traditional convolutional networks with l layers have l connections - one between each layer and its subsequent layer - our network has l ( l + 1 ) / 2 direct connections. for each layer, the feature - maps of all preceding layers are used as inputs, and its own feature - maps are used as inputs into all subsequent layers. densenets have several compelling advantages : they alleviate the vanishing - gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. we evaluate our proposed architecture on four highly competitive object recognition benchmark tasks ( cifar - 10, cifar - 100, svhn, and imagenet ). densenets obtain significant improvements over the state - of - the - art on most of them, whilst requiring less memory and computation to achieve high performance. code and pre - trained models are available at https',\n",
       "  'score': 0.44734230637550354},\n",
       " {'rank': 7,\n",
       "  'text': 'in this work we investigate the effect of the convolutional network depth on its accuracy in the large - scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3x3 ) convolution filters, which shows that a significant improvement on the prior - art configurations can be achieved by pushing the depth to 16 - 19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state - of - the - art results. we have made our two best - performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.',\n",
       "  'score': 0.44426923990249634},\n",
       " {'rank': 8,\n",
       "  'text': 'compared to the great progress of large - scale vision transformers ( vits ) in recent years, large - scale models based on convolutional neural networks ( cnns ) are still in an early state. this work presents a new large - scale cnn - based foundation model, termed internimage, which can obtain the gain from increasing parameters and training data like vits. different from the recent cnns that focus on large dense kernels, internimage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. as a result, the proposed internimage reduces the strict inductive bias of traditional cnns and makes it possible to learn stronger and more robust patterns with large - scale parameters from massive data like vits. the effectiveness of our model is proven on challenging benchmarks including imagenet, coco, andade20k. it is worth mentioning that internimage - h achieved a new record 65. 4 map on coco test - dev and 62. 9 miou on ade20k, outperforming current leading cnns and',\n",
       "  'score': 0.44404953718185425},\n",
       " {'rank': 9,\n",
       "  'text': 'developing neural network image classification models often requires significant architecture engineering. in this paper, we study a method to learn the model architectures directly on the dataset of interest. as this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. the key contribution of this work is the design of a new search space ( which we call the \" nasnet search space \" ) which enables transferability. in our experiments, we search for the best convolutional layer ( or \" cell \" ) on the cifar - 10 dataset and then apply this cell to the imagenet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \" nasnet architecture \". we also introduce a new regularization technique called scheduleddroppath that significantly improves generalization in the nasnet models. on cifar - 10 itself, a nasnet found by our method achieves 2. 4 % error rate, which is state - of - the - art. although the cell is not searched for directly on imagenet, a nasnet constructed from the best cell achieve',\n",
       "  'score': 0.4394289553165436},\n",
       " {'rank': 10,\n",
       "  'text': 'while the transformer architecture has become the de - facto standard for natural language processing tasks, its applications to computer vision remain limited. in vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. we show that this reliance on cnns is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. when pre - trained on large amounts of data and transferred to multiple mid - sized or small image recognition benchmarks ( imagenet, cifar - 100, vtab, etc. ), vision transformer ( vit ) attains excellent results compared to state - of - the - art convolutional networks while requiring substantially fewer computational resources to train.',\n",
       "  'score': 0.43924883008003235},\n",
       " {'rank': 11,\n",
       "  'text': 'while it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. despite this, the most widely used perceptual metrics today, such as psnr and ssim, are simple, shallow functions, and fail to account for many nuances of human perception. recently, the deep learning community has found that features of the vgg network trained on imagenet classification has been remarkably useful as a training loss for image synthesis. but how perceptual are these so - called \" perceptual losses \"? what elements are critical for their success? to answer these questions, we introduce a new dataset of human perceptual similarity judgments. we systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. we find that deep features outperform all previous metrics by large margins on our dataset. more surprisingly, this result is not restricted to imagenet - trained vgg features, but holds across different deep architectures and levels of supervision ( supervised, self - supervised, or even unsupervised ). our results suggest that perceptual similarity is an emergent property shared across deep visual representations.',\n",
       "  'score': 0.4314422011375427},\n",
       " {'rank': 12,\n",
       "  'text': 'in this work we describe how to train a multi - layer generative model of natural images. we use a dataset of millions of tiny colour images, described in the next section. this has been attempted by several groups but without success. the models on which we focus are rbms ( restricted boltzmann machines ) and dbns ( deep belief networks ). these models learn interesting - looking filters, which we show are more useful to a classifier than the raw pixels. we train the classifier on a labeled subset that we have collected and call the cifar - 10 dataset.',\n",
       "  'score': 0.4299197494983673},\n",
       " {'rank': 13,\n",
       "  'text': 'l $ _ 2 $ regularization and weight decay regularization are equivalent for standard stochastic gradient descent ( when rescaled by the learning rate ), but as we demonstrate this is \\\\ emph { not } the case for adaptive gradient algorithms, such as adam. while common implementations of these algorithms employ l $ _ 2 $ regularization ( often calling it \" weight decay \" in what may be misleading due to the inequivalence we expose ), we propose a simple modification to recover the original formulation of weight decay regularization by \\\\ emph { decoupling } the weight decay from the optimization steps taken w. r. t. the loss function. we provide empirical evidence that our proposed modification ( i ) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard sgd and adam and ( ii ) substantially improves adam \\' s generalization performance, allowing it to compete with sgd with momentum on image classification datasets ( on which it was previously typically outperformed by the latter ). our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in tensorflow and pytorch ; the complete source code for our experiments is available at',\n",
       "  'score': 0.42833784222602844},\n",
       " {'rank': 14,\n",
       "  'text': 'in the past decade, advances in deep learning have resulted in breakthroughs in a variety of areas, including computer vision, natural language understanding, speech recognition, and reinforcement learning. specialized, high - performing neural architectures are crucial to the success of deep learning in these areas. neural architecture search ( nas ), the process of automating the design of neural architectures for a given task, is an inevitable next step in automating machine learning and has already outpaced the best human - designed architectures on many tasks. in the past few years, research in nas has been progressing rapidly, with over 1000 papers released since 2020 ( deng and lindauer, 2021 ). in this survey, we provide an organized and comprehensive guide to neural architecture search. we give a taxonomy of search spaces, algorithms, and speedup techniques, and we discuss resources such as benchmarks, best practices, other surveys, and open - source libraries.',\n",
       "  'score': 0.4268895983695984},\n",
       " {'rank': 15,\n",
       "  'text': 'in this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. our motivation stems from the observation that 2d cnns applied to individual frames of the video have remained solid performers in action recognition. in this work we empirically demonstrate the accuracy advantages of 3d cnns over 2d cnns within the framework of residual learning. furthermore, we show that factorizing the 3d convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. our empirical study leads to the design of a new spatiotemporal convolutional block \" r ( 2 + 1 ) d \" which produces cnns that achieve results comparable or superior to the state - of - the - art on sports - 1m, kinetics, ucf101, and hmdb51.',\n",
       "  'score': 0.4205192029476166},\n",
       " {'rank': 16,\n",
       "  'text': 'the \" roaring 20s \" of visual recognition began with the introduction of vision transformers ( vits ), which quickly superseded convnets as the state - of - the - art image classification model. a vanilla vit, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. it is the hierarchical transformers ( e. g., swin transformers ) that reintroduced several convnet priors, making transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. however, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of transformers, rather than the inherent inductive biases of convolutions. in this work, we reexamine the design spaces and test the limits of what a pure convnet can achieve. we gradually \" modernize \" a standard resnet toward the design of a vision transformer, and discover several key components that contribute to the performance difference along the way. the outcome of this exploration is a family of pure convnet models dubbed convnext. constructed entirely from standard convnet modules, convnexts compete favorably with transformers in terms of accuracy and scalability, achieving 87. 8',\n",
       "  'score': 0.4189750552177429},\n",
       " {'rank': 17,\n",
       "  'text': 'we consider the problem of depth estimation from a single monocular image in this work. it is a challenging task as no reliable depth cues are available, e. g., stereo correspondences, motions etc. previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand - crafted features. recently, there is mounting evidence that features from deep convolutional neural networks ( cnn ) are setting new records for various vision applications. on the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field ( crf ) learning problem. therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep cnn and continuous crf. specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous crf in a unified deep cnn framework. the proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. in our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log - likelihood optimization. moreover, solving',\n",
       "  'score': 0.41539162397384644},\n",
       " {'rank': 18,\n",
       "  'text': 'we describe the deepmind kinetics human action video dataset. the dataset contains 400 human action classes, with at least 400 video clips for each action. each clip lasts around 10s and is taken from a different youtube video. the actions are human focussed and cover a broad range of classes including human - object interactions such as playing instruments, as well as human - human interactions such as shaking hands. we describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. we also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.',\n",
       "  'score': 0.40570175647735596},\n",
       " {'rank': 19,\n",
       "  'text': 'convolutional neural networks ( cnns ) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. optical flow estimation has not been among the tasks cnns succeeded at. in this paper we construct cnns which are capable of solving the optical flow estimation problem as a supervised learning task. we propose and compare two architectures : a generic architecture and another one including a layer that correlates feature vectors at different image locations. since existing ground truth data sets are not sufficiently large to train a cnn, we generate a large synthetic flying chairs dataset. we show that networks trained on this unrealistic data still generalize very well to existing datasets such as sintel and kitti, achieving competitive accuracy at frame rates of 5 to 10 fps.',\n",
       "  'score': 0.39965009689331055},\n",
       " {'rank': 20,\n",
       "  'text': '., object detection, class activation mapping, and salient object detection, further verify the superiority of the res2net over the state - of - the - art baseline methods. the source code and trained models are available on https : / / mmcheng. net / res2net /.',\n",
       "  'score': 0.3908083140850067},\n",
       " {'rank': 21,\n",
       "  'text': 'we present a simple, highly modularized network architecture for image classification. our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. our simple design results in a homogeneous, multi - branch architecture that has only a few hyper - parameters to set. this strategy exposes a new dimension, which we call cardinality ( the size of the set of transformations ), as an essential factor in addition to the dimensions of depth and width. on the imagenet - 1k dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. our models, named resnext, are the foundations of our entry to the ilsvrc 2016 classification task in which we secured 2nd place. we further investigate resnext on an imagenet - 5k set and the coco detection set, also showing better results than its resnet counterpart. the code and models are publicly available online.',\n",
       "  'score': 0.3897707164287567},\n",
       " {'rank': 22,\n",
       "  'text': 'designing convolutional neural networks ( cnn ) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. although significant efforts have been dedicated to design and improve mobile cnns on all dimensions, it is very difficult to manually balance these trade - offs when there are so many architectural possibilities to consider. in this paper, we propose an automated mobile neural architecture search ( mnas ) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade - off between accuracy and latency. unlike previous work, where latency is considered via another, often inaccurate proxy ( e. g., flops ), our approach directly measures real - world inference latency by executing the model on mobile phones. to further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. experimental results show that our approach consistently outperforms state - of - the - art mobile cnn models across multiple vision tasks. on the imagenet classification task, our mnasnet achieves 75. 2 % top - 1 accuracy with 78ms latency on a pixel phone, which is 1. 8× faster',\n",
       "  'score': 0.3875884711742401},\n",
       " {'rank': 23,\n",
       "  'text': 'this work investigates a simple yet powerful dense prediction task adapter for vision transformer ( vit ). unlike recently advanced variants that incorporate vision - specific inductive biases into their architectures, the plain vit suffers inferior performance on dense predictions due to weak prior assumptions. to address this issue, we propose the vit - adapter, which allows plain vit to achieve comparable performance to vision - specific transformers. specifically, the backbone in our framework is a plain vit that can learn powerful representations from large - scale multi - modal data. when transferring to downstream tasks, a pre - training - free adapter is used to introduce the image - related inductive biases into the model, making it suitable for these tasks. we verify vit - adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. notably, without using extra detection data, our vit - adapter - l yields state - of - the - art 60. 9 box ap and 53. 0 mask ap on coco test - dev. we hope that the vit - adapter could serve as an alternative for vision - specific transformers and facilitate future research. the code and models will be released at https : / / gi',\n",
       "  'score': 0.38235539197921753},\n",
       " {'rank': 24,\n",
       "  'text': 'the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder - decoder configuration. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data.',\n",
       "  'score': 0.3774770200252533},\n",
       " {'rank': 25,\n",
       "  'text': 'bigearthnet provides much higher accuracy compared to a state - of - the - art cnn model pre - trained on the imagenet ( which is a very popular large - scale benchmark archive in computer vision ). the bigearthnet opens up promising directions to advance operational rs applications and research in massive sentinel - 2 image archives.',\n",
       "  'score': 0.3747589588165283},\n",
       " {'rank': 26,\n",
       "  'text': 'learned features by nasnet used with the faster - rcnn framework surpass state - of - the - art by 4. 0 % achieving 43. 1 % map on the coco dataset.',\n",
       "  'score': 0.3738296329975128},\n",
       " {'rank': 27,\n",
       "  'text': 'recent work has demonstrated that deep neural networks are vulnerable to adversarial examples - - - inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. in fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. to address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. this approach provides us with a broad and unifying view on much of the prior work on this topic. its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. in particular, they specify a concrete security guarantee that would protect against any adversary. these methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. they also suggest the notion of security against a first - order adversary as a natural and broad security guarantee. we believe that robustness against such well - defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. code and pre - trained models are available at https : / / github. com / madrylab / mnist _ challenge',\n",
       "  'score': 0.37092316150665283},\n",
       " {'rank': 28,\n",
       "  'text': '##ext. constructed entirely from standard convnet modules, convnexts compete favorably with transformers in terms of accuracy and scalability, achieving 87. 8 % imagenet top - 1 accuracy and outperforming swin transformers on coco detection and ade20k segmentation, while maintaining the simplicity and efficiency of standard convnets.',\n",
       "  'score': 0.3707790672779083},\n",
       " {'rank': 29,\n",
       "  'text': 'neural architecture search ( nas ) has attracted increasingly more attention in recent years because of its capability to design deep neural network automatically. among them, differential nas approaches such as darts, have gained popularity for the search efficiency. however, they suffer from two main issues, the weak robustness to the performance collapse and the poor generalization ability of the searched architectures. to solve these two problems, a simple - but - efficient regularization method, termed as beta - decay, is proposed to regularize the darts - based nas searching process. specifically, beta - decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. furthermore, we provide in - depth theoretical analysis on how it works and why it works. experimental results on nas - bench - 201 show that our proposed method can help to stabilize the searching process and makes the searched network more transferable across different datasets. in addition, our search scheme shows an outstanding property of being less dependent on training time and data. comprehensive experiments on a variety of search spaces and datasets validate the effectiveness of the proposed method. the code is available at https : / / github. com / sunshine - ye / beta - darts.',\n",
       "  'score': 0.37010446190834045},\n",
       " {'rank': 30,\n",
       "  'text': \"bird ' s - eye - view ( bev ) is a powerful and widely adopted representation for road scenes that captures surrounding objects and their spatial locations, along with overall context in the scene. in this work, we focus on bird ' s eye semantic segmentation, a task that predicts pixel - wise semantic segmentation in bev from side rgb images. this task is made possible by simulators such as carla, which allow for cheap data collection, arbitrary camera placements, and supervision in ways otherwise not possible in the real world. there are two main challenges to this task : the view transformation from side view to bird ' s eye view, as well as transfer learning to unseen domains. existing work transforms between views through fully connected layers and transfer learns via gans. this suffers from a lack of depth reasoning and performance degradation across domains. our novel 2 - staged perception pipeline explicitly predicts pixel depths and combines them with pixel semantics in an efficient manner, allowing the model to leverage depth information to infer objects ' spatial locations in the bev. in addition, we transfer learning by abstracting high - level geometric features and predicting an intermediate representation that is common across different domains. we publish a new dataset called bevseg - carla and show that our\",\n",
       "  'score': 0.36861398816108704},\n",
       " {'rank': 31,\n",
       "  'text': 'the recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. these models could greatly simplify the use of images in any system by producing all - purpose visual features, i. e., features that work across image distributions and tasks without finetuning. this work shows that existing pretraining methods, especially self - supervised methods, can produce such features if trained on enough curated data from diverse sources. we revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. most of the technical contributions aim at accelerating and stabilizing the training at scale. in terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self - supervised literature. in terms of models, we train a vit model ( dosovitskiy et al., 2020 ) with 1b parameters and distill it into a series of smaller models that surpass the best available all - purpose features, openclip ( ilharco et al., 2021 ) on most of the benchmarks at image and pixel levels.',\n",
       "  'score': 0.36618876457214355},\n",
       " {'rank': 32,\n",
       "  'text': 'the release of tabular benchmarks, such as nas - bench - 101 and nas - bench - 201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search ( nas ). although they have been widely adopted and used to tune real - world nas algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. recently, several new nas benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. however, substantial differences among these nas benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. in this work, we present an in - depth analysis of popular nas algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few nas benchmarks do not generalize to other benchmarks. to help remedy this problem, we introduce nas - bench - suite, a comprehensive and extensible collection of nas benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid nas research. our code is available at https : / / github. com / automl /',\n",
       "  'score': 0.36444711685180664},\n",
       " {'rank': 33,\n",
       "  'text': 'we introduce a new language representation model called bert, which stands for bidirectional encoder representations from transformers. unlike recent language representation models, bert is designed to pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. as a result, the pre - trained bert model can be fine - tuned with just one additional output layer to create state - of - the - art models for a wide range of tasks, such as question answering and language inference, without substantial task - specific architecture modifications. bert is conceptually simple and empirically powerful. it obtains new state - of - the - art results on eleven natural language processing tasks, including pushing the glue score to 80. 5 % ( 7. 7 % point absolute improvement ), multinli accuracy to 86. 7 % ( 4. 6 % absolute improvement ), squad v1. 1 question answering test f1 to 93. 2 ( 1. 5 point absolute improvement ) and squad v2. 0 test f1 to 83. 1 ( 5. 1 point absolute improvement ).',\n",
       "  'score': 0.3628567159175873},\n",
       " {'rank': 34,\n",
       "  'text': ', which is state - of - the - art. although the cell is not searched for directly on imagenet, a nasnet constructed from the best cell achieves, among the published works, state - of - the - art accuracy of 82. 7 % top - 1 and 96. 2 % top - 5 on imagenet. our model is 1. 2 % better in top - 1 accuracy than the best human - invented architectures while having 9 billion fewer flops - a reduction of 28 % in computational demand from the previous state - of - the - art model. when evaluated at different levels of computational cost, accuracies of nasnets exceed those of the state - of - the - art human - designed models. for instance, a small version of nasnet also achieves 74 % top - 1 accuracy, which is 3. 1 % better than equivalently - sized, state - of - the - art models for mobile platforms. finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. on the task of object detection, the learned features by nasnet used with the faster - rcnn framework surpass state - of - the - art by 4. 0 % achieving 43. 1 %',\n",
       "  'score': 0.3618428409099579},\n",
       " {'rank': 35,\n",
       "  'text': 'the flownet demonstrated that optical flow estimation can be cast as a learning problem. however, the state of the art with regard to the quality of the flow has still been defined by traditional methods. particularly on small displacements and real - world data, flownet cannot compete with variational methods. in this paper, we advance the concept of end - to - end learning of optical flow and make it work really well. the large improvements in quality and speed are caused by three major contributions : first, we focus on the training data and show that the schedule of presenting data during training is very important. second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. flownet 2. 0 is only marginally slower than the original flownet but decreases the estimation error by more than 50 %. it performs on par with state - of - the - art methods, while running at interactive frame rates. moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original flownet.',\n",
       "  'score': 0.3575122058391571},\n",
       " {'rank': 36,\n",
       "  'text': 'we introduce discobox, a novel framework that jointly learns instance segmentation and semantic correspondence using bounding box supervision. specifically, we propose a self - ensembling framework where instance segmentation and semantic correspondence are jointly guided by a structured teacher in addition to the bounding box supervision. the teacher is a structured energy model incorporating a pairwise potential and a cross - image potential to model the pairwise pixel relationships both within and across the boxes. minimizing the teacher energy simultaneously yields refined object masks and dense correspondences between intra - class objects, which are taken as pseudo - labels to supervise the task network and provide positive / negative correspondence pairs for dense contrastive learning. we show a symbiotic relationship where the two tasks mutually benefit from each other. our best model achieves 37. 9 % ap on coco instance segmentation, surpassing prior weakly supervised methods and is competitive to supervised methods. we also obtain state of the art weakly supervised results on pascal voc12 and pf - pascal with real - time inference.',\n",
       "  'score': 0.3538137972354889},\n",
       " {'rank': 37,\n",
       "  'text': ', and achieve state of the art performance across two popular datasets. through combining a scene - centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction.',\n",
       "  'score': 0.35379260778427124},\n",
       " {'rank': 38,\n",
       "  'text': 'this paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non - differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. extensive experiments on cifar - 10, imagenet, penn treebank and wikitext - 2 show that our algorithm excels in discovering high - performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state - of - the - art non - differentiable techniques. our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.',\n",
       "  'score': 0.349403440952301},\n",
       " {'rank': 39,\n",
       "  'text': 'resistant deep learning models. code and pre - trained models are available at https : / / github. com / madrylab / mnist _ challenge and https : / / github. com / madrylab / cifar10 _ challenge.',\n",
       "  'score': 0.349340558052063},\n",
       " {'rank': 40,\n",
       "  'text': 'multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. though, state of the art ml models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence. we propose an evolutionary method capable of generating large scale multitask models that support the dynamic addition of new tasks. the generated multitask models are sparsely activated and integrates a task - based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands. the proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. we demonstrate empirically that the proposed method can jointly solve and achieve competitive results on 69public image classification tasks, for example improving the state of the art on a competitive benchmark such as cifar10 by achieving a 15 % relative error reduction compared to the best model trained on public data',\n",
       "  'score': 0.3450734615325928},\n",
       " {'rank': 41,\n",
       "  'text': 'in this paper, we tackle the problem of estimating the depth of a scene from a single image. this is a challenging task, since a single image on its own does not provide any depth cue. to address this, we exploit the availability of a pool of images for which the depth is known. more specifically, we formulate monocular depth estimation as a discrete - continuous optimization problem, where the continuous variables encode the depth of the superpixels in the input image, and the discrete ones represent relationships between neighboring superpixels. the solution to this discrete - continuous optimization problem is then obtained by performing inference in a graphical model using particle belief propagation. the unary potentials in this graphical model are computed by making use of the images with known depth. we demonstrate the effectiveness of our model in both the indoor and outdoor scenarios. our experimental evaluation shows that our depth estimates are more accurate than existing methods on standard datasets.',\n",
       "  'score': 0.34160715341567993},\n",
       " {'rank': 42,\n",
       "  'text': \"predicting the future trajectories of surrounding agents is essential for autonomous vehicles to operate safely. this paper presents qcnet, a modeling framework toward pushing the boundaries of trajectory prediction. first, we identify that the agent - centric modeling scheme used by existing approaches requires re - normalizing and re - encoding the input whenever the observation window slides forward, leading to redundant computations during online prediction. to overcome this limitation and achieve faster inference, we introduce a query - centric paradigm for scene encoding, which enables the reuse of past computations by learning representations independent of the global spacetime coordinate system. sharing the invariant scene features among all target agents further allows the parallelism of multi - agent trajectory decoding. second, even given rich encodings of the scene, existing decoding strategies struggle to capture the multimodality inherent in agents ' future behavior, especially when the prediction horizon is long. to tackle this challenge, we first employ anchor - free queries to generate trajectory proposals in a recurrent fashion, which allows the model to utilize different scene contexts when decoding waypoints at different horizons. a refinement module then takes the trajectory proposals as anchors and leverages anchor - based queries to refine the trajectories further. by supplying\",\n",
       "  'score': 0.34160587191581726},\n",
       " {'rank': 43,\n",
       "  'text': 'robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. as machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. in this work we present nutonomy scenes ( nuscenes ), the first dataset to carry the full autonomous vehicle sensor suite : 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuscenes comprises 1000 scenes, each 20s long and fully annotated with 3d bounding boxes for 23 classes and 8 attributes. it has 7x as many annotations and 100x as many images as the pioneering kitti dataset. we define novel 3d detection and tracking metrics. we also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. data, development kit and more information are available online.',\n",
       "  'score': 0.3413153290748596},\n",
       " {'rank': 44,\n",
       "  'text': 'we present a compact but effective cnn model for optical flow, called pwc - net. pwc - net has been designed according to simple and well - established principles : pyramidal processing, warping, and the use of a cost volume. cast in a learnable feature pyramid, pwc - net uses the current optical flow estimate to warp the cnn features of the second image. it then uses the warped features and features of the first image to construct a cost volume, which is processed by a cnn to estimate the optical flow. pwc - net is 17 times smaller in size and easier to train than the recent flownet2 model. moreover, it outperforms all published optical flow methods on the mpi sintel final pass and kitti 2015 benchmarks, running at about 35 fps on sintel resolution ( 1024 a × 436 ) images. our models are available on our project website.',\n",
       "  'score': 0.335767537355423},\n",
       " {'rank': 45,\n",
       "  'text': 'recent advance in 2d cnns has revealed that large kernels are important. however, when directly applying large convolutional kernels in 3d cnns, severe difficulties are met, where those successful module designs in 2d become surprisingly ineffective on 3d networks, including the popular depth - wise convolution. to address this vital challenge, we instead propose the spatial - wise partition convolution and its large - kernel module. as a result, it avoids the optimization and efficiency issues of naive 3d large kernels. our large - kernel 3d cnn network, largekernel3d, yields notable improvement in 3d tasks of semantic segmentation and object detection. it achieves 73. 9 % miou on the scannetv2 semantic segmentation and 72. 8 % nds nuscenes object detection benchmarks, ranking 1st on the nuscenes lidar leaderboard. the performance further boosts to 74. 2 % nds with a simple multi - modal fusion. in addition, largekernel3d can be scaled to 17×17×17 kernel size on waymo 3d object detection. for the first time, we show that large kernels are feasible and essential for 3d visual tasks. our code and models is available at github',\n",
       "  'score': 0.3327103555202484},\n",
       " {'rank': 46,\n",
       "  'text': 'existing top - performance 3d object detectors typically rely on the multi - modal fusion strategy. this design is however fundamentally restricted due to overlooking the modality - specific useful information and finally hampering the model performance. to address this limitation, in this work we introduce a novel modality interaction strategy where individual per - modality representations are learned and maintained throughout for enabling their unique characteristics to be exploited during object detection. to realize this proposed strategy, we design a deepinteraction architecture characterized by a multi - modal representational interaction encoder and a multi - modal predictive interaction decoder. experiments on the large - scale nuscenes dataset show that our proposed method surpasses all prior arts often by a large margin. crucially, our method is ranked at the first position at the highly competitive nuscenes object detection leaderboard.',\n",
       "  'score': 0.33242660760879517},\n",
       " {'rank': 47,\n",
       "  'text': '##1, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.',\n",
       "  'score': 0.33107832074165344},\n",
       " {'rank': 48,\n",
       "  'text': 'feature pyramids are a basic component in recognition systems for detecting objects at different scales. but pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. in this paper, we exploit the inherent multi - scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. a top - down architecture with lateral connections is developed for building high - level semantic feature maps at all scales. this architecture, called a feature pyramid network ( fpn ), shows significant improvement as a generic feature extractor in several applications. using a basic faster r - cnn system, our method achieves state - of - the - art single - model results on the coco detection benchmark without bells and whistles, surpassing all existing single - model entries including those from the coco 2016 challenge winners. in addition, our method can run at 5 fps on a gpu and thus is a practical and accurate solution to multi - scale object detection. code will be made publicly available.',\n",
       "  'score': 0.33041810989379883},\n",
       " {'rank': 49,\n",
       "  'text': '##net through three simple applications in object recognition, image classification and automatic object clustering. we hope that the scale, accuracy, diversity and hierarchical structure of imagenet can offer unparalleled opportunities to researchers in the computer vision community and beyond.',\n",
       "  'score': 0.3292371928691864},\n",
       " {'rank': 50,\n",
       "  'text': 'in this paper, we introduce a new large - scale object detection dataset, objects365, which has 365 object categories over 600k training images. more than 10 million, high - quality bounding boxes are manually labeled through a three - step, carefully designed annotation pipeline. it is the largest object detection dataset ( with full annotation ) so far and establishes a more challenging benchmark for the community. objects365 can serve as a better feature learning dataset for localization - sensitive tasks like object detection and semantic segmentation. the objects365 pre - trained models significantly outperform imagenet pre - trained models with 5. 6 points gain ( 42 vs 36. 4 ) based on the standard setting of 90k iterations on coco benchmark. even compared with much long training time like 540k iterations, our objects365 pretrained model with 90k iterations still have 2. 7 points gain ( 42 vs 39. 3 ). meanwhile, the finetuning time can be greatly reduced ( up to 10 times ) when reaching the same accuracy. better generalization ability of object365 has also been verified on citypersons, voc segmentation, and ade tasks. the dataset as well',\n",
       "  'score': 0.3239826261997223},\n",
       " {'rank': 51,\n",
       "  'text': '- supervised, or even unsupervised ). our results suggest that perceptual similarity is an emergent property shared across deep visual representations.',\n",
       "  'score': 0.3236856758594513},\n",
       " {'rank': 52,\n",
       "  'text': 'this paper presents the bigearthnet that is a new large - scale multi - label sentinel - 2 benchmark archive. the bigearthnet consists of 590, 326 sentinel - 2 image patches, each of which is a section of i ) 120 × 120 pixels for 10m bands ; ii ) 60×60 pixels for 20m bands ; and iii ) 20×20 pixels for 60m bands. unlike most of the existing archives, each image patch is annotated by multiple land - cover classes ( i. e., multi - labels ) that are provided from the corine land cover database of the year 2018 ( clc 2018 ). the bigearthnet is significantly larger than the existing archives in remote sensing ( rs ) and thus is much more convenient to be used as a training source in the context of deep learning. this paper first addresses the limitations of the existing archives and then describes the properties of the bigearthnet. experimental results obtained in the framework of rs image scene classification problems show that a shallow convolutional neural network ( cnn ) architecture trained on the bigearthnet provides much higher accuracy compared to a state - of - the - art cnn model pre - trained on the imagenet ( which is a very',\n",
       "  'score': 0.32350999116897583},\n",
       " {'rank': 53,\n",
       "  'text': 'recent work has demonstrated substantial gains on many nlp tasks and benchmarks by pre - training on a large corpus of text followed by fine - tuning on a specific task. while typically task - agnostic in architecture, this method still requires task - specific fine - tuning datasets of thousands or tens of thousands of examples. by contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current nlp systems still largely struggle to do. here we show that scaling up language models greatly improves task - agnostic, few - shot performance, sometimes even reaching competitiveness with prior state - of - the - art fine - tuning approaches. specifically, we train gpt - 3, an autoregressive language model with 175 billion parameters, 10x more than any previous non - sparse language model, and test its performance in the few - shot setting. for all tasks, gpt - 3 is applied without any gradient updates or fine - tuning, with tasks and few - shot demonstrations specified purely via text interaction with the model. gpt - 3 achieves strong performance on many nlp datasets, including translation, question - answering, and cloze tasks, as well as several tasks that require on',\n",
       "  'score': 0.32222405076026917},\n",
       " {'rank': 54,\n",
       "  'text': 'recent progress in 3d object detection from single images leverages monocular depth estimation as a way to produce 3d pointclouds, turning cameras into pseudo - lidar sensors. these two - stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large - scale self - supervised learning. however, they tend to suffer from overfitting more than end - to - end methods, are more complex, and the gap with similar lidar - based detectors remains significant. in this work, we propose an end - to - end, single stage, monocular 3d object detector, dd3d, that can benefit from depth pre - training like pseudo - lidar methods, but without their limitations. our architecture is designed for effective information transfer between depth estimation and 3d detection, allowing us to scale with the amount of unlabeled pre - training data. our method achieves state - of - the - art results on two challenging benchmarks, with 16. 34 % and 9. 28 % ap for cars and pedestrians ( respectively ) on the kitti - 3d benchmark, and 41. 5 % map on nuscenes.',\n",
       "  'score': 0.32178813219070435},\n",
       " {'rank': 55,\n",
       "  'text': 'as densenet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. although feature reuse enables densenet to produce strong features with a small number of model parameters and flops, the detector with densenet backbone shows rather slow speed and low energy efficiency. we find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. to solve the inefficiency of densenet, we propose an energy and computation efficient architecture called vovnet comprised of one - shot aggregation ( osa ). the osa not only adopts the strength of densenet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. to validate the effectiveness of vovnet as a backbone network, we design both lightweight and large - scale vovnet and apply them to one - stage and two - stage object detectors. our vovnet based detectors outperform densenet based ones with 2× faster speed and the energy consumptions are reduced by 1. 6× - 4. 1×. in addition to',\n",
       "  'score': 0.3184928596019745},\n",
       " {'rank': 56,\n",
       "  'text': 'this paper presents a new vision transformer, called swin transformer, that capably serves as a general - purpose backbone for computer vision. challenges in adapting transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. to address these differences, we propose a hierarchical transformer whose representation is computed with shifted windows. the shifted windowing scheme brings greater efficiency by limiting self - attention computation to non - overlapping local windows while also allowing for cross - window connection. this hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. these qualities of swin transformer make it compatible with a broad range of vision tasks, including image classification ( 87. 3 top - 1 accuracy on imagenet - 1k ) and dense prediction tasks such as object detection ( 58. 7 box ap and 51. 1 mask ap on coco test - dev ) and semantic segmentation ( 53. 5 miou on ade20k val ). its performance surpasses the previous state - of - the - art by a large margin of + 2. 7 box ap and + 2. 6 mask ap on coco',\n",
       "  'score': 0.3167276382446289},\n",
       " {'rank': 57,\n",
       "  'text': 'already been adopted by many researchers, and the community has implemented it in tensorflow and pytorch ; the complete source code for our experiments is available at https : / / github. com / loshchil / adamw - and - sgdw',\n",
       "  'score': 0.3158910870552063},\n",
       " {'rank': 58,\n",
       "  'text': 'driving requires interacting with road agents and predicting their future behaviour in order to navigate safely. we present fiery : a probabilistic future prediction model in bird ’ s - eye view from monocular cameras. our model predicts future instance segmentation and motion of dynamic agents that can be transformed into non - parametric future trajectories. our approach combines the perception, sensor fusion and prediction components of a traditional autonomous driving stack by estimating bird ’ s - eye - view prediction directly from surround rgb monocular camera inputs. fiery learns to model the inherent stochastic nature of the future solely from camera driving data in an end - to - end manner, without relying on hd maps, and predicts multimodal future trajectories. we show that our model outperforms previous prediction baselines on the nuscenes and lyft datasets. the code and trained models are available at https : / / github. com / wayveai / fiery.',\n",
       "  'score': 0.3140954375267029},\n",
       " {'rank': 59,\n",
       "  'text': \"we present a novel bird ' s - eye - view ( bev ) detector with perspective supervision, which converges faster and bet - suits modern image backbones. existing state - of - the - art bev detectors are often tied to certain depth pretrained backbones like vo vn et, hindering the synergy between booming image backbones and bev detectors. to address this limitation, we prioritize easing the optimization of bev detectors by introducing perspective view supervision. to this end, we propose a two - stage bev detector ; where proposals from the perspective head are fed into the bird ' s - eye - view head for final predictions. to evaluate the effectiveness of our model, we conduct extensive ablation studies focusing on the form of supervision and the gener - ality of the proposed detector. the proposed method is ver - ified with a wide spectrum of traditional and modern image backbones and achieves new sota results on the large - scale nuscenes dataset. the code shall be released soon.\",\n",
       "  'score': 0.31337782740592957},\n",
       " {'rank': 60,\n",
       "  'text': 'one - shot neural architecture search ( nas ) aims to minimize the computational expense of discovering state - of - the - art models. however, in the past year attention has been drawn to the comparable performance of naive random search across the same search spaces used by leading nas algorithms. to address this, we explore the effects of drastically relaxing the nas search space, and we present bonsai - net, an efficient one - shot nas method to explore our relaxed search space. bonsai - net is built around a modified differential pruner and can consistently discover state - of - the - art architectures that are significantly better than random search with fewer parameters than other state - of - the - art methods. additionally, bonsai - net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully - trained models from scratch.',\n",
       "  'score': 0.31221458315849304},\n",
       " {'rank': 61,\n",
       "  'text': 'the explosion of image data on the internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. but exactly how such data can be harnessed and organized remains a critical problem. we introduce here a new database called \" imagenet \", a large - scale ontology of images built upon the backbone of the wordnet structure. imagenet aims to populate the majority of the 80, 000 synsets of wordnet with an average of 500 – 1000 clean and full resolution images. this will result in tens of millions of annotated images organized by the semantic hierarchy of wordnet. this paper offers a detailed analysis of imagenet in its current state : 12 subtrees with 5247 synsets and 3. 2 million images in total. we show that imagenet is much larger in scale and diversity and much more accurate than the current image datasets. constructing such a large - scale database is a challenging task. we describe the data collection scheme with amazon mechanical turk. lastly, we illustrate the usefulness of imagenet through three simple applications in object recognition, image classification and automatic object clustering. we hope that the scale, accuracy, diversity and hierarchical structure of imagenet',\n",
       "  'score': 0.309894859790802},\n",
       " {'rank': 62,\n",
       "  'text': '- level geometric features and predicting an intermediate representation that is common across different domains. we publish a new dataset called bevseg - carla and show that our approach improves state - of - the - art by 24 % miou and performs well when transferred to a new domain.',\n",
       "  'score': 0.3094203770160675},\n",
       " {'rank': 63,\n",
       "  'text': 'examples in a simple transferability test we show can also be used to break defensive distillation. we hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.',\n",
       "  'score': 0.30719754099845886},\n",
       " {'rank': 64,\n",
       "  'text': 'we propose efficient neural architecture search ( enas ), a fast and inexpensive approach for automatic model design. in enas, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. the controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. thanks to parameter sharing between child models, enas is fast : it delivers strong empirical performances using much fewer gpu - hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard neural architecture search. on the penn treebank dataset, enas discovers a novel architecture that achieves a test perplexity of 55. 8, establishing a new state - of - the - art among all methods without post - training processing. on the cifar - 10 dataset, enas designs novel architectures that achieve a test error of 2. 89 %, which is on par with nasnet ( zoph et al., 2018 ), whose test error is 2. 65 %.',\n",
       "  'score': 0.29942455887794495},\n",
       " {'rank': 65,\n",
       "  'text': 'detr has been recently proposed to eliminate the need for many hand - designed components in object detection while demonstrating good performance. however, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of transformer attention modules in processing image feature maps. to mitigate these issues, we proposed deformable detr, whose attention modules only attend to a small set of key sampling points around a reference. deformable detr can achieve better performance than detr ( especially on small objects ) with 10 times less training epochs. extensive experiments on the coco benchmark demonstrate the effectiveness of our approach. code is released at this https url.',\n",
       "  'score': 0.2950435280799866},\n",
       " {'rank': 66,\n",
       "  'text': 'this report presents our method which wins the nuscenes3d detection challenge [ 17 ] held in workshop on autonomous driving ( wad, cvpr 2019 ). generally, we utilize sparse 3d convolution to extract rich semantic features, which are then fed into a class - balanced multi - head network to perform 3d object detection. to handle the severe class imbalance problem inherent in the autonomous driving scenarios, we design a class - balanced sampling and augmentation strategy to generate a more balanced data distribution. furthermore, we propose a balanced group - ing head to boost the performance for the categories withsimilar shapes. based on the challenge results, our methodoutperforms the pointpillars [ 14 ] baseline by a large mar - gin across all metrics, achieving state - of - the - art detection performance on the nuscenes dataset. code will be released at cbgs.',\n",
       "  'score': 0.2934407889842987},\n",
       " {'rank': 67,\n",
       "  'text': 'detection. for the first time, we show that large kernels are feasible and essential for 3d visual tasks. our code and models is available at github. com / dvlab - research / largekernel3d.',\n",
       "  'score': 0.29261523485183716},\n",
       " {'rank': 68,\n",
       "  'text': '3d object detectors usually rely on hand - crafted proxies, e. g., anchors or centers, and translate well - studied 2d frameworks to 3d. thus, sparse voxel features need to be densified and processed by dense prediction heads, which inevitably costs extra computation. in this paper, we instead propose voxelnext for fully sparse 3d object detection. our core insight is to predict objects directly based on sparse voxel features, without relying on hand - crafted proxies. our strong sparse convolutional network voxelnext detects and tracks 3d objects through voxel features entirely. it is an elegant and efficient framework, with no need for sparse - to - dense conversion or nms post - processing. our method achieves a better speed - accuracy trade - off than other mainframe detectors on the nuscenes dataset. for the first time, we show that a fully sparse voxel - based representation works decently for lidar 3d object detection and tracking. extensive experiments on nuscenes, waymo, and argoverse2 benchmarks validate the effectiveness of our approach. without bells and whistles, our model outperforms all existing lidar methods on the nuscenes tracking test benchmark',\n",
       "  'score': 0.29236117005348206},\n",
       " {'rank': 69,\n",
       "  'text': 'neural architecture search ( nas ) has attracted a lot of attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. architecture topology and architecture size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both aspects of the neural architectures. however, the performance gain from these searching algorithms is achieved under different search spaces and training setups. this makes the overall performance of the algorithms to some extent incomparable and the improvement from a sub - module of the searching model unclear. in this paper, we propose nats - bench, a unified benchmark on searching for both topology and size, for ( almost ) any up - to - date nas algorithm. nats - bench includes the search space of 15, 625 neural cell candidates for architecture topology and 32, 768 for architecture size on three datasets. we analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. we also show the versatility of nats - bench by benchmarking 13 recent state - of - the - art nas algorithms on it. all logs and diagnostic information trained using the same setup for each candidate',\n",
       "  'score': 0.291614830493927},\n",
       " {'rank': 70,\n",
       "  'text': 'we introduce a framework for multi - camera 3d object detection. in contrast to existing works, which estimate 3d bounding boxes directly from monocular images or use depth prediction networks to generate input for 3d object detection from 2d information, our method manipulates predictions directly in 3d space. our architecture extracts 2d features from multiple camera images and then uses a sparse set of 3d object queries to index into these 2d features, linking 3d positions to multi - view images using camera transformation matrices. finally, our model makes a bounding box prediction per object query, using a set - to - set loss to measure the discrepancy between the ground - truth and the prediction. this top - down approach outperforms its bottom - up counterpart in which object bounding box prediction follows per - pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. moreover, our method does not require post - processing such as non - maximum suppression, dramatically improving inference speed. we achieve state - of - the - art performance on the nuscenes autonomous driving benchmark.',\n",
       "  'score': 0.29045939445495605},\n",
       " {'rank': 71,\n",
       "  'text': 'better generalization ability of object365 has also been verified on citypersons, voc segmentation, and ade tasks. the dataset as well as the pretrained - models have been released at www. objects365. org.',\n",
       "  'score': 0.29032519459724426},\n",
       " {'rank': 72,\n",
       "  'text': 'most state - of - the - art techniques for multi - class image segmentation and labeling use conditional random fields defined over pixels or image regions. while region - level models often feature dense pairwise connectivity, pixel - level models are considerably larger and have only permitted sparse graph structures. in this paper, we consider fully connected crf models defined on the complete set of pixels in an image. the resulting graphs have billions of edges, making traditional inference algorithms impractical. our main contribution is a highly efficient approximate inference algorithm for fully connected crf models in which the pairwise edge potentials are defined by a linear combination of gaussian kernels. our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.',\n",
       "  'score': 0.28915688395500183},\n",
       " {'rank': 73,\n",
       "  'text': 'long - term temporal fusion is a crucial but often overlooked technique in camera - based bird \\' s - eye - view ( bev ) 3d perception. existing methods are mostly in a parallel manner. while parallel fusion can benefit from long - term information, it suffers from increasing computational and memory overheads as the fusion window size grows. alternatively, bevformer adopts a recurrent fusion pipeline so that history information can be efficiently integrated, yet it fails to benefit from longer temporal frames. in this paper, we explore an embarrassingly simple long - term recurrent fusion strategy built upon the lss - based methods and find it already able to enjoy the merits from both sides, i. e., rich long - term information and efficient fusion pipeline. a temporal embedding module is further proposed to improve the model \\' s robustness against occasionally missed frames in practical scenarios. we name this simple but effective fusing pipeline videobev. experimental results on the nuscenes benchmark show that videobev obtains strong performance on various camera - based 3d perception tasks, including object detection ( < bold xmlns : mml = \" http : / / www. w3. org / 1998 / math / mathml \" xmlns : xlink = \"',\n",
       "  'score': 0.28677618503570557},\n",
       " {'rank': 74,\n",
       "  'text': '##perform densenet based ones with 2× faster speed and the energy consumptions are reduced by 1. 6× - 4. 1×. in addition to densenet, vovnet also outperforms widely used resnet backbone with faster speed and better energy efficiency. in particular, the small object detection performance has been significantly improved over densenet and resnet.',\n",
       "  'score': 0.2866878807544708},\n",
       " {'rank': 75,\n",
       "  'text': 'recently 3d object detection from surround - view images has made notable advancements with its low deployment cost. however, most works have primarily focused on close perception range while leaving long - range detection less explored. expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. to address these limitations, this paper proposes a novel sparse query - based framework, dubbed far3d. by utilizing high - quality 2d object priors, we generate 3d adaptive queries that complement the 3d global queries. to efficiently capture discriminative features across different views and scales for long - range objects, we introduce a perspective - aware aggregation module. additionally, we propose a range - modulated 3d denoising approach to address query error propagation and mitigate convergence issues in long - range tasks. significantly, far3d demonstrates sota performance on the challenging argoverse 2 dataset, covering a wide range of 150 meters, surpassing several lidar - based approaches. the code is available at https : / / github. com / megvii - research / far3d.',\n",
       "  'score': 0.28535374999046326},\n",
       " {'rank': 76,\n",
       "  'text': '- of - the - art on most of them, whilst requiring less memory and computation to achieve high performance. code and pre - trained models are available at https : / / github. com / liuzhuang13 / densenet.',\n",
       "  'score': 0.2824649214744568},\n",
       " {'rank': 77,\n",
       "  'text': 'in lidar - based 3d object detection for autonomous driving, the ratio of the object size to input scene size is significantly smaller compared to 2d detection cases. over - looking this difference, many 3d detectors directly follow the common practice of 2d detectors, which downsample the feature maps even after quantizing the point clouds. in this paper, we start by rethinking how such multi - stride stereotype affects the lidar - based 3d object detectors. our experiments point out that the downsampling operations bring few advantages, and lead to inevitable information loss. to remedy this issue, we propose single - stride sparse transformer ( sst ) to maintain the original resolution from the beginning to the end of the network. armed with transformers, our method addresses the problem of insufficient receptive field in single - stride architectures. it also cooperates well with the sparsity of point clouds and naturally avoids expensive computation. eventually, our sst achieves state - of - the - art results on the large - scale waymo open dataset. it is worth mentioning that our method can achieve exciting performance ( 83. 8 level _ 1 ap on validation split ) on small object ( pedestrian ) detection due to the characteristic of single stride. our codes will be',\n",
       "  'score': 0.28141674399375916},\n",
       " {'rank': 78,\n",
       "  'text': \"neural networks provide state - of - the - art results for most machine learning tasks. unfortunately, neural networks are vulnerable to adversarial examples : given an input x and any target classification t, it is possible to find a new input x ' that is similar to x but classified as t. this makes it difficult to apply neural networks in security - critical areas. defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks ' ability to find adversarial examples from 95 % to 0. 5 %. in this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100 % probability. our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective ( and never worse ). furthermore, we propose using high - confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. we hope our attacks will be used as a benchmark in\",\n",
       "  'score': 0.28007546067237854},\n",
       " {'rank': 79,\n",
       "  'text': 'robotic perception requires the modeling of both 3d geometry and semantics. existing methods typically focus on estimating 3d bounding boxes, neglecting finer geometric details and struggling to handle general, out - of - vocabulary objects. 3d occupancy prediction, which estimates the detailed occupancy states and semantics of a scene, is an emerging task to overcome these limitations. to support 3d occupancy prediction, we develop a label generation pipeline that produces dense, visibility - aware labels for any given scene. this pipeline comprises three stages : voxel densification, occlusion reasoning, and image - guided voxel refinement. we establish two benchmarks, derived from the waymo open dataset and the nuscenes dataset, namely occ3d - waymo and occ3d - nuscenes benchmarks. furthermore, we provide an extensive analysis of the proposed dataset with various baseline models. lastly, we propose a new model, dubbed coarse - to - fine occupancy ( ctf - occ ) network, which demonstrates superior performance on the occ3d benchmarks. the code, data, and benchmarks are released at https : / / tsinghua - mars - lab. github. io / o',\n",
       "  'score': 0.2789725661277771},\n",
       " {'rank': 80,\n",
       "  'text': 'offline post - processing methods for object delineation. we apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging pascal voc 2012 segmentation benchmark.',\n",
       "  'score': 0.274816632270813},\n",
       " {'rank': 81,\n",
       "  'text': 'a refinement module then takes the trajectory proposals as anchors and leverages anchor - based queries to refine the trajectories further. by supplying adaptive and high - quality anchors to the refinement module, our query - based decoder can better deal with the multimodality in the output of trajectory prediction. our approach ranks 1 < sup xmlns : mml = \" http : / / www. w3. org / 1998 / math / mathml \" xmlns : xlink = \" http : / / www. w3. org / 1999 / xlink \" > st < / sup > on argoverse 1 and argoverse 2 motion forecasting benchmarks, outperforming all methods on all main metrics by a large margin. meanwhile, our model can achieve streaming scene encoding and parallel multi - agent decoding thanks to the query - centric design ethos.',\n",
       "  'score': 0.27440381050109863},\n",
       " {'rank': 82,\n",
       "  'text': 'recent multi - camera 3d object detectors usually leverage temporal information to construct multi - view stereo that alleviates the ill - posed depth estimation. however, they typically assume all the objects are static and directly aggregate features across frames. this work begins with a theoretical and empirical analysis to reveal that ignoring the motion of moving objects can result in serious localization bias. therefore, we propose to model dynamic objects in recurrent ( dort ) to tackle this problem. in contrast to previous global bird - eye - view ( bev ) methods, dort extracts object - wise local volumes for motion estimation that also alleviates the heavy computational burden. by iteratively refining the estimated object motion and location, the preceding features can be precisely aggregated to the current frame to mitigate the aforementioned adverse effects. the simple framework has two significant appealing properties. it is flexible and practical that can be plugged into most camera - based 3d object detectors. as there are predictions of object motion in the loop, it can easily track objects across frames according to their nearest center distances. without bells and whistles, dort outperforms all the previous methods on the nuscenes detection and tracking benchmarks with 62. 5 \\\\ % nds and 57. 6 \\\\ % amota, respectively',\n",
       "  'score': 0.2741837501525879},\n",
       " {'rank': 83,\n",
       "  'text': 'lidar and camera are two important sensors for 3d object detection in autonomous driving. despite the increasing popularity of sensor fusion in this field, the robustness against inferior image conditions, e. g., bad illumination and sensor misalignment, is under - explored. existing fusion methods are easily affected by such conditions, mainly due to a hard association of lidar points and image pixels, established by calibration matrices. we propose transfusion, a robust solution to lidar - camera fusion with a soft - association mechanism to handle inferior image conditions. specifically, our transfusion consists of convolutional backbones and a detection head based on a transformer decoder. the first layer of the decoder predicts initial bounding boxes from a lidar point cloud using a sparse set of object queries, and its second decoder layer adaptively fuses the object queries with useful image features, leveraging both spatial and contextual relationships. the attention mechanism of the transformer enables our model to adaptively determine where and what information should be taken from the image, leading to a robust and effective fusion strategy. we additionally design an image - guided query initialization strategy to deal with objects that are difficult to detect in point clouds. transfusion achieve',\n",
       "  'score': 0.2710835635662079},\n",
       " {'rank': 84,\n",
       "  'text': 'modern autonomous driving system is characterized as modular tasks in sequential order, i. e., perception, prediction, and planning. in order to perform a wide diversity of tasks and achieve advanced - level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi - task paradigm with separate heads. however, they might suffer from accumulative errors or deficient task coordination. instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i. e., planning of the self - driving car. oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. we introduce unified autonomous driving ( uniad ), a comprehensive framework up - to - date that incorporates full - stack driving tasks in one network. it is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. tasks are communicated with unified query interfaces to facilitate each other toward planning. we instantiate uniad on the challenging nuscenes benchmark. with extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state - of -',\n",
       "  'score': 0.26757583022117615},\n",
       " {'rank': 85,\n",
       "  'text': 'sparse algorithms offer great flexibility for multi - view temporal perception tasks. in this paper, we present an enhanced version of sparse4d, in which we improve the temporal fusion module by implementing a recursive form of multi - frame feature sampling. by effectively decoupling image features and structured anchor features, sparse4d enables a highly efficient transformation of temporal features, thereby facilitating temporal fusion solely through the frame - by - frame transmission of sparse features. the recurrent temporal fusion approach provides two main benefits. firstly, it reduces the computational complexity of temporal fusion from $ o ( t ) $ to $ o ( 1 ) $, resulting in significant improvements in inference speed and memory usage. secondly, it enables the fusion of long - term information, leading to more pronounced performance improvements due to temporal fusion. our proposed approach, sparse4dv2, further enhances the performance of the sparse perception algorithm and achieves state - of - the - art results on the nuscenes 3d detection benchmark. code will be available at \\\\ url { https : / / github. com / linxuewu / sparse4d }.',\n",
       "  'score': 0.2674674987792969},\n",
       " {'rank': 86,\n",
       "  'text': 'obstacles. stereo - based approaches have been classified into inverse perspective mapping ( ipm ) - based and disparity histogram - based methods. whether aerial or terrestrial, disparity histogram - based methods suffer from common problems : computational complexity, sensitivity to illumination changes, and the need for accurate camera calibration, especially when implemented on small robots. in addition, until recently, both monocular and stereo methods relied on conventional image processing techniques and, thus, did not meet the requirements of real - time applications. therefore, deep learning networks have been the centre of focus in recent years to develop fast and reliable obstacle detection solutions. however, we observed that despite significant progress, deep learning techniques also face difficulties in complex and unknown environments where objects of varying types and shapes are present. the review suggests that detecting narrow and small, moving obstacles and fast obstacle detection are the most challenging problem to focus on in future studies.',\n",
       "  'score': 0.26617079973220825},\n",
       " {'rank': 87,\n",
       "  'text': 'in this paper, we propose petrv2, a unified framework for 3d perception from multi - view images. based on petr [ 25 ], petrv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previous frames to boost 3d object detection. more specifically, we extend the 3d position embedding ( 3d pe ) in petr for temporal modeling. the 3d pe achieves the temporal alignment on object position of different frames. to support for multi - task learning ( e. g., bev segmentation and 3d lane detection ), petrv2 provides a simple yet effective solution by introducing task - specific queries, which are initialized under different spaces. petrv2 achieves state - of - the - art performance on 3d object detection, bev segmentation and 3d lane detection. detailed robustness analysis is also conducted on petr framework. code is available at https : / / github. com / megvii - research / petr.',\n",
       "  'score': 0.2631317675113678},\n",
       " {'rank': 88,\n",
       "  'text': \"while recent camera - only 3d detection methods leverage multiple timesteps, the limited history they use significantly hampers the extent to which temporal fusion can improve object perception. observing that existing works ' fusion of multi - frame images are instances of temporal stereo matching, we find that performance is hindered by the interplay between 1 ) the low granularity of matching resolution and 2 ) the sub - optimal multi - view setup produced by limited history usage. our theoretical and empirical analysis demonstrates that the optimal temporal difference between views varies significantly for different pixels and depths, making it necessary to fuse many timesteps over long - term history. building on our investigation, we propose to generate a cost volume from a long history of image observations, compensating for the coarse but efficient matching resolution with a more optimal multi - view matching setup. further, we augment the per - frame monocular depth predictions used for long - term, coarse matching with short - term, fine - grained matching and find that long and short term temporal fusion are highly complementary. while maintaining high efficiency, our framework sets new state - of - the - art on nuscenes, achieving first place on the test set and outperforming previous best art by 5. 2 % map and\",\n",
       "  'score': 0.2605372965335846},\n",
       " {'rank': 89,\n",
       "  'text': 'monocular 3d object detection is an important task for autonomous driving considering its advantage of low cost. it is much more challenging than conventional 2d cases due to its inherent ill - posed property, which is mainly reflected in the lack of depth information. recent progress on 2d detection offers opportunities to better solving this problem. however, it is non - trivial to make a general adapted 2d detector work in this 3d task. in this paper, we study this problem with a practice built on a fully convolutional single - stage detector and propose a general framework fcos3d. specifically, we first transform the commonly defined 7 - dof 3d targets to the image domain and decouple them as 2d and 3d attributes. then the objects are distributed to different feature levels with consideration of their 2d scales and assigned only according to the projected 3d - center for the training procedure. furthermore, the center - ness is redefined with a 2d gaussian distribution based on the 3d - center to fit the 3d target formulation. all of these make this framework simple yet effective, getting rid of any 2d detection or 2d - 3d correspondence priors. our solution achieves 1st place out of all the vision - only methods in the nuscenes 3d detection challenge of neurips 2020.',\n",
       "  'score': 0.25785091519355774},\n",
       " {'rank': 90,\n",
       "  'text': 'correspondence priors. our solution achieves 1st place out of all the vision - only methods in the nuscenes 3d detection challenge of neurips 2020. code and models are released at https : / / github. com / open - mmlab / mmdetection3d.',\n",
       "  'score': 0.2560981512069702},\n",
       " {'rank': 91,\n",
       "  'text': 'new record 65. 4 map on coco test - dev and 62. 9 miou on ade20k, outperforming current leading cnns and vits.',\n",
       "  'score': 0.2550739645957947},\n",
       " {'rank': 92,\n",
       "  'text': \"this technical report summarizes the winning solution for the 3d occupancy prediction challenge, which is held in conjunction with the cvpr 2023 workshop on end - to - end autonomous driving and cvpr 23 workshop on vision - centric autonomous driving workshop. our proposed solution fb - occ builds upon fb - bev, a cutting - edge camera - based bird ' s - eye view perception design using forward - backward projection. on top of fb - bev, we further study novel designs and optimization tailored to the 3d occupancy prediction task, including joint depth - semantic pre - training, joint voxel - bev representation, model scaling up, and effective post - processing strategies. these designs and optimization result in a state - of - the - art miou score of 54. 19 % on the nuscenes dataset, ranking the 1st place in the challenge track. code and models will be released at : https : / / github. com / nvlabs / fb - bev.\",\n",
       "  'score': 0.2547275722026825},\n",
       " {'rank': 93,\n",
       "  'text': 'view transformation module ( vtm ), where transformations happen between multi - view image features and bird - eye - view ( bev ) representation, is a crucial step in camera - based bev perception systems. currently, the two most prominent vtm paradigms are forward projection and backward projection. forward projection, represented by lift - splat - shoot, leads to sparsely projected bev features without post - processing. backward projection, with bev - former being an example, tends to generate false - positive bev features from incorrect projections due to the lack of utilization on depth. to address the above limitations, we propose a novel forward - backward view transformation module. our approach compensates for the deficiencies in both existing methods, allowing them to enhance each other to obtain higher quality bev representations mutually. we instantiate the proposed module with fb - bev, which achieves a new state - of - the - art result of 62. 4 % nds on the nuscenes test set. code and models are available at https : / / github. com / nvlabs / fb - bev.',\n",
       "  'score': 0.2545165717601776},\n",
       " {'rank': 94,\n",
       "  'text': 'injected. in our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log - likelihood optimization. moreover, solving the map problem for predicting depths of a new image is highly efficient as closed - form solutions exist. we experimentally demonstrate that the proposed method outperforms state - of - the - art depth estimation methods on both indoor and outdoor scene datasets.',\n",
       "  'score': 0.250812292098999},\n",
       " {'rank': 95,\n",
       "  'text': 'proposed method on both indoor and outdoor benchmark rgb - depth datasets and achieve state - of - the - art performance.',\n",
       "  'score': 0.24679292738437653},\n",
       " {'rank': 96,\n",
       "  'text': 'we propose mask auto - labeler ( mal ), a high - quality transformer - based mask auto - labeling framework for instance segmentation using only box annotations. mal takes box - cropped images as inputs and conditionally generates their mask pseudo - labels. we show that vision transformers are good mask auto - labelers. our method significantly reduces the gap between auto - labeling and human annotation regarding mask quality. instance segmentation models trained using the mal - generated masks can nearly match the performance of their fully - supervised counterparts, retaining up to 97. 4 % performance of fully supervised models. the best model achieves 44. 1 % map on coco instance segmentation ( test - dev 2017 ), outperforming state - of - the - art box - supervised methods by significant margins. qualitative results indicate that masks produced by mal are, in some cases, even better than human annotations.',\n",
       "  'score': 0.24673491716384888},\n",
       " {'rank': 97,\n",
       "  'text': 'single frame data contains finite information which limits the performance of the existing vision - based multi - camera 3d object detection paradigms. for fundamentally pushing the performance boundary in this area, a novel paradigm dubbed bevdet4d is proposed to lift the scalable bevdet paradigm from the spatial - only 3d space to the spatial - temporal 4d space. we upgrade the naive bevdet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. in this way, with negligible additional computing budget, we enable bevdet4d to access the temporal cues by querying and comparing the two candidate features. beyond this, we simplify the task of velocity prediction by removing the factors of ego - motion and time in the learning target. as a result, bevdet4d with robust generalization performance reduces the velocity error by up to - 62. 9 %. this makes the vision - based methods, for the first time, become comparable with those relied on lidar or radar in this aspect. on challenge benchmark nuscenes, we report a new record of 54. 5 % nds with the high - performance configuration dubbed bevdet4d - base, which surpass',\n",
       "  'score': 0.24626055359840393},\n",
       " {'rank': 98,\n",
       "  'text': ', our mnasnet achieves 75. 2 % top - 1 accuracy with 78ms latency on a pixel phone, which is 1. 8× faster than mobilenetv2 with 0. 5 % higher accuracy and 2. 3× faster than nasnet with 1. 2 % higher accuracy. our mnasnet also achieves better map quality than mobilenets for coco object detection. code is at https : / / github. com / tensorflow / tpu / tree / master / models / official / mnasnet.',\n",
       "  'score': 0.24362434446811676},\n",
       " {'rank': 99,\n",
       "  'text': \"##s strong performance on many nlp datasets, including translation, question - answering, and cloze tasks, as well as several tasks that require on - the - fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3 - digit arithmetic. at the same time, we also identify some datasets where gpt - 3 ' s few - shot learning still struggles, as well as some datasets where gpt - 3 faces methodological issues related to training on large web corpora. finally, we find that gpt - 3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. we discuss broader societal impacts of this finding and of gpt - 3 in general.\",\n",
       "  'score': 0.24098671972751617},\n",
       " {'rank': 100,\n",
       "  'text': ') method is used to enhance the depth quality of the lifted points at each interaction level. second, a gated modality - aware convolution ( gma - conv ) block is applied to modulate voxels involved with the camera modality in a fine - grained manner and then aggregate multi - modal features into a unified space. together they provide the detection head with more comprehensive features from lidar and camera. on the nuscenes test benchmark, our proposed method, abbreviated as msmd - fusion, achieves state - of - the - art results on both 3d object detection and tracking tasks without using test - time - augmentation and ensemble techniques. the code is available at https : / / github. com / sxjyjay / msmdfusion.',\n",
       "  'score': 0.23845794796943665}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.search('Deep residual networks', top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dev (tokenisation, embedding and pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search import store\n",
    "reload(store)\n",
    "from semantic_search.store.models import LocalEmbeddingModel\n",
    "\n",
    "model = LocalEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking and encoding: 100%|██████████| 4/4 [00:00<00:00, 585.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 8 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tmp_input = \"Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model. You can concatenate multiple poolings together.\"\n",
    "\n",
    "inputs = [tmp_input*10, tmp_input*3, tmp_input, tmp_input*30]\n",
    "\n",
    "_, all_chunks_encoded = model.chunk_and_encode(inputs, progress_bar=True)\n",
    "\n",
    "# Flatten encoded\n",
    "tmp = {}\n",
    "for single_text_encoded in all_chunks_encoded:\n",
    "    for k, v in single_text_encoded.items():\n",
    "        if k not in tmp:\n",
    "            tmp[k] = []\n",
    "        tmp[k].append(v)\n",
    "encoded_flattened = {k: torch.cat(v) for k, v in tmp.items()}\n",
    "\n",
    "# Get embeddings for all chunks\n",
    "print(f\"Generating embeddings for {len(list(encoded_flattened.values())[0])} chunks...\")\n",
    "embeddings = model.get_embeddings(encoded_flattened, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "_, encoded = model.chunk_and_encode(tmp_input*100)\n",
    "emb = model.get_embeddings(encoded)\n",
    "\n",
    "\n",
    "encoded_input = model.tokenizer(tmp_input*100, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model.model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0] == sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_search.utils import load_metadata\n",
    "\n",
    "df, ref_df = load_metadata(\n",
    "    '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/raw-data/metadata3',\n",
    "    filter_good_papers=True,\n",
    "    filter_good_references=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oaid</th>\n",
       "      <th>doi</th>\n",
       "      <th>ref_via</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>type</th>\n",
       "      <th>topic</th>\n",
       "      <th>domain</th>\n",
       "      <th>field</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2194775991</td>\n",
       "      <td>10.1109/cvpr.2016.90</td>\n",
       "      <td>openalex_id</td>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>Deeper neural networks are more difficult to t...</td>\n",
       "      <td>article</td>\n",
       "      <td>Advanced Neural Network Applications</td>\n",
       "      <td>Physical Sciences</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          oaid                   doi      ref_via  \\\n",
       "0  W2194775991  10.1109/cvpr.2016.90  openalex_id   \n",
       "\n",
       "                                          title  \\\n",
       "0  Deep Residual Learning for Image Recognition   \n",
       "\n",
       "                                            abstract     type  \\\n",
       "0  Deeper neural networks are more difficult to t...  article   \n",
       "\n",
       "                                  topic             domain             field  \\\n",
       "0  Advanced Neural Network Applications  Physical Sciences  Computer Science   \n",
       "\n",
       "                                  subfield  \n",
       "0  Computer Vision and Pattern Recognition  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.31851532.lcarretero/ipykernel_946913/2934055006.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started(): pt.init(home_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrier-assemblies 5.11 jar-with-dependencies not found, downloading to /cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/terrier-assemblies-5.11-jar-with-dependencies.jar4pglkysh.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyterrier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pt.started(): pt.init(home_dir=\u001b[33m'\u001b[39m\u001b[33m/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# import your documents into an index\u001b[39;00m\n\u001b[32m     14\u001b[39m indexer = pt.TerrierIndexer(\u001b[33m'\u001b[39m\u001b[33m/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/terrier_index\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:57\u001b[39m, in \u001b[36mbefore_init.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m started():\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mYou can only call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m before either you start using java or call pt.java.init()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:232\u001b[39m, in \u001b[36mlegacy_init\u001b[39m\u001b[34m(version, mem, packages, jvm_opts, redirect_io, logging, home_dir, boot_packages, tqdm, no_download, helper_version)\u001b[39m\n\u001b[32m    229\u001b[39m     pt.java.mavenresolver.offline()\n\u001b[32m    230\u001b[39m     deprecated_calls.append(\u001b[33m'\u001b[39m\u001b[33mpt.java.mavenresolver.offline()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m deprecated_calls.append(\u001b[33m'\u001b[39m\u001b[33mpt.java.init() # optional, forces java initialisation\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Import other java packages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:96\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/utils.py:94\u001b[39m, in \u001b[36monce.<locals>._once.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has already been run\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# how to handle errors?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m res = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m called = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/_utils.py:126\u001b[39m, in \u001b[36m_init\u001b[39m\u001b[34m(trigger)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# run pre-initialization setup\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, initializer \u001b[38;5;129;01min\u001b[39;00m initializers:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[43minitializer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnius_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnius\u001b[39;00m \u001b[38;5;66;03m# noqa: PT100 \u001b[39;00m\n\u001b[32m    129\u001b[39m _started = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/terrier/java.py:50\u001b[39m, in \u001b[36mTerrierJavaInit.pre_init\u001b[39m\u001b[34m(self, jnius_config)\u001b[39m\n\u001b[32m     48\u001b[39m     trJar = pt.java.mavenresolver.get_package_jar(\u001b[33m\"\u001b[39m\u001b[33mcom.github.terrier-org.terrier-core\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mterrier-assemblies\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m5.x-SNAPSHOT\u001b[39m\u001b[33m\"\u001b[39m, artifact=\u001b[33m\"\u001b[39m\u001b[33mjar-with-dependencies\u001b[39m\u001b[33m\"\u001b[39m, force_download=configure[\u001b[33m'\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     trJar = \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmavenresolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_package_jar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTERRIER_PKG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterrier-assemblies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterrier_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjar-with-dependencies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m jnius_config.add_classpath(trJar)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# now the helper classes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyterrier/java/mavenresolver.py:112\u001b[39m, in \u001b[36mget_package_jar\u001b[39m\u001b[34m(orgName, packageName, version, file_path, artifact, force_download)\u001b[39m\n\u001b[32m    109\u001b[39m     mvnUrl = MAVEN_BASE_URL + filelocation\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43mwget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvnUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.HTTPError, urllib.error.URLError) \u001b[38;5;28;01mas\u001b[39;00m he:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == OnlineMode.UNSET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/wget.py:506\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, out, bar)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# get filename for temp file in current directory\u001b[39;00m\n\u001b[32m    505\u001b[39m prefix = detect_filename(url, out)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m (fd, tmpfile) = \u001b[43mtempfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkstemp\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.tmp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m os.close(fd)\n\u001b[32m    508\u001b[39m os.unlink(tmpfile)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/tempfile.py:341\u001b[39m, in \u001b[36mmkstemp\u001b[39m\u001b[34m(suffix, prefix, dir, text)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     flags = _bin_openflags\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/tempfile.py:256\u001b[39m, in \u001b[36m_mkstemp_inner\u001b[39m\u001b[34m(dir, pre, suf, flags, output_type)\u001b[39m\n\u001b[32m    254\u001b[39m _sys.audit(\u001b[33m\"\u001b[39m\u001b[33mtempfile.mkstemp\u001b[39m\u001b[33m\"\u001b[39m, file)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     fd = \u001b[43m_os\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/terrier-assemblies-5.11-jar-with-dependencies.jar4pglkysh.tmp'"
     ]
    }
   ],
   "source": [
    "# Running below in REPL works\n",
    "\n",
    "import os\n",
    "\n",
    "# Set PyTerrier home directory\n",
    "os.environ['PYTERRIER_HOME'] = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev'\n",
    "os.environ['TMPDIR'] = '/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/tmp'\n",
    "\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "\n",
    "if not pt.started(): pt.init(home_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev')\n",
    "\n",
    "# import your documents into an index\n",
    "# Create a pandas DataFrame with the documents\n",
    "\n",
    "doc_texts = ref_df.abstract\n",
    "\n",
    "docs_df = pd.DataFrame({\n",
    "    'docno': [str(i) for i in range(len(doc_texts))],\n",
    "    'text': doc_texts\n",
    "})\n",
    "\n",
    "# Use IterDictIndexer instead of TerrierIndexer\n",
    "indexer = pt.IterDictIndexer('/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/src/semantic_search/dev/terrier_index')\n",
    "index_ref = indexer.index(docs_df.to_dict('records'))\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")\n",
    "results = bm25.transform(pt.Utils.prepare_qrels(pd.DataFrame({'qid':['q1'], 'query':['your keywords here']})))\n",
    "print(results[['qid', 'docno', 'score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev for new cleaner store setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find javac",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:347\u001b[39m, in \u001b[36mget_jdk_home\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     jdk_home = \u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjavac\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.replace(\u001b[33m'\u001b[39m\u001b[33mbin/javac\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/software/stacks/2024-05/python-cuda/3.11.6/lib/python3.11/posixpath.py:415\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[33;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     filename = os.fspath(filename)\n\u001b[32m    416\u001b[39m     path, ok = _joinrealpath(filename[:\u001b[32m0\u001b[39m], filename, strict, {})\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_search\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[32m      3\u001b[39m reload(store)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_search\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISSDocumentStore\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dsl/dsl-research-assistant/src/semantic_search/store/store.py:14\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# from rank_bm25 import BM25Okapi # Removed\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# import pickle # Removed, Pyserini handles its own persistence\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Pyserini imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msearch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlucene\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LuceneSearcher\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindex\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlucene\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexCollection\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/search/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pyserini: Reproducible IR research with sparse and dense representations\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_topics, get_topics_with_reader, get_qrels_file, get_qrels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/search/_base.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyserini\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autoclass\n\u001b[32m     27\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     28\u001b[39m logging.basicConfig(level=logging.WARNING, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/pyserini/pyclass.py:31\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# This might happen if the JVM's already been initialized. Just eat the error.\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnius\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autoclass, cast\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Base Java classes\u001b[39;00m\n\u001b[32m     34\u001b[39m JString = autoclass(\u001b[33m'\u001b[39m\u001b[33mjava.lang.String\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/__init__.py:45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjnius\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreflect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# XXX monkey patch methods that cannot be in cython.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Cython doesn't allow to set new attribute on methods it compiled\u001b[39;00m\n\u001b[32m     50\u001b[39m HASHCODE_MAX = \u001b[32m2\u001b[39m ** \u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/reflect.py:19\u001b[39m\n\u001b[32m     14\u001b[39m __all__ = (\u001b[33m'\u001b[39m\u001b[33mautoclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mensureclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprotocol_map\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m log = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mkivy\u001b[39m\u001b[33m'\u001b[39m).getChild(\u001b[34m__name__\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mClass\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mJavaClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetaclass\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetaJavaClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__javaclass__\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjava/lang/Class\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesiredAssertionStatus\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mJavaMethod\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m()Z\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_export_class.pxi:117\u001b[39m, in \u001b[36mjnius.MetaJavaClass.__new__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_export_class.pxi:177\u001b[39m, in \u001b[36mjnius.MetaJavaClass.resolve_class\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_env.pxi:11\u001b[39m, in \u001b[36mjnius.get_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_jvm_dlopen.pxi:95\u001b[39m, in \u001b[36mjnius.get_platform_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mjnius/jnius_jvm_dlopen.pxi:54\u001b[39m, in \u001b[36mjnius.create_jnienv\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:60\u001b[39m, in \u001b[36mget_java_setup\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# go hunting for Javac and Java programs, in that order\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     JAVA_HOME = \u001b[43mget_jdk_home\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_set(JAVA_HOME):\n\u001b[32m     63\u001b[39m     JAVA_HOME = get_jre_home(platform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/dsl-research-assistant/lib/python3.11/site-packages/jnius/env.py:351\u001b[39m, in \u001b[36mget_jdk_home\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m    347\u001b[39m             jdk_home = realpath(\n\u001b[32m    348\u001b[39m                 which(\u001b[33m'\u001b[39m\u001b[33mjavac\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    349\u001b[39m             ).replace(\u001b[33m'\u001b[39m\u001b[33mbin/javac\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    350\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnable to find javac\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jdk_home \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(jdk_home):\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: Unable to find javac"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from semantic_search.store import store\n",
    "reload(store)\n",
    "from semantic_search.store.store import FAISSDocumentStore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:32:08.412831: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 12:32:09.511891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 12:32:11.497976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded index with 41220 vectors\n"
     ]
    }
   ],
   "source": [
    "ds = FAISSDocumentStore(db_dir='/cluster/home/lcarretero/workspace/dsl/dsl-research-assistant/db/hybrid-dev3')\n",
    "assert ds.load_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'id': 'W2964137095',\n",
       "  'doi': '10.5244/c.30.87',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Wide Residual Networks',\n",
       "  'text': 'Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 2,\n",
       "  'id': 'W2194775991',\n",
       "  'doi': '10.1109/cvpr.2016.90',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Deep Residual Learning for Image Recognition',\n",
       "  'text': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 3,\n",
       "  'id': 'W2798581168',\n",
       "  'doi': '10.48550/arxiv.1804.10123',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image Classification',\n",
       "  'text': 'Deep residual networks (ResNets) made a recent breakthrough in deep learning. The core idea of ResNets is to have shortcut connections between layers that allow the network to be much deeper while still being easy to optimize avoiding vanishing gradients. These shortcut connections have interesting side-effects that make ResNets behave differently from other typical network architectures. In this work we use these properties to design a network based on a ResNet but with parameter sharing and with adaptive computation time. The resulting network is much smaller than the original network and can adapt the computational cost to the complexity of the input image.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 4,\n",
       "  'id': 'W2737740651',\n",
       "  'doi': '10.48550/arxiv.1707.04585',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'The Reversible Residual Network: Backpropagation Without Storing Activations',\n",
       "  'text': \"Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.\",\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 5,\n",
       "  'id': 'W2952574409',\n",
       "  'doi': '10.48550/arxiv.1611.04231',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Identity Matters in Deep Learning',\n",
       "  'text': 'An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \\\\emph{batch normalization}, but was also key to the immense success of \\\\emph{residual networks}. In this work, we put the principle of \\\\emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Adversarial Robustness in Machine Learning',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Artificial Intelligence'},\n",
       " {'rank': 6,\n",
       "  'id': 'W2964350391',\n",
       "  'doi': '10.1609/aaai.v31i1.11231',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning',\n",
       "  'text': 'Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Domain Adaptation and Few-Shot Learning',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Artificial Intelligence'},\n",
       " {'rank': 7,\n",
       "  'id': 'W2593110912',\n",
       "  'doi': '10.1109/cvpr.2017.539',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation',\n",
       "  'text': 'Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.',\n",
       "  'type': 'preprint',\n",
       "  'topic': 'Advanced Neural Network Applications',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 8,\n",
       "  'id': 'W2747898905',\n",
       "  'doi': '10.1109/cvpr.2017.298',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'Image Super-Resolution via Deep Recursive Residual Network',\n",
       "  'text': 'Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks, recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Advanced Image Processing Techniques',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'},\n",
       " {'rank': 9,\n",
       "  'id': 'W4390872537',\n",
       "  'doi': '10.1109/iccv51070.2023.00499',\n",
       "  'ref_via': 'openalex_id',\n",
       "  'title': 'Poincaré ResNet',\n",
       "  'text': 'This paper introduces an end-to-end residual network that operates entirely on the Poincaré ball model of hyperbolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only performed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard Euclidean networks. In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincaré ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincaré 2D convolutions up to Poincaré residual connections. We identify three roadblocks for training convolutional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initializations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initialization that preserves norms over many layers. (ii) Residual networks rely heavily on batch normalization, which comes with expensive Fréchet mean calculations in hyperbolic space. We introduce Poincaré midpoint batch normalization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincaré layers, the computation graphs of deep learning libraries blow up, limiting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Topological and Geometric Data Analysis',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computational Theory and Mathematics'},\n",
       " {'rank': 10,\n",
       "  'id': 'W3129492101',\n",
       "  'doi': '10.1109/access.2021.3061062',\n",
       "  'ref_via': 'doi',\n",
       "  'title': 'A Residual Dense U-Net Neural Network for Image Denoising',\n",
       "  'text': 'In recent years, convolutional neural networks have achieved considerable success in different computer vision tasks, including image denoising. In this work, we present a residual dense neural network (RDUNet) for image denoising based on the densely connected hierarchical network. The encoding and decoding layers of the RDUNet consist of densely connected convolutional layers to reuse the feature maps and local residual learning to avoid the vanishing gradient problem and speed up the learning process. Moreover, global residual learning is adopted such that, instead of directly predicting the denoised image, the model predicts the residual noise of the corrupted image. The algorithm was trained for the case of additive white Gaussian noise and using a wide range of noise levels. Hence, one advantage of the proposal is that the denoising process does not require prior knowledge about the noise level. In order to evaluate the model, we conducted several experiments with natural image databases available online, achieving competitive results compared with state-of-the-art networks for image denoising. For comparison purpose, we use additive Gaussian noise with levels 10, 30, 50. In the case of grayscale images, we achieved PSNR of 34.39, 29.11, 26.99, and SSIM of 0.9297, 0.8193, 0.7491. For color images we obtained PSNR of 36.68, 31.43, 29.12, and SSIM of 0.9600, 0.8961, 0.8465.',\n",
       "  'type': 'article',\n",
       "  'topic': 'Image and Signal Denoising Methods',\n",
       "  'domain': 'Physical Sciences',\n",
       "  'field': 'Computer Science',\n",
       "  'subfield': 'Computer Vision and Pattern Recognition'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.search('Deep residual networks', top_k=10, retrieval_method='hybrid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsl-research-assistant)",
   "language": "python",
   "name": "dsl-research-assistant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
